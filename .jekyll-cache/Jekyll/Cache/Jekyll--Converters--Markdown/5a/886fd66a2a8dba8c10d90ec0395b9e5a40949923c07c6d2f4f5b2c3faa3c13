I"â;<h1 id="kubevirt-and-prometheus-metrics">KubeVirt and Prometheus metrics</h1>

<p>In this blog post, we will explore the current state of integration between KubeVirt and Prometheus. For that, we‚Äôll be using the following bits and pieces:</p>

<ul>
  <li><a href="https://github.com/kubernetes/minikube">minikube</a>, as local Kubernetes deployment.</li>
  <li><a href="https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus">kube-prometheus bundle</a>, to quickly and easily deploy the whole monitoring stack, Promtheus, Grafana, ‚Ä¶</li>
  <li><a href="https://kubevirt.io">KubeVirt</a></li>
</ul>

<h2 id="starting-kubernetes-up">Starting Kubernetes up</h2>

<ul>
  <li>Installing <em>minikube</em> is detailed on the <a href="https://github.com/kubernetes/minikube#installation">Installation section</a> of the project‚Äôs README. If you happen to be running <a href="https://getfedora.org/">Fedora 29</a>, this <a href="https://copr.fedorainfracloud.org/coprs/tripledes/fedora-rpms/">Copr</a> repository can be used.</li>
  <li>
    <p>Following the documentation on both <em>minikube</em> and <em>kube-prometheus</em> bundle, the command I used to start Kubernetes is the following one:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ minikube start --cpus 2 --disk-size 30g --memory 10240 --vm-driver kvm2 --feature-gates=DevicePlugins=true --bootstrapper=kubeadm --extra-config=kubelet.authentication-token-webhook=true --extra-config=kubelet.authorization-mode=Webhook --extra-config=scheduler.address=0.0.0.0 --extra-config=controller-manager.address=0.0.0.0 --kubernetes-version=v1.11.5
</code></pre></div>    </div>

    <p>With that command you‚Äôll get a VM, using 2 vCPUS with 10GiB of RAM and running Kubernetes version 1.11.5, please adjust that to your needs.</p>
  </li>
</ul>

<h2 id="installing-the-monitoring-stack">Installing the monitoring stack</h2>

<ul>
  <li>Follow <a href="https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/README.md">this README</a> for step by step installation instructions.</li>
  <li>
    <p>Once installed, we can verify everything is up and running by checking out the <em>monitoring</em> namespace:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get all -n monitoring
NAME                                       READY     STATUS    RESTARTS   AGE
pod/alertmanager-main-0                    2/2       Running   2          3d
pod/alertmanager-main-1                    2/2       Running   2          3d
pod/alertmanager-main-2                    2/2       Running   2          3d
pod/grafana-7b9578fb4-jb2ts                1/1       Running   1          3d
pod/kube-state-metrics-fb7d5f59b-dr5zp     4/4       Running   5          3d
pod/node-exporter-jf2zk                    2/2       Running   2          3d
pod/prometheus-adapter-69bd74fc7-vlfcq     1/1       Running   2          3d
pod/prometheus-k8s-0                       3/3       Running   4          3d
pod/prometheus-k8s-1                       3/3       Running   4          3d
pod/prometheus-operator-6db8dbb7dd-5cb6r   1/1       Running   2          3d

NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/alertmanager-main       ClusterIP   10.100.239.1     &lt;none&gt;        9093/TCP            3d
service/alertmanager-operated   ClusterIP   None             &lt;none&gt;        9093/TCP,6783/TCP   3d
service/grafana                 ClusterIP   10.104.160.71    &lt;none&gt;        3000/TCP            3d
service/kube-state-metrics      ClusterIP   None             &lt;none&gt;        8443/TCP,9443/TCP   3d
service/node-exporter           ClusterIP   None             &lt;none&gt;        9100/TCP            3d
service/prometheus-adapter      ClusterIP   10.109.240.201   &lt;none&gt;        443/TCP             3d
service/prometheus-k8s          ClusterIP   10.103.208.241   &lt;none&gt;        9090/TCP            3d
service/prometheus-operated     ClusterIP   None             &lt;none&gt;        9090/TCP            3d
service/prometheus-operator     ClusterIP   None             &lt;none&gt;        8080/TCP            3d

NAME                           DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
daemonset.apps/node-exporter   1         1         1         1            1           beta.kubernetes.io/os=linux   3d

NAME                                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana               1         1         1            1           3d
deployment.apps/kube-state-metrics    1         1         1            1           3d
deployment.apps/prometheus-adapter    1         1         1            1           3d
deployment.apps/prometheus-operator   1         1         1            1           3d

NAME                                             DESIRED   CURRENT   READY     AGE
replicaset.apps/grafana-7b9578fb4                1         1         1         3d
replicaset.apps/kube-state-metrics-6dc79554cd    0         0         0         3d
replicaset.apps/kube-state-metrics-fb7d5f59b     1         1         1         3d
replicaset.apps/prometheus-adapter-69bd74fc7     1         1         1         3d
replicaset.apps/prometheus-operator-6db8dbb7dd   1         1         1         3d

NAME                                 DESIRED   CURRENT   AGE
statefulset.apps/alertmanager-main   3         3         3d
statefulset.apps/prometheus-k8s      2         2         3d
</code></pre></div>    </div>

    <p>So we can see that everything is up and running and we can even test that the access to Grafana and PromUI are working:</p>

    <ul>
      <li>
        <p>For Grafana, forward the port 3000 as follows and access http://localhost:3000:</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl --namespace monitoring port-forward svc/grafana 3000
</code></pre></div>        </div>

        <p>At the time of this writing, the username and password for Grafana are both <em>admin</em>.</p>
      </li>
      <li>
        <p>For PromUI, forward the port 9090 as follows and access http://localhost:9090:</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lets-deploy-kubevirt-and-dig-on-the-metrics-components">Let‚Äôs deploy KubeVirt and dig on the metrics components</h2>

<ul>
  <li>Deploy KubeVirt using the <a href="https://kubevirt.io/user-guide/#/README">official documentation</a>.For this blog post the version <em>0.11.0</em> has been used.</li>
  <li>Metrics:
    <ul>
      <li>If you‚Äôve installed KubeVirt before, there‚Äôs a service that might be unfamiliar to you, <em>service/kubevirt-prometheus-metrics</em>, this service uses a selector set to match the label <em>prometheus.kubevirt.io: ‚Äú‚Äú</em> which is included on all the main KubeVirt components, like the <em>virt-api</em>, <em>virt-controllers</em> and <em>virt-handler</em>.</li>
      <li>The <em>kubevirt-prometheus-metrics</em> also exposes the <em>metrics</em> port set to <em>443</em> so Promtheus can scrape the metrics for all the components through it.</li>
    </ul>
  </li>
  <li>
    <p>Let‚Äôs have a first look to the metrics that are exported:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl --namespace kube-system port-forward svc/kubevirt-prometheus-metrics 8443:443
$ curl -k https://localhost:8443/metrics
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 2.856e-05
go_gc_duration_seconds{quantile="0.25"} 8.4197e-05
go_gc_duration_seconds{quantile="0.5"} 0.000148234
go_gc_duration_seconds{quantile="0.75"} 0.000358119
go_gc_duration_seconds{quantile="1"} 0.014123096
go_gc_duration_seconds_sum 0.481708749
go_gc_duration_seconds_count 328
...
# HELP rest_client_requests_total Number of HTTP requests, partitioned by status code, method, and host.
# TYPE rest_client_requests_total counter
rest_client_requests_total{code="200",host="10.96.0.1:443",method="GET"} 125
rest_client_requests_total{code="200",host="10.96.0.1:443",method="PATCH"} 284
rest_client_requests_total{code="404",host="10.96.0.1:443",method="GET"} 284
rest_client_requests_total{code="&lt;error&gt;",host="10.96.0.1:443",method="GET"} 2
</code></pre></div>    </div>

    <p>As can be seen in the output from <em>curl</em>, there are quite some metrics, but we‚Äôll focus here mostly about the ones starting by <code class="highlighter-rouge">rest</code> as the others are mostly about Golang runtime and few other process internals, so the metrics list will be reduced to the following:</p>

    <ul>
      <li>rest_client_request_latency_seconds_bucket</li>
      <li>rest_client_request_latency_seconds_count</li>
      <li>rest_client_request_latency_seconds_sum</li>
      <li>rest_client_requests_total</li>
    </ul>

    <p>The <em>rest_client_request_latency_seconds</em>, represents the latency for each request being made to the API components broken down by verb and URL.</p>

    <p>The <em>rest_client_requests_total</em>, represents the number of HTTP requests, partitioned by status code, method, and host.</p>
  </li>
  <li>
    <p>Now, following again <a href="https://kubevirt.io/user-guide/#/installation/monitoring">KubeVirt‚Äôs docs</a>, we need to deploy two resources:</p>

    <ol>
      <li>
        <p>A <a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/design.md#prometheus">Prometheus resource</a>. Just copy the YAML snippet from <em>KubeVirt‚Äôs docs</em> and apply it as follows:</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl apply -f kubevirt-prometheus.yml -n kube-system
</code></pre></div>        </div>
      </li>
      <li>
        <p>A <a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/design.md#servicemonitor">ServiceMonitor resource</a>, again, take the YAML snippet from <em>KubeVirt‚Äôs docs</em> and apply it to the cluster:</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl apply -f kubevirt-servicemonitor.yml -n kube-system
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
  <li>
    <p>At this point we‚Äôre ready to fire up PromUI and start querying, accessing to it at http://localhost:9090, here are some examples:</p>

    <ol>
      <li>
        <p>Let‚Äôs query for the <em>rest_client_requests_total</em> filterying by service name <em>kubevirt-prometheus-metrics</em>:</p>

        <p><img src="../assets/2018-12-24-An-overview-to-KubeVirt-metrics/promui_1.png" alt="total_requests_text" /></p>
      </li>
      <li>
        <p>Now, the same metric, but let‚Äôs apply <a href="https://prometheus.io/docs/prometheus/latest/querying/functions/#rate">rate function</a>, on 1 minute intervals, looking at the graph tab we can see each component, with different HTTP status codes, methods (verbs) and more labels being added by Prometheus itself:</p>

        <p><img src="../assets/2018-12-24-An-overview-to-KubeVirt-metrics/promui_2.png" alt="total_requests_graph" /></p>
      </li>
    </ol>
  </li>
</ul>

<p>On both images, there is one status code, that I feel it‚Äôs worth a special mention, as it might be confusing, it‚Äôs <em>&lt;error&gt;</em>. This is not actual HTTP code, obvsiously, but rather a real error logged out by the component in question, in this case it was the pod <em>virt-handler-2pxcb</em>. What does it mean? To keep the variaty of metrics under control, any error string logged out during a request is translated by the string we see in the images, <em>&lt;error&gt;</em>, and it‚Äôs meant for us to notice that there might be issues that need our attention. Checking the pod for errors in the logs we can find the following ones:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl logs virt-handler-2pxcb -n kube-system | grep -i error
{"component":"virt-handler","level":"error","msg":"kubevirt.io/kubevirt/pkg/virt-handler/vm.go:440: Failed to list *v1.VirtualMachineInstance: Get https://10.96.0.1:443/apis/kubevirt.io/v1alpha2/virtualmachineinstances?labelSelector=kubevirt.io%2FnodeName+in+%28minikube%29\u0026limit=500\u0026resourceVersion=0: dial tcp 10.96.0.1:443: i/o timeout","pos":"reflector.go:205","timestamp":"2018-12-21T09:46:27.921051Z"}
{"component":"virt-handler","level":"error","msg":"kubevirt.io/kubevirt/pkg/virt-handler/vm.go:441: Failed to list *v1.VirtualMachineInstance: Get https://10.96.0.1:443/apis/kubevirt.io/v1alpha2/virtualmachineinstances?labelSelector=kubevirt.io%2FmigrationTargetNodeName+in+%28minikube%29\u0026limit=500\u0026resourceVersion=0: dial tcp 10.96.0.1:443: i/o timeout","pos":"reflector.go:205","timestamp":"2018-12-21T09:46:27.921168Z"}
</code></pre></div></div>

<p>Looking back at the first image, we can see the information there, matches what the logs say, exactly two ocurrances with method <em>GET</em>. So far, in this case, it‚Äôs nothing to worry about as it seems to be a temporary issue, but if the count grows, it‚Äôs likely there are serious issues that need fixing.</p>

<p>With that in mind, it‚Äôs not hard to create a dashboard in Grafana that would give us a glimpse of how KubeVirt is doing.</p>

<p><img src="../assets/2018-12-24-An-overview-to-KubeVirt-metrics/grafana_1.png" alt="grafana" /></p>

<p>The three rectangles on the top, are <em>singlestat</em>, in Grafana‚Äôs own terms, and those are first applying <em>rate()</em> by 5 minutes samples, then applying <em><a href="https://prometheus.io/docs/prometheus/latest/querying/operators/">count()</a></em> to aggragate the results in a single value. So the query is:</p>

<ul>
  <li><em>count(rate(rest_client_requests_total{service=‚Äùkubevirt-prometheus-metrics‚Äù,code=‚ÄùXXX‚Äù} [5m]))</em></li>
</ul>

<p>Replacing <em>XXX</em> by 404, 500 or <em>&lt;error&gt;</em>. The <em>singlestat</em> is useful for counters and for quickly seeing how a system/service is doing, as thresholds can be defined, changing the background (or the value) color based on the current measured amount.</p>

<p><img src="../assets/2018-12-24-An-overview-to-KubeVirt-metrics/grafana_thresholds.png" alt="thresholds" /></p>

<p>The graph below, runs the same query, but without the aggregation so we can see each component with different status codes and verbs.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>Even though the current state might not look very exciting, it‚Äôs a start, we can now monitor the <em>KubeVirt</em> components and make sure we get alarms when something is wrong.</p>

<p>Besides, <a href="https://github.com/kubevirt/kubevirt/pull/1840">there‚Äôs more to come</a>, the <em>KubeVirt team</em> is working hard to bring VM metrics to the table. Once this work is completed, we‚Äôll write another blog post, so stay tuned!</p>
:ET