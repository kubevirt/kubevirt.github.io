<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2024-07-08T13:41:39+00:00</updated><id>https://kubevirt.io//feed.xml</id><title type="html">KubeVirt.io</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">KubeVirt Summit 2024 CfP is open!</title><link href="https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP.html" rel="alternate" type="text/html" title="KubeVirt Summit 2024 CfP is open!" /><published>2024-03-19T00:00:00+00:00</published><updated>2024-03-19T00:00:00+00:00</updated><id>https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP</id><content type="html" xml:base="https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP.html"><![CDATA[<p>We are very pleased to announce the details for this year’s KubeVirt Summit!!</p>

<h2 id="what-is-kubevirt-summit">What is KubeVirt Summit?</h2>

<p>KubeVirt Summit is our annual online conference, now in its fourth year, in which the entire broader community meets to showcase technical architecture, new features, proposed changes, and in-depth tutorials.
We have two tracks to cater for developer talks, and another for end users to share their deployment journey with KubeVirt and their use case(s) at scale. And there’s no reason why a talk can’t be both :)</p>

<h2 id="when-is-it">When is it?</h2>

<p>The event will take place online over two half-days:</p>

<ul>
  <li>Dates: <del>June 25 and 26</del> <strong>June 24 and 25</strong>, 2024</li>
  <li>Time: TBD 
(In the past we have aimed for 1200-1700 UTC but may modify these times slightly depending on our speaker timezones)</li>
</ul>

<h2 id="how-to-submit-a-proposal">How to submit a proposal?</h2>

<ul>
  <li>Please submit through this <a href="https://docs.google.com/forms/d/e/1FAIpQLSeELmfpD_20kZnrciXkdSdDS_MLFLN9xSaZDKptNPjg3JGLaA/viewform">Googleform</a></li>
  <li>CfP closes: <strong>May 20</strong>, 2024</li>
  <li>Schedule will announced at the end of May</li>
</ul>

<p>Do consider proposing a session, and help make our fourth Summit as valuable as possible. We welcome a range of session types, any of which can be simple and intended for beginners or face-meltingly technical. Check out <a href="https://www.youtube.com/playlist?list=PLnLpXX8KHIYwe_V5pCXfXVDs-lY5dX55Q">last year’s talks</a> for some ideas.</p>

<h2 id="have-questions">Have questions?</h2>

<p>Our <a href="/summit/">KubeVirt Summit 2024</a> page will continue to evolve with details as we get closer.</p>

<p>You can also reach out on our <a href="https://kubernetes.slack.com/messages/virtualization">virtualization</a> Slack channel (in the Kubernetes workspace).</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our mailing list, slack channels, weekly meetings, and more, all list in our <a href="https://github.com/kubevirt/community">community repo</a>.</p>

<p>Good luck!</p>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's fourth annual dedicated online event]]></summary></entry><entry><title type="html">KubeVirt v1.2.0</title><link href="https://kubevirt.io//2024/changelog-v1.2.0.html" rel="alternate" type="text/html" title="KubeVirt v1.2.0" /><published>2024-03-05T00:00:00+00:00</published><updated>2024-03-05T00:00:00+00:00</updated><id>https://kubevirt.io//2024/changelog-v1.2.0</id><content type="html" xml:base="https://kubevirt.io//2024/changelog-v1.2.0.html"><![CDATA[<h2 id="v120">v1.2.0</h2>

<p>Released on: Tue Mar 5 20:25:04 2024 +0000</p>

<ul>
  <li>[PR #11318][fossedihelm] fix(vmclone): delete vmclone resource when the target vm is deleted</li>
  <li>[PR #11393][kubevirt-bot] Bug-fix: Fix nil panic if VM update fails</li>
  <li>[PR #11354][kubevirt-bot] Fix perfscale buckets error</li>
  <li>[PR #11378][fossedihelm] fix(ksm): set the <code class="language-plaintext highlighter-rouge">kubevirt.io/ksm-enabled</code> node label to true if the ksm is managed by KubeVirt, instead of reflect the actual ksm value.</li>
  <li>[PR #11271][kubevirt-bot] Bug fix: VM controller doesn’t corrupt its cache anymore</li>
  <li>[PR #11242][kubevirt-bot] Fix migration breaking in case the VM has an rng device after hotplugging a block volume on cgroupsv2</li>
  <li>[PR #11144][0xFelix] virtctl: Specifying size when creating a VM and using –volume-import to clone a PVC or a VolumeSnapshot is optional now</li>
  <li>[PR #11054][jean-edouard] New cluster-wide <code class="language-plaintext highlighter-rouge">vmRolloutStrategy</code> setting to define whether changes to VMs should either be always staged or live-updated when possible.</li>
  <li>[PR #11064][AlonaKaplan] Introduce a new API to mark a binding plugin as migratable.</li>
  <li>[PR #11122][brianmcarey] Update runc dependency to v1.1.12</li>
  <li>[PR #10982][machadovilaca] Refactor monitoring metrics</li>
  <li>[PR #11069][ormergi] Bug fix: Packet drops during the initial phase of VM live migration https://issues.redhat.com/browse/CNV-28040</li>
  <li>[PR #10961][jcanocan] Reduced VM rescheduling time on node failure</li>
  <li>[PR #11065][fossedihelm] fix(vmclone): Generate VM patches from vmsnapshotcontent, instead of current VM</li>
  <li>[PR #10888][fossedihelm] [Bugfix] Clone VM with WaitForFirstConsumer binding mode PVC now works.</li>
  <li>[PR #11068][brianmcarey] Update container base image to use current stable debian 12 base</li>
  <li>[PR #11047][jschintag] Fix potential crash when trying to list USB devices on host without any</li>
  <li>[PR #10970][alromeros] Expose fs disk information via GuestOsInfo</li>
  <li>[PR #11050][fossedihelm] restrict default cluster role to authenticated only users</li>
  <li>[PR #11025][0xFelix] Allow unprivileged users read-only access to VirtualMachineCluster{Instancetypes,Preferences} by default.</li>
  <li>[PR #10853][machadovilaca] Refactor monitoring collectors</li>
  <li>[PR #11001][fossedihelm] Allow <code class="language-plaintext highlighter-rouge">kubevirt.io:default</code> clusterRole to get,list kubevirts</li>
  <li>[PR #10905][tiraboschi] Aggregate DVs conditions on VMI (and so VM)</li>
  <li>[PR #10963][alromeros] Bugfix: Reject volume exports when no output is specified</li>
  <li>[PR #10962][machadovilaca] Update monitoring file structure</li>
  <li>[PR #10981][AlonaKaplan] Report IP of interfaces using network binding plugin.</li>
  <li>[PR #10922][kubevirt-bot] Updated common-instancetypes bundles to v0.4.0</li>
  <li>[PR #10914][brianmcarey] KubeVirt is now built with go 1.21.5</li>
  <li>[PR #10846][RamLavi] Change vm.status.PrintableStatus default value to “Stopped”</li>
  <li>[PR #10787][matthewei] # Create a manifest for a clone with template label filters:</li>
  <li>[PR #10918][orelmisan] VMClone: Emit an event in case restore creation fails</li>
  <li>[PR #10916][orelmisan] Fix the value of VMI <code class="language-plaintext highlighter-rouge">Status.GuestOSInfo.Version</code></li>
  <li>[PR #10924][AlonaKaplan] Deprecate macvtap</li>
  <li>[PR #10898][matthewei] vmi status’s guestOsInfo adds <code class="language-plaintext highlighter-rouge">Machine</code></li>
  <li>[PR #10866][AlonaKaplan] Raise an error in case passt feature gate or API are used.</li>
  <li>[PR #10879][brianmcarey] Built with golang 1.20.12</li>
  <li>[PR #10872][RamLavi] IsolateEmulatorThread: Add cluster-wide parity completion setting</li>
  <li>[PR #10700][machadovilaca] Refactor monitoring alerts</li>
  <li>[PR #10839][RamLavi] Change second emulator thread assign strategy to best-effort.</li>
  <li>[PR #10863][dhiller] Remove year from generated code copyright</li>
  <li>[PR #10747][acardace] Fix KubeVirt for CRIO 1.28 by using checksums to verify containerdisks when migrating VMIs</li>
  <li>[PR #10860][akalenyu] BugFix: Double cloning with filter fails</li>
  <li>[PR #10567][awels] Attachment pod creation is now rate limited</li>
  <li>[PR #10845][orelmisan] Reject VirtualMachineClone creation when target name is equal to source name</li>
  <li>[PR #10840][acardace] Requests/Limits can now be configured when using CPU/Memory hotplug</li>
  <li>[PR #10418][machadovilaca] Add total VMs created metric</li>
  <li>[PR #10800][AlonaKaplan] Support macvtap as a binding plugin</li>
  <li>[PR #10753][victortoso] Fixes device permission when using USB host passthrough</li>
  <li>[PR #10774][victortoso] Windows offline activation with ACPI SLIC table</li>
  <li>[PR #10783][RamLavi] Support multiple CPUs in Housekeeping cgroup</li>
  <li>[PR #10809][orelmisan] Source virt-launcher: Log migration info by default</li>
  <li>[PR #10046][victortoso] Add v1alpha3 for hooks</li>
  <li>[PR #10651][machadovilaca] Refactor monitoring  recording-rules</li>
  <li>[PR #10732][AlonaKaplan] Extend kubvirt CR by adding domain attachment option to the network binding plugin API.</li>
  <li>[PR #10244][hshitomi] Added “adm” subcommand under “virtctl”, and “log-verbosity” subcommand under “adm”. The log-verbosity command is:</li>
  <li>[PR #10658][matthewei] 1. Support “Clone API” to filter VirtualMachine.spec.template.annotation and VirtualMachine.spec.template.label</li>
  <li>[PR #10593][RamLavi] Fixes SMT Alignment Error in virt-launcher pod by optimizing isolateEmulatorThread feature (BZ#2228103).</li>
  <li>[PR #10720][awels] Restored hotplug attachment pod request/limit to original value</li>
  <li>[PR #10657][germag] Exposing Filesystem Persistent Volumes (PVs)  to the VM using unprivilege virtiofsd.</li>
  <li>[PR #10637][dharmit] Functional tests for sidecar hook with ConfigMap</li>
  <li>[PR #10598][alicefr] Add PVC option to the hook sidecars for supplying additional debugging tools</li>
  <li>[PR #10526][cfilleke]</li>
  <li>[PR #10699][qinqon] virt-launcher: fix qemu non root log path</li>
  <li>[PR #10689][akalenyu] BugFix: cgroupsv2 device allowlist is bound to virt-handler internal state/block disk device overwritten on hotplug</li>
  <li>[PR #10693][machadovilaca] Remove MigrateVmiDiskTransferRateMetric</li>
  <li>[PR #10615][orelmisan] Remove leftover NonRoot feature gate</li>
  <li>[PR #10529][alromeros] Allow LUN disks to be hotplugged</li>
  <li>[PR #10582][orelmisan] Remove leftover NonRootExperimental feature gate</li>
  <li>[PR #10596][mhenriks] Disable HTTP/2 to mitigate CVE-2023-44487</li>
  <li>[PR #10570][machadovilaca] Fix LowKVMNodesCount not firing</li>
  <li>[PR #10571][tiraboschi] vmi memory footprint increase by 35M when guest serial console logging is turned on (default on).</li>
  <li>[PR #10425][ormergi] Introduce network binding plugin for Passt networking, interfacing with Kubevirt new network binding plugin API.</li>
  <li>[PR #10479][dharmit] Ability to run scripts through hook sidecar</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.2.0 changes]]></summary></entry><entry><title type="html">Announcing KubeVirt v1.1</title><link href="https://kubevirt.io//2023/Announcing-KubeVirt-v1-1.html" rel="alternate" type="text/html" title="Announcing KubeVirt v1.1" /><published>2023-11-07T00:00:00+00:00</published><updated>2023-11-07T00:00:00+00:00</updated><id>https://kubevirt.io//2023/Announcing-KubeVirt-v1-1</id><content type="html" xml:base="https://kubevirt.io//2023/Announcing-KubeVirt-v1-1.html"><![CDATA[<p>The KubeVirt Community is very pleased to announce the release of KubeVirt v1.1. This comes 17 weeks after our celebrated v1.0 release, and follows the predictable schedule we moved to three releases ago to follow the Kubernetes release cadence.</p>

<p>You can read the full <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.1.0">v1.1 release notes here</a>, but we’ve asked the KubeVirt SIGs to summarize their largest successes, as well as one of the community members from Arm to list their integration accomplishments for this release.</p>

<h2 id="sig-compute">SIG-compute</h2>
<p>SIG-compute covers the core functionality of KubeVirt. This includes scheduling VMs, the API, and all KubeVirt operators.</p>

<p>For the v1.1 release, we have added quite a few features. This includes memory hotplug, as a follow up to CPU hotplug, which was part of the 1.0 release. Basic KSM support was already part of KubeVirt, but we have now extended that with more tuning parameters and KubeVirt can also dynamically configure KSM based on system pressure. We’ve added persistent NVRAM support (requires that a VM use UEFI) so that settings are preserved across reboots.</p>

<p>We’ve also added host-side USB passthrough support, so that USB devices on a cluster node can be made available to workloads. KubeVirt can now automatically apply limits to a VM running in a namespace with quotas. We’ve also added refinements to VM cloning, as well as the ability to create clones using the virtctl CLI tool. And you can now stream guest’s console logs.</p>

<p>Finally, on the confidential computing front, we now have an API for SEV attestation.</p>

<h2 id="sig-infra">SIG-infra</h2>
<p>SIG-infra takes care of KubeVirt’s own infrastructure, user workloads and other user-focused integrations through automation and the reduction of complexity wherever possible, providing a quality experience for end users.</p>

<p>In this release, two major instance type-related features were added to KubeVirt. The first feature is the deployment of Common InstanceTypes by the virt-operator. This provides users with a useful set of InstanceTypes and Preferences right out of the box and allows them to easily create virtual machines tailored to the needs of their workloads. For now this feature remains behind a feature gate, but in future versions we aim to enable the deployment by default.</p>

<p>Secondly, the inference of InstanceTypes and Preferences has been enabled by default when creating virtual machines with virtctl. This feature was already present in the previous release, but users still needed to explicitly enable it. Now it is enabled by default, being as transparent as possible so as to not let the creation of virtual machines fail if inference should not be possible. This significantly improves usability, as the command line for creating virtual machines is now even simpler.</p>

<h2 id="sig-network">SIG-network</h2>
<p>SIG-network is committed to enhancing and maintaining all aspects of Virtual Machine network connectivity and management in KubeVirt.</p>

<p>For the v1.1 release, we have re-designed the interface hot plug/unplug API, while adding hotplug support for SR-IOV interfaces. On top of that, we have added a network binding option allowing the community to extend the KubeVirt network configuration in the pod by injecting custom CNI plugins to configure the networking stack, and a sidecar to configure the libvirt domain. The existing <code class="language-plaintext highlighter-rouge">slirp</code> network configuration has been extracted from the code and re-designed as one such network binding, and can be used by the community as an example on how to extend KubeVirt bindings.</p>

<h2 id="sig-scale">SIG-scale</h2>
<p>SIG-scale continues to track scale and performance across releases.  The v1.1 testing lanes ran on Kubernetes 1.27 and we observed a slight performance improvement from Kubernetes.  There’s no other notable performance or scale changes in KubeVirt v1.1 as our focus has been on improving our tracking.</p>

<h4 id="vmicreationtorunningsecondsp95">vmiCreationToRunningSecondsP95</h4>
<ul>
  <li>The gray dotted line in the graph is Feb 1, 2023, denoting release of v0.59</li>
  <li>The blue dotted line in the graph is March 1, 2023, denoting release of v0.60</li>
  <li>The green dotted line in the graph is July 6, 2023, denoting release of v1.0.0</li>
  <li>The red dotted line in the graph is September 6, 2023, denoting change in k8s provider from v1.25 to v1.27</li>
</ul>

<p><img src="/assets/2023-11-07-Announcing-KubeVirt-v1-1/vmi-p95-Creation-to-Running.png" alt="Alt text" />
<img src="/assets/2023-11-07-Announcing-KubeVirt-v1-1/vm-p95-Creation-to-Running.png" alt="Alt text" /></p>

<p>Full v1.1 data source: <a href="https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md">https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md</a></p>

<h2 id="sig-storage">SIG-storage</h2>
<p>SIG-storage is focused on providing persistent storage to KubeVirt VMs and managing that storage throughout the lifecycle of the VM. This begins with provisioning and populating PVCs with bootable images but also includes features such as disk hotplug, snapshots, backup and restore, disaster recovery, and virtual machine export.</p>

<p>For this release we aimed to draw closer to Kubernetes principles when it comes to managing storage artifacts. Introducing CDI volume populators, which is CDI’s implementation of importing/uploading/cloning data to PVCs using the <code class="language-plaintext highlighter-rouge">dataSourceRef</code> field. This follows the Kubernetes way of populating PVCs and enables us to populate PVCs directly without the need for DataVolumes, an important but bespoke object that has served the KubeVirt use case for many years.</p>

<p>Speaking of DataVolumes, they will no longer be garbage collected by default, something that violated a fundamental principle of Kubernetes (even though it was very useful for our use case).</p>

<p>And, finally, we can now use snapshots to store operating system “golden images”, to serve as the base image for cloning.</p>

<h2 id="kubevirt-and-arm">KubeVirt and Arm</h2>
<p>We are excited to announce the successful integration of KubeVirt on Arm64 platforms. Here are some key accomplishments:</p>
<ol>
  <li><strong>Building and Compiling</strong>: We have released multi-architecture KubeVirt component images and binaries, while also allowing cross-compiling Arm64 architecture images and binaries on x86_64 platforms.</li>
  <li><strong>Core Functionality</strong>: Our dedicated efforts have focused on enabling the core functionality of KubeVirt on Arm64 platforms.</li>
  <li><strong>Testing Integration</strong>: Quality assurance is of paramount importance. We have integrated unit tests and end-to-end tests on Arm64 servers into the pull request (PR) pre-submit process. This guarantees that KubeVirt maintains its reliability and functionality on Arm64.</li>
  <li><strong>Comprehensive Documentation</strong>: To provide valuable insights into KubeVirt’s capabilities on Arm64 platforms, we have compiled extensive documentation. Explore the status of <a href="https://kubevirt.io/user-guide/operations/feature_gate_status_on_Arm64/">feature gates</a> and dive into <a href="https://kubevirt.io/user-guide/virtual_machines/device_status_on_Arm64/">device status documentation</a>.</li>
  <li><strong>Hybrid Cluster Compatibility Preview</strong>: Hybrid x86_64 and Arm64 clusters can work together now as a preview feature. Try it out and provide feedback.</li>
</ol>

<p>We are thrilled to declare that KubeVirt now offers tier-one support on Arm64 platforms. This milestone represents a culmination of collaborative efforts, unwavering dedication, and a commitment to innovation within the KubeVirt community. KubeVirt is no longer just an option; it has evolved to become a first-class citizen on Arm64 platforms.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Thank you to everyone in the KubeVirt Community who contributed to this release, whether you pitched in on any of the features listed above, helped out with any of the other features or maintenance improvements listed in our release notes, or made any number of non-code contributions to our website, user guide or meetings.</p>]]></content><author><name>KubeVirt Community</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.1.0" /><category term="release" /><category term="community" /><category term="cncf" /><summary type="html"><![CDATA[We are very pleased to announce the release of KubeVirt v1.1!]]></summary></entry><entry><title type="html">KubeVirt v1.1.0</title><link href="https://kubevirt.io//2023/changelog-v1.1.0.html" rel="alternate" type="text/html" title="KubeVirt v1.1.0" /><published>2023-11-06T00:00:00+00:00</published><updated>2023-11-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/changelog-v1.1.0</id><content type="html" xml:base="https://kubevirt.io//2023/changelog-v1.1.0.html"><![CDATA[<h2 id="v110">v1.1.0</h2>

<p>Released on: Mon Nov 6 16:28:56 2023 +0000</p>

<ul>
  <li>[PR #10669][kubevirt-bot] Introduce network binding plugin for Passt networking, interfacing with Kubevirt new network binding plugin API.</li>
  <li>[PR #10646][jean-edouard] The dedicated migration network should now always be properly detected by virt-handler</li>
  <li>[PR #10602][kubevirt-bot] Fix LowKVMNodesCount not firing</li>
  <li>[PR #10566][fossedihelm] Add 100Mi of memory overhead for vmi with dedicatedCPU or that wants GuaranteedQos</li>
  <li>[PR #10568][ormergi] Network binding plugin API support CNIs, new integration point on virt-launcher pod creation.</li>
  <li>[PR #10496][fossedihelm] Automatically set cpu limits when a resource quota with cpu limits is associated to the creation namespace and the <code class="language-plaintext highlighter-rouge">AutoResourceLimits</code> FeatureGate is enabled</li>
  <li>[PR #10309][lyarwood] cluster-wide <a href="https://github.com/kubevirt/common-instancetypes"><code class="language-plaintext highlighter-rouge">common-instancetypes</code></a> resources can now deployed by <code class="language-plaintext highlighter-rouge">virt-operator</code> using the <code class="language-plaintext highlighter-rouge">CommonInstancetypesDeploymentGate</code> feature gate.</li>
  <li>[PR #10543][0xFelix] Clear VM guest memory when ignoring inference failures</li>
  <li>[PR #9590][xuzhenglun] fix embed version info of virt-operator</li>
  <li>[PR #10532][alromeros] Add –volume-mode flag in image-upload</li>
  <li>[PR #10515][iholder101] Bug-fix: Stop copying VMI spec to VM during snapshots</li>
  <li>[PR #10320][victortoso] sidecar-shim implements PreCloudInitIso hook</li>
  <li>[PR #10463][0xFelix] VirtualMachines: Introduce InferFromVolumeFailurePolicy in Instancetype- and PreferenceMatchers</li>
  <li>[PR #10393][iholder101] [Bugfix] [Clone API] Double-cloning is now working as expected.</li>
  <li>[PR #10486][assafad] Deprecation notice for the metrics listed in the PR. Please update your systems to use the new metrics names.</li>
  <li>[PR #10438][lyarwood] A new <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io:view</code> <code class="language-plaintext highlighter-rouge">ClusterRole</code> has been introduced that can be bound to users via a <code class="language-plaintext highlighter-rouge">ClusterRoleBinding</code> to provide read only access to the cluster scoped <code class="language-plaintext highlighter-rouge">VirtualMachineCluster{Instancetype,Preference}</code> resources.</li>
  <li>[PR #10477][jean-edouard] Dynamic KSM enabling and configuration</li>
  <li>[PR #10110][tiraboschi] Stream guest serial console logs from a dedicated container</li>
  <li>[PR #10015][victortoso] Implements USB host passthrough in permittedHostDevices of KubeVirt CRD</li>
  <li>[PR #10184][acardace] Add memory hotplug feature</li>
  <li>[PR #10044][machadovilaca] Add operator-observability package</li>
  <li>[PR #10489][maiqueb] Remove the network-attachment-definition <code class="language-plaintext highlighter-rouge">list</code> and <code class="language-plaintext highlighter-rouge">watch</code> verbs from virt-controller’s RBAC</li>
  <li>[PR #10450][0xFelix] virtctl: Enable inference in create vm subcommand by default</li>
  <li>[PR #10447][fossedihelm] Add a Feature Gate to KV CR to automatically set memory limits when a resource quota with memory limits is associated to the creation namespace</li>
  <li>[PR #10253][rmohr] Stop trying to create unused directory /var/run/kubevirt-ephemeral-disk in virt-controller</li>
  <li>[PR #10231][kvaps] Propogate public-keys to cloud-init NoCloud meta-data</li>
  <li>[PR #10400][alromeros] Add new vmexport flags to download raw images, either directly (–raw) or by decompressing (–decompress) them</li>
  <li>[PR #9673][germag] DownwardMetrics: Expose DownwardMetrics through virtio-serial channel.</li>
  <li>[PR #10086][vladikr] allow live updating VM affinity and node selector</li>
  <li>[PR #10050][victortoso] Updating the virt stack: QEMU 8.0.0, libvirt to 9.5.0, edk2 20230524,</li>
  <li>[PR #10370][benjx1990] N/A</li>
  <li>[PR #10391][awels] BugFix: VMExport now works in a namespace with quotas defined.</li>
  <li>[PR #10386][liuzhen21] KubeSphere added to the adopter’s file!</li>
  <li>[PR #10380][alromeros] Bugfix: Allow image-upload to recover from PendingPopulation phase</li>
  <li>[PR #10366][ormergi] Kubevirt now delegates Slirp networking configuration to Slirp network binding plugin.  In case you haven’t registered Slirp network binding plugin image yet (i.e.: specify in Kubevirt config) the following default image would be used: <code class="language-plaintext highlighter-rouge">quay.io/kubevirt/network-slirp-binding:20230830_638c60fc8</code>. On next release (v1.2.0) no default image will be set and registering an image would be mandatory.</li>
  <li>[PR #10167][0xFelix] virtctl: Apply namespace to created manifests</li>
  <li>[PR #10148][alromeros] Add port-forward functionalities to vmexport</li>
  <li>[PR #9821][sradco] Deprecation notice for the metrics listed in the PR. Please update your systems to use the new metrics names.</li>
  <li>[PR #10272][ormergi] Introduce network binding plugin for Slirp networking, interfacing with Kubevirt new network binding plugin API.</li>
  <li>[PR #10284][AlonaKaplan] Introduce an API for network binding plugins. The feature is behind “NetworkBindingPlugins” gate.</li>
  <li>[PR #10275][awels] Ensure new hotplug attachment pod is ready before deleting old attachment pod</li>
  <li>[PR #9231][victortoso] Introduces sidecar-shim container image</li>
  <li>[PR #10254][rmohr] Don’t mark the KubeVirt “Available” condition as false on up-to-date and ready but misscheduled virt-handler pods.</li>
  <li>[PR #10185][AlonaKaplan] Add support to migration based SRIOV hotplug.</li>
  <li>[PR #10182][iholder101] Stop considering nodes without <code class="language-plaintext highlighter-rouge">kubevirt.io/schedulable</code> label when finding lowest TSC frequency on the cluster</li>
  <li>[PR #10138][machadovilaca] Change kubevirt_vmi_*_usage_seconds from Gauge to Counter</li>
  <li>[PR #10173][rmohr]</li>
  <li>[PR #10101][acardace] Deprecate <code class="language-plaintext highlighter-rouge">spec.config.machineType</code> in KubeVirt CR.</li>
  <li>[PR #10020][akalenyu] Use auth API for DataVolumes, stop importing kubevirt.io/containerized-data-importer</li>
  <li>[PR #10107][PiotrProkop] Expose kubevirt_vmi_vcpu_delay_seconds_total reporting amount of seconds VM spent in  waiting in the queue instead of running.</li>
  <li>[PR #10099][iholder101] Bugfix: target virt-launcher pod hangs when migration is cancelled.</li>
  <li>[PR #10056][jean-edouard] UEFI guests now use Bochs display instead of VGA emulation</li>
  <li>[PR #10070][machadovilaca] Remove affinities label from kubevirt_vmi_cpu_affinity and use sum as value</li>
  <li>[PR #10165][awels] BugFix: deleting hotplug attachment pod will no longer detach volumes that were not removed.</li>
  <li>[PR #9878][jean-edouard] The EFI NVRAM can now be configured to persist across reboots</li>
  <li>[PR #9932][lyarwood] <code class="language-plaintext highlighter-rouge">ControllerRevisions</code> containing <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io</code> <code class="language-plaintext highlighter-rouge">CRDs</code> are now decorated with labels detailing specific metadata of the underlying stashed object</li>
  <li>[PR #10039][simonyangcj] fix guaranteed qos of virt-launcher pod broken when use virtiofs</li>
  <li>[PR #10116][ormergi] Existing detached interfaces with ‘absent’ state will be cleared from VMI spec.</li>
  <li>[PR #9982][fabiand] Introduce a support lifecycle and Kubernetes target version.</li>
  <li>[PR #10118][akalenyu] Change exportserver default UID to succeed exporting CDI standalone PVCs (not attached to VM)</li>
  <li>[PR #10106][acardace] Add boot-menu wait time when starting the VM as paused.</li>
  <li>[PR #10058][alicefr] Add field errorPolicy for disks</li>
  <li>[PR #10004][AlonaKaplan] Hoyplug/unplug interfaces should be done by updating the VM spec template. virtctl and REST API endpoints were removed.</li>
  <li>[PR #10067][iholder101] Bug fix: <code class="language-plaintext highlighter-rouge">virtctl create clone</code> marshalling and replacement of <code class="language-plaintext highlighter-rouge">kubectl</code> with <code class="language-plaintext highlighter-rouge">kubectl virt</code></li>
  <li>[PR #9989][alaypatel07] Add perf scale benchmarks for VMIs</li>
  <li>[PR #10001][machadovilaca] Fix kubevirt_vmi_phase_count not being created</li>
  <li>[PR #9896][ormergi] The VM controller now replicates spec interfaces MAC addresses to the corresponding interfaces in the VMI spec.</li>
  <li>[PR #9840][dhiller] Increase probability for flake checker script to find flakes</li>
  <li>[PR #9988][enp0s3] always deploy the outdated VMI workload alert</li>
  <li>[PR #7708][VirrageS] <code class="language-plaintext highlighter-rouge">nodeSelector</code> and <code class="language-plaintext highlighter-rouge">schedulerName</code> fields have been added to VirtualMachineInstancetype spec.</li>
  <li>[PR #7197][vasiliy-ul] Experimantal support of SEV attestation via the new API endpoints</li>
  <li>[PR #9958][AlonaKaplan] Disable network interface hotplug/unplug for VMIs. It will be supported for VMs only.</li>
  <li>[PR #9882][dhiller] Add some context for initial contributors about automated testing and draft pull requests.</li>
  <li>[PR #9935][xpivarc] Bug fix - correct logging in container disk</li>
  <li>[PR #9552][phoracek] gRPC client now works correctly with non-Go gRPC servers</li>
  <li>[PR #9918][ShellyKa13] Fix for hotplug with WFFC SCI storage class which uses CDI populators</li>
  <li>[PR #9737][AlonaKaplan] On hotunplug - remove bridge, tap and dummy interface from virt-launcher and the caches (file and volatile) from the node.</li>
  <li>[PR #9861][rmohr] Fix the possibility of data corruption when requestin a force-restart via “virtctl restart”</li>
  <li>[PR #9818][akrejcir] Added “virtctl credentials” commands to dynamically change SSH keys in a VM, and to set user’s password.</li>
  <li>[PR #9872][alromeros] Bugfix: Allow lun disks to be mapped to DataVolume sources</li>
  <li>[PR #9073][machadovilaca] Fix incorrect KubevirtVmHighMemoryUsage description</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.1.0 changes]]></summary></entry><entry><title type="html">Running KubeVirt with Cluster Autoscaler</title><link href="https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes.html" rel="alternate" type="text/html" title="Running KubeVirt with Cluster Autoscaler" /><published>2023-09-06T00:00:00+00:00</published><updated>2023-09-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes</id><content type="html" xml:base="https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>For this article, we’ll learn about the process of setting up
<a href="https://kubevirt.io/">KubeVirt</a> with <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">Cluster
Autoscaler</a>
on EKS. In addition, we’ll be using bare metal nodes to host KubeVirt VMs.</p>

<h2 id="required-base-knowledge">Required Base Knowledge</h2>

<p>This article will talk about how to make various software systems work together
but introducing each one in detail is outside of its scope. Thus, you must already:</p>

<ol>
  <li>Know how to administer a Kubernetes cluster;</li>
  <li>Be familiar with AWS, specifically IAM and EKS; and</li>
  <li>Have some experience with KubeVirt.</li>
</ol>

<h2 id="companion-code">Companion Code</h2>

<p>All the code used in this article may also be found at
<a href="https://github.com/relaxdiego/kubevirt-cas-baremetal">github.com/relaxdiego/kubevirt-cas-baremetal</a>.</p>

<h2 id="set-up-the-cluster">Set Up the Cluster</h2>

<h3 id="shared-environment-variables">Shared environment variables</h3>

<p>First let’s set some environment variables:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The name of the EKS cluster we're going to create</span>
<span class="nb">export </span><span class="nv">RD_CLUSTER_NAME</span><span class="o">=</span>my-cluster

<span class="c"># The region where we will create the cluster</span>
<span class="nb">export </span><span class="nv">RD_REGION</span><span class="o">=</span>us-west-2

<span class="c"># Kubernetes version to use</span>
<span class="nb">export </span><span class="nv">RD_K8S_VERSION</span><span class="o">=</span>1.27

<span class="c"># The name of the keypair that we're going to inject into the nodes. You</span>
<span class="c"># must create this ahead of time in the correct region.</span>
<span class="nb">export </span><span class="nv">RD_EC2_KEYPAIR_NAME</span><span class="o">=</span>eks-my-cluster
</code></pre></div></div>

<h3 id="prepare-the-clusteryaml-file">Prepare the cluster.yaml file</h3>

<p>Using <a href="https://eksctl.io/">eksctl</a>, prepare an EKS cluster config:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eksctl create cluster <span class="se">\</span>
    <span class="nt">--dry-run</span> <span class="se">\</span>
    <span class="nt">--name</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--nodegroup-name</span> ng-infra <span class="se">\</span>
    <span class="nt">--node-type</span> m5.xlarge <span class="se">\</span>
    <span class="nt">--nodes</span> 2 <span class="se">\</span>
    <span class="nt">--nodes-min</span> 2 <span class="se">\</span>
    <span class="nt">--nodes-max</span> 2 <span class="se">\</span>
    <span class="nt">--node-labels</span> <span class="nv">workload</span><span class="o">=</span>infra <span class="se">\</span>
    <span class="nt">--region</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--ssh-access</span> <span class="se">\</span>
    <span class="nt">--ssh-public-key</span> <span class="k">${</span><span class="nv">RD_EC2_KEYPAIR_NAME</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--version</span> <span class="k">${</span><span class="nv">RD_K8S_VERSION</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--vpc-nat-mode</span> HighlyAvailable <span class="se">\</span>
    <span class="nt">--with-oidc</span> <span class="se">\</span>
<span class="o">&gt;</span> cluster.yaml
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">--dry-run</code> means the command will not actually create the cluster but will
instead output a config to stdout which we then write to <code class="language-plaintext highlighter-rouge">cluster.yaml</code>.</p>

<p>Open the file and look at what it has produced.</p>

<blockquote>
  <p>For more info on the schema used by <code class="language-plaintext highlighter-rouge">cluster.yaml</code>, see the <a href="https://eksctl.io/usage/schema/">Config file
schema</a> page from eksctl.io</p>
</blockquote>

<p>This cluster will start out with a node group that we will use to host our
“infra” services. This is why we are using the cheaper <code class="language-plaintext highlighter-rouge">m5.xlarge</code> rather than
a baremetal instance type. However, we also need to ensure that none of our VMs
will ever be scheduled in these nodes. Thus we need to taint them. In the
generated <code class="language-plaintext highlighter-rouge">cluster.yaml</code> file, append the following taint to the only node
group in the <code class="language-plaintext highlighter-rouge">managedNodeGroups</code> list:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">managedNodeGroups</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">amiFamily</span><span class="pi">:</span> <span class="s">AmazonLinux2</span>
  <span class="s">...</span>
  <span class="na">taints</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">CriticalAddonsOnly</span>
      <span class="na">effect</span><span class="pi">:</span> <span class="s">NoSchedule</span>
</code></pre></div></div>

<h3 id="create-the-cluster">Create the cluster</h3>

<p>We can now create the cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eksctl create cluster <span class="nt">--config-file</span> cluster.yaml
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2023-08-20 07:59:14 [ℹ]  eksctl version ...
2023-08-20 07:59:14 [ℹ]  using region us-west-2 ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2a ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2b ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2c ...
...
2023-08-20 08:14:06 [ℹ]  kubectl command should work with ...
2023-08-20 08:14:06 [✔]  EKS cluster "my-cluster" in "us-west-2" is ready
</code></pre></div></div>

<p>Once the command is done, you should be able to query the the kube API. For
example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get nodes
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                      STATUS   ROLES    AGE     VERSION
ip-XXX.compute.internal   Ready    &lt;none&gt;   32m     v1.27.4-eks-2d98532
ip-YYY.compute.internal   Ready    &lt;none&gt;   32m     v1.27.4-eks-2d98532
</code></pre></div></div>

<h3 id="create-the-node-groups">Create the Node Groups</h3>

<p>As per <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">this section of the Cluster Autoscaler
docs</a>:</p>

<blockquote>
  <p>If you’re using Persistent Volumes, your deployment needs to run in the same
AZ as where the EBS volume is, otherwise the pod scheduling could fail if it
is scheduled in a different AZ and cannot find the EBS volume. To overcome
this, either use a single AZ ASG for this use case, or an ASG-per-AZ while
enabling <code class="language-plaintext highlighter-rouge">--balance-similar-node-groups</code>.</p>
</blockquote>

<p>Based on the above, we will create a node group for each of the availability
zones (AZs) that was declared in <code class="language-plaintext highlighter-rouge">cluster.yaml</code> so that the Cluster Autoscaler will
always bring up a node in the AZ where a VM’s EBS-backed PV is located.</p>

<p>To do that, we will first prepare a template that we can then feed to
<code class="language-plaintext highlighter-rouge">envsubst</code>. Save the following in <code class="language-plaintext highlighter-rouge">node-group.yaml.template</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="c1"># See: Config File Schema &lt;https://eksctl.io/usage/schema/&gt;</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">eksctl.io/v1alpha5</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterConfig</span>

<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">${RD_CLUSTER_NAME}</span>
  <span class="na">region</span><span class="pi">:</span> <span class="s">${RD_REGION}</span>

<span class="na">managedNodeGroups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
    <span class="na">amiFamily</span><span class="pi">:</span> <span class="s">AmazonLinux2</span>
    <span class="na">instanceType</span><span class="pi">:</span> <span class="s">c5.metal</span>
    <span class="na">availabilityZones</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">${EKS_AZ}</span>
    <span class="na">desiredCapacity</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">maxSize</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">minSize</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">alpha.eksctl.io/cluster-name</span><span class="pi">:</span> <span class="s">my-cluster</span>
      <span class="na">alpha.eksctl.io/nodegroup-name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
      <span class="na">workload</span><span class="pi">:</span> <span class="s">vm</span>
    <span class="na">privateNetworking</span><span class="pi">:</span> <span class="no">false</span>
    <span class="na">ssh</span><span class="pi">:</span>
      <span class="na">allow</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">publicKeyPath</span><span class="pi">:</span> <span class="s">${RD_EC2_KEYPAIR_NAME}</span>
    <span class="na">volumeSize</span><span class="pi">:</span> <span class="m">500</span>
    <span class="na">volumeIOPS</span><span class="pi">:</span> <span class="m">10000</span>
    <span class="na">volumeThroughput</span><span class="pi">:</span> <span class="m">750</span>
    <span class="na">volumeType</span><span class="pi">:</span> <span class="s">gp3</span>
    <span class="na">propagateASGTags</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">tags</span><span class="pi">:</span>
      <span class="na">alpha.eksctl.io/nodegroup-name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
      <span class="na">alpha.eksctl.io/nodegroup-type</span><span class="pi">:</span> <span class="s">managed</span>
      <span class="na">k8s.io/cluster-autoscaler/my-cluster</span><span class="pi">:</span> <span class="s">owned</span>
      <span class="na">k8s.io/cluster-autoscaler/enabled</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
      <span class="c1"># The following tags help CAS determine that this node group is able</span>
      <span class="c1"># to satisfy the label and resource requirements of the KubeVirt VMs.</span>
      <span class="c1"># See: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/label/kubevirt.io/schedulable</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
</code></pre></div></div>

<p>The last few tags bears additional emphasis. They are required because when a
virtual machine is created, it will have the following requirements:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">requests</span><span class="pi">:</span>
  <span class="na">devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>

<span class="na">nodeSelectors</span><span class="pi">:</span> <span class="s">kubevirt.io/schedulable=true</span>
</code></pre></div></div>

<p>However, at least when scaling from zero for the first time, CAS will have no
knowledge of this information unless the correct AWS tags are added to the node
group. This is why we have the following added to the managed node group’s
tags:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/label/kubevirt.io/schedulable</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
</code></pre></div></div>

<blockquote>
  <p>For more information on these tags, see <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup">Auto-Discovery
Setup</a>.</p>
</blockquote>

<h3 id="create-the-vm-node-groups">Create the VM Node Groups</h3>

<p>We can now create the node group:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yq .availabilityZones[] cluster.yaml <span class="nt">-r</span> | <span class="se">\</span>
    xargs <span class="nt">-I</span><span class="o">{}</span> bash <span class="nt">-c</span> <span class="s2">"
        export EKS_AZ={};
        envsubst &lt; node-group.yaml.template | </span><span class="se">\</span><span class="s2">
        eksctl create nodegroup --config-file -
    "</span>
</code></pre></div></div>

<h2 id="deploy-kubevirt">Deploy KubeVirt</h2>

<blockquote>
  <p>The following was adapted from <a href="https://kubevirt.io/quickstart_cloud/">KubeVirt quickstart with cloud
providers</a>.</p>
</blockquote>

<p>Deploy the KubeVirt operator:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/v1.0.0/kubevirt-operator.yaml
</code></pre></div></div>

<p>So that the operator will know how to deploy KubeVirt, let’s add the <code class="language-plaintext highlighter-rouge">KubeVirt</code>
resource:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: kubevirt.io/v1
kind: KubeVirt
metadata:
  name: kubevirt
  namespace: kubevirt
spec:
  certificateRotateStrategy: {}
  configuration:
    developerConfiguration:
      featureGates: []
  customizeComponents: {}
  imagePullPolicy: IfNotPresent
  workloadUpdateStrategy: {}
  infra:
    nodePlacement:
      nodeSelector:
        workload: infra
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
</span><span class="no">EOF
</span></code></pre></div></div>

<blockquote>
  <p>Notice how we are specifically configuring KubeVirt itself to tolerate the
<code class="language-plaintext highlighter-rouge">CriticalAddonsOnly</code> taint. This is so that the KubeVirt services themselves
can be scheduled in the infra nodes instead of the bare metal nodes which we
want to scale down to zero when there are no VMs.</p>
</blockquote>

<p>Wait until KubeVirt is in a <code class="language-plaintext highlighter-rouge">Deployed</code> state:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get <span class="nt">-n</span> kubevirt <span class="nt">-o</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.phase}"</span> <span class="se">\</span>
	kubevirt.kubevirt.io/kubevirt
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Deployed
</code></pre></div></div>

<p>Double check that all KubeVirt components are healthy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-n</span> kubevirt
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                 READY   STATUS    RESTARTS       AGE
pod/virt-api-674467958c-5chhj        1/1     Running   0              98d
pod/virt-api-674467958c-wzcmk        1/1     Running   0              5d
pod/virt-controller-6768977b-49wwb   1/1     Running   0              98d
pod/virt-controller-6768977b-6pfcm   1/1     Running   0              5d
pod/virt-handler-4hztq               1/1     Running   0              5d
pod/virt-handler-x98x5               1/1     Running   0              98d
pod/virt-operator-85f65df79b-lg8xb   1/1     Running   0              5d
pod/virt-operator-85f65df79b-rp8p5   1/1     Running   0              98d
</code></pre></div></div>

<h2 id="deploy-a-vm-to-test">Deploy a VM to test</h2>

<blockquote>
  <p>The following is copied from
<a href="https://kubevirt.io/user-guide/virtual_machines/accessing_virtual_machines/#static-ssh-public-key-injection-via-cloud-init">kubevirt.io</a>.</p>
</blockquote>

<p>First create a secret from your public key:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create secret generic my-pub-key <span class="nt">--from-file</span><span class="o">=</span><span class="nv">key1</span><span class="o">=</span>~/.ssh/id_rsa.pub
</code></pre></div></div>

<p>Next, create the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a VM referencing the Secret using propagation method configDrive</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: testvm
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          rng: {}
        resources:
          requests:
            memory: 1024M
      terminationGracePeriodSeconds: 0
      accessCredentials:
      - sshPublicKey:
          source:
            secret:
              secretName: my-pub-key
          propagationMethod:
            configDrive: {}
      volumes:
      - containerDisk:
          image: quay.io/containerdisks/fedora:latest
        name: containerdisk
      - cloudInitConfigDrive:
          userData: |-
            #cloud-config
            password: fedora
            chpasswd: { expire: False }
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Check that the test VM is running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get vm
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME        AGE     STATUS               READY
testvm      30s     Running              True
</code></pre></div></div>

<p>Delete the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl delete testvm
</code></pre></div></div>

<h2 id="set-up-cluster-autoscaler">Set Up Cluster Autoscaler</h2>

<h3 id="prepare-the-permissions-for-cluster-autoscaler">Prepare the permissions for Cluster Autoscaler</h3>

<p>So that CAS can set the desired capacity of each node group dynamically, we
must grant it limited access to certain AWS resources. The first step to this
is to define the IAM policy.</p>

<blockquote>
  <p>This section is based off of the “Create an IAM policy and role” section of
the <a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html">AWS
Autoscaling</a>
documentation.</p>
</blockquote>

<h3 id="create-the-cluster-specific-policy-document">Create the cluster-specific policy document</h3>

<p>Prepare the policy document by rendering the following file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> policy.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "autoscaling:SetDesiredCapacity",
                "autoscaling:TerminateInstanceInAutoScalingGroup"
            ],
            "Resource": "*"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeAutoScalingInstances",
                "autoscaling:DescribeAutoScalingGroups",
                "ec2:DescribeLaunchTemplateVersions",
                "autoscaling:DescribeTags",
                "autoscaling:DescribeLaunchConfigurations",
                "ec2:DescribeInstanceTypes"
            ],
            "Resource": "*"
        }
    ]
}
</span><span class="no">EOF
</span></code></pre></div></div>

<p>The above should be enough for CAS to do its job. Next, create the policy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws iam create-policy <span class="se">\</span>
    <span class="nt">--policy-name</span> eks-<span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span>-<span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span><span class="nt">-ClusterAutoscalerPolicy</span> <span class="se">\</span>
    <span class="nt">--policy-document</span> file://policy.json
</code></pre></div></div>

<blockquote>
  <p>IMPORTANT: Take note of the returned policy ARN. You will need that below.</p>
</blockquote>

<h3 id="create-the-iam-role-and-k8s-service-account-pair">Create the IAM role and k8s service account pair</h3>

<p>The Cluster Autoscaler needs a service account in the k8s cluster that’s
associated with an IAM role that consumes the policy document we created in the
previous section. This is normally a two-step process but can be created in a
single command using <code class="language-plaintext highlighter-rouge">eksctl</code>:</p>

<blockquote>
  <p>For more information on what <code class="language-plaintext highlighter-rouge">eksctl</code> is doing under the covers, see <a href="https://eksctl.io/usage/iamserviceaccounts/#how-it-works">How It
Works</a> from the
<code class="language-plaintext highlighter-rouge">eksctl</code> documentation for IAM Roles for Service Accounts.</p>
</blockquote>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RD_POLICY_ARN</span><span class="o">=</span><span class="s2">"&lt;Get this value from the last command's output&gt;"</span>

eksctl create iamserviceaccount <span class="se">\</span>
	<span class="nt">--cluster</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--region</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--namespace</span><span class="o">=</span>kube-system <span class="se">\</span>
	<span class="nt">--name</span><span class="o">=</span>cluster-autoscaler <span class="se">\</span>
	<span class="nt">--attach-policy-arn</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_POLICY_ARN</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--override-existing-serviceaccounts</span> <span class="se">\</span>
	<span class="nt">--approve</span>
</code></pre></div></div>

<p>Double check that the <code class="language-plaintext highlighter-rouge">cluster-autoscaler</code> service account has been correctly
annotated with the IAM role that was created by <code class="language-plaintext highlighter-rouge">eksctl</code> in the same step:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get sa cluster-autoscaler <span class="nt">-n</span> kube-system <span class="nt">-ojson</span> | <span class="se">\</span>
	jq <span class="nt">-r</span> <span class="s1">'.metadata.annotations | ."eks.amazonaws.com/role-arn"'</span>
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>arn:aws:iam::365499461711:role/eksctl-my-cluster-addon-iamserviceaccount-...
</code></pre></div></div>

<p>Check from the AWS Console if the above role contains the policy that we created
earlier.</p>

<h3 id="deploy-cluster-autoscaler">Deploy Cluster Autoscaler</h3>

<p>First, find the most recent Cluster Autoscaler version that has the same MAJOR
and MINOR version as the kubernetes cluster you’re deploying to.</p>

<p>Get the kube cluster’s version:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl version <span class="nt">-ojson</span> | jq <span class="nt">-r</span> .serverVersion.gitVersion
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v1.27.4-eks-2d98532
</code></pre></div></div>

<p>Choose the appropriate version for CAS. You can get the latest Cluster
Autoscaler versions from its <a href="https://github.com/kubernetes/autoscaler/releases?q=cluster-autoscaler+1&amp;expanded=true">Github Releases
Page</a>.</p>

<p>Example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">CLUSTER_AUTOSCALER_VERSION</span><span class="o">=</span>1.27.3
</code></pre></div></div>

<p>Next, deploy the cluster autoscaler using the deployment template that I
prepared in the <a href="https://github.com/relaxdiego/kubevirt-cas-baremetal">companion
repo</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>envsubst &lt; &lt;<span class="o">(</span>curl https://raw.githubusercontent.com/relaxdiego/kubevirt-cas-baremetal/main/cas-deployment.yaml.template<span class="o">)</span> | <span class="se">\</span>
  kubectl apply <span class="nt">-f</span> -
</code></pre></div></div>

<p>Check the cluster autoscaler status:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get deploy,pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>cluster-autoscaler <span class="nt">-n</span> kube-system
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cluster-autoscaler   1/1     1            1           4m1s

NAME                                      READY   STATUS    RESTARTS   AGE
pod/cluster-autoscaler-6c58bd6d89-v8wbn   1/1     Running   0          60s
</code></pre></div></div>

<p>Tail the <code class="language-plaintext highlighter-rouge">cluster-autoscaler</code> pod’s logs to see what’s happening:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> kube-system logs <span class="nt">-f</span> deployment.apps/cluster-autoscaler
</code></pre></div></div>

<p>Below are example log entries from Cluster Autoscaler terminating an unneeded
node:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>node ip-XXXX.YYYY.compute.internal may be removed
...
ip-XXXX.YYYY.compute.internal was unneeded for 1m3.743475455s
</code></pre></div></div>

<p>Once the timeout has been reached (default: 10 minutes), CAS will scale down
the group:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scale-down: removing empty node ip-XXXX.YYYY.compute.internal
Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"kube-system", ...
Successfully added ToBeDeletedTaint on node ip-XXXX.YYYY.compute.internal
Terminating EC2 instance: i-ZZZZ
DeleteInstances was called: ...
</code></pre></div></div>

<blockquote>
  <p>For more information on how Cluster Autoscaler scales down a node group, see
<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work">How does scale-down
work?</a>
from the project’s FAQ.</p>
</blockquote>

<p>When you try to get the list of nodes, you should see the bare metal nodes
tainted such that they are no longer schedulable:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME       STATUS                     ROLES    AGE    VERSION
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready                      &lt;none&gt;   112m   v1.27.3-eks-a5565ad
ip-XXXX    Ready                      &lt;none&gt;   112m   v1.27.3-eks-a5565ad
</code></pre></div></div>

<p>In a few more minutes, the nodes will be deleted.</p>

<p>To try the scale up, just deploy a VM.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Expanding Node Group eks-ng-eacf8ebb ...
Best option to resize: eks-ng-eacf8ebb
Estimated 1 nodes needed in eks-ng-eacf8ebb
Final scale-up plan: [{eks-ng-eacf8ebb 0-&gt;1 (max: 3)}]
Scale-up: setting group eks-ng-eacf8ebb size to 1
Setting asg eks-ng-eacf8ebb size to 1
</code></pre></div></div>

<h2 id="done">Done</h2>

<p>At this point you should have a working, auto-scaling EKS cluster that can host
VMs on bare metal nodes. If you have any questions, ask them
<a href="https://github.com/relaxdiego/relaxdiego.github.com/discussions/new?category=general">here</a>.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html">Amazon EKS Autoscaling</a></li>
  <li><a href="https://aws.plainenglish.io/cluster-autoscaler-amazon-eks-7ffaa24e5938">Cluster Autoscaler in Plain English</a></li>
  <li><a href="https://aws.github.io/aws-eks-best-practices/">AWS EKS Best Practices Guide</a></li>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html">IAM roles for service accounts</a></li>
  <li><a href="https://eksctl.io/usage/iamserviceaccounts/">eksctl create iamserviceaccount</a></li>
</ul>]]></content><author><name>Mark Maglana, Jonathan Kinred, Paul Myjavec</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="Cluster Autoscaler" /><category term="AWS" /><category term="EKS" /><summary type="html"><![CDATA[This post explains how to set up KubeVirt with Cluster Autoscaler on EKS]]></summary></entry><entry><title type="html">Managing KubeVirt VMs with Ansible</title><link href="https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible.html" rel="alternate" type="text/html" title="Managing KubeVirt VMs with Ansible" /><published>2023-09-05T00:00:00+00:00</published><updated>2023-09-05T00:00:00+00:00</updated><id>https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible</id><content type="html" xml:base="https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Infrastructure teams managing virtual machines (VMs) and the end users of these systems make use of a variety of tools as part of their day-to-day world. One such tool that is shared amongst these two groups is Ansible, an agentless automation tool for the enterprise. To simplify both the adoption and usage of KubeVirt as well as to integrate seamlessly into existing workflows, the KubeVirt community is excited to introduce the release of the first version of the KubeVirt collection for <a href="https://docs.ansible.com/ansible/latest/index.html">Ansible</a>, called <code class="language-plaintext highlighter-rouge">kubevirt.core</code>, which includes a number of tools that you do not want to miss.</p>

<p>This article will review some of the features and their use associated with this initial release.</p>

<p>Note: There is also a video version of this blog, which can be found on the <a href="https://youtu.be/GVROaPgJD_8">KubeVirt YouTube channel</a>.</p>

<h2 id="motivation">Motivation</h2>

<p>Before diving into the featureset of the collection itself, let’s review why the collection was created in the first place.</p>

<p>While adopting KubeVirt and Kubernetes has the potential to disrupt the workflows of teams that typically manage VM infrastructure, including the end users themselves, many of the same paradigms remain:</p>

<ul>
  <li>Kubernetes and the resources associated with KubeVirt can be represented in a declarative fashion.</li>
  <li>In many cases, communicating with KubeVirt VMs makes use of the same protocols and schemes as non-Kubernetes-based environments.</li>
  <li>The management of VMs still represents a challenge.</li>
</ul>

<p>For these reasons and more, it is only natural that a tool, like Ansible, is introduced within the KubeVirt community. Not only can it help manage KubeVirt and Kubernetes resources, like <code class="language-plaintext highlighter-rouge">VirtualMachines</code>, but also to enable the extensive Ansible ecosystem for managing guest configurations.</p>

<h2 id="included-capabilities">Included capabilities</h2>

<p>As part of the initial release, an <a href="https://docs.ansible.com/ansible/latest/plugins/inventory.html">Ansible Inventory plugin</a> and management module is included. They are available in the same distribution location containing Ansible automation content, <a href="https://galaxy.ansible.com/kubevirt/core">Ansible Galaxy</a>. The resources encompassing the collection itself are detailed in the following sections.</p>

<h3 id="inventory">Inventory</h3>

<p>To work with KubeVirt VMs in Ansible, they need to be available in Ansible’s hosts <a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html">inventory</a>. Since KubeVirt is already using the Kubernetes API to manage VMs, it would be nice to leverage this API to discover hosts with Ansible too. This is where the <a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_dynamic_inventory.html">dynamic inventory</a> of the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection comes into play.</p>

<p>The dynamic inventory capability allows you to query the Kubernetes API for available VMs in a given namespace or namespaces, along with additional filtering options, such as labels. To allow Ansible to find the right connection parameters for a VM, the network name of a secondary interface can also be specified.</p>

<p>Under the hood, the dynamic inventory uses either your default kubectl credentials or credentials specified in the inventory parameters to establish the connection with a cluster.</p>

<h3 id="managing-vms">Managing VMs</h3>

<p>While working with existing VMs is already quite useful, it would be even better to control the entire lifecycle of KubeVirt <code class="language-plaintext highlighter-rouge">VirtualMachines</code> from Ansible. This is made possible by the <code class="language-plaintext highlighter-rouge">kubevirt_vm</code> module provided by the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection.</p>

<p>The <code class="language-plaintext highlighter-rouge">kubevirt_vm</code> module is a thin wrapper around the <a href="https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html">kubernetes.core.k8s</a> module and it allows you to control the essential fields of a KubeVirt <code class="language-plaintext highlighter-rouge">VirtualMachine</code>’s specification. In true Ansible fashion, this module tries to be as idempotent as possible and only makes changes to objects within Kubernetes if necessary. With its <code class="language-plaintext highlighter-rouge">wait</code> feature, it is possible to delay further tasks until a VM was successfully created or updated and the VM is in the ready state or was successfully deleted.</p>

<h2 id="getting-started">Getting started</h2>

<p>Now that we’ve provided an introduction to the featureset, it is time to illustrate how you can get up to speed using the collection including a few examples to showcase the capabilities provided by the collection.</p>

<h3 id="prerequisites">Prerequisites</h3>

<p>Please note that as a prerequisite, Ansible needs to be installed and configured along with a working Kubernetes cluster with KubeVirt and the <a href="https://github.com/kubevirt/cluster-network-addons-operator">KubeVirt Cluster Network Addons Operator</a>. The cluster also needs to have a <a href="https://kubevirt.io/user-guide/virtual_machines/interfaces_and_networks/#bridge">secondary network configured</a>, which can be attached to VMs so that the machine can be reached from the Ansible control node.</p>

<h3 id="items-covered">Items covered</h3>

<ol>
  <li>Installing the collection from Ansible Galaxy</li>
  <li>Creating a Namespace and a Secret with an SSH public key</li>
  <li>Creating a VM</li>
  <li>Listing available VMs</li>
  <li>Executing a command on the VM</li>
  <li>Removing the previously created resources</li>
</ol>

<h3 id="walkthrough">Walkthrough</h3>

<p>First, install the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection from Ansible Galaxy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-galaxy collection <span class="nb">install </span>kubevirt.core
</code></pre></div></div>

<p>This will also install the <code class="language-plaintext highlighter-rouge">kubernetes.core</code> collection as a dependency.</p>

<p>Second, create a new Namespace and a Secret containing a public key for SSH authentication:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-f</span> my-key
kubectl create namespace kubevirt-ansible
kubectl create secret generic my-pub-key <span class="nt">--from-file</span><span class="o">=</span><span class="nv">key1</span><span class="o">=</span>my-key.pub <span class="nt">-n</span> kubevirt-ansible
</code></pre></div></div>

<p>With the collection now installed and the public key pair created, create a file called <code class="language-plaintext highlighter-rouge">play-create.yml</code> containing an Ansible playbook to deploy a new VM called <code class="language-plaintext highlighter-rouge">testvm</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span> <span class="s">localhost</span>
  <span class="na">connection</span><span class="pi">:</span> <span class="s">local</span>
  <span class="na">tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Create VM</span>
    <span class="na">kubevirt.core.kubevirt_vm</span><span class="pi">:</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">present</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">testvm</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">test</span>
      <span class="na">instancetype</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">u1.medium</span>
      <span class="na">preference</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
      <span class="na">spec</span><span class="pi">:</span>
        <span class="na">domain</span><span class="pi">:</span>
          <span class="na">devices</span><span class="pi">:</span>
            <span class="na">interfaces</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
              <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">secondary-network</span>
              <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">networks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
          <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">secondary-network</span>
          <span class="na">multus</span><span class="pi">:</span>
            <span class="na">networkName</span><span class="pi">:</span> <span class="s">secondary-network</span>
        <span class="na">accessCredentials</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">sshPublicKey</span><span class="pi">:</span>
            <span class="na">source</span><span class="pi">:</span>
              <span class="na">secret</span><span class="pi">:</span>
                <span class="na">secretName</span><span class="pi">:</span> <span class="s">my-pub-key</span>
            <span class="na">propagationMethod</span><span class="pi">:</span>
              <span class="na">configDrive</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/containerdisks/fedora:latest</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
        <span class="pi">-</span> <span class="na">cloudInitConfigDrive</span><span class="pi">:</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s"># The default username is: fedora</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
      <span class="na">wait</span><span class="pi">:</span> <span class="s">yes</span>
</code></pre></div></div>

<p>Run the playbook by executing the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook play-create.yml
</code></pre></div></div>

<p>Once the playbook completes successfully, the defined VM will be running in the <code class="language-plaintext highlighter-rouge">kubevirt-ansible</code> namespace, which can be confirmed by querying for <code class="language-plaintext highlighter-rouge">VirtualMachines</code> in this namespace:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get VirtualMachine <span class="nt">-n</span> kubevirt-ansible
</code></pre></div></div>

<p>With the VM deployed, it is eligible for use in Ansible automation activities. Let’s illustrate how it can be queried and added to an Ansible inventory dynamically using the plugin provided by the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection.</p>

<p>Create a file called <code class="language-plaintext highlighter-rouge">inventory.kubevirt.yml</code> containing the following content:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">plugin</span><span class="pi">:</span> <span class="s">kubevirt.core.kubevirt</span>
<span class="na">connections</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">namespaces</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">kubevirt-ansible</span>
  <span class="na">network_name</span><span class="pi">:</span> <span class="s">secondary-network</span>
  <span class="na">label_selector</span><span class="pi">:</span> <span class="s">app=test</span>
</code></pre></div></div>

<p>Use the <code class="language-plaintext highlighter-rouge">ansible-inventory</code> command to confirm the VM becomes added to the Ansible inventory:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-inventory <span class="nt">-i</span> inventory.kubevirt.yml <span class="nt">--list</span>
</code></pre></div></div>

<p>Next, make use of the host by querying for all of the facts exposed by the VM using the setup module:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible <span class="nt">-i</span> inventory.kubevirt.yml <span class="nt">-u</span> fedora <span class="nt">--key-file</span> my-key all <span class="nt">-m</span> setup
</code></pre></div></div>

<p>Complete the lifecycle of the VM by destroying the previously created <code class="language-plaintext highlighter-rouge">VirtualMachine</code> and <code class="language-plaintext highlighter-rouge">Namespace</code>. Create a file called <code class="language-plaintext highlighter-rouge">play-delete.yml</code> containing the following playbook:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span> <span class="s">localhost</span>
  <span class="na">tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Delete VM</span>
    <span class="na">kubevirt.core.kubevirt_vm</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">testvm</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">absent</span>
      <span class="na">wait</span><span class="pi">:</span> <span class="s">yes</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Delete namespace</span>
    <span class="na">kubernetes.core.k8s</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">api_version</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">kind</span><span class="pi">:</span> <span class="s">Namespace</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">absent</span>
</code></pre></div></div>

<p>Run the playbook to remove the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook play-delete.yml
</code></pre></div></div>

<p>More information including the full list of parameters and options can be found within the collection documentation:</p>

<p><a href="https://kubevirt.io/kubevirt.core">https://kubevirt.io/kubevirt.core</a></p>

<h2 id="what-next">What next?</h2>

<p>This has been a brief introduction to the concepts and usage of the newly released <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection. Nevertheless, we hope that it helped to showcase the integration now available between KubeVirt and Ansible, including how easy it is to manage KubeVirt assets. A next potential iteration could be to expose a VM via a Kubernetes <code class="language-plaintext highlighter-rouge">Service</code> using one of the methods described in <a href="https://kubevirt.io/user-guide/virtual_machines/service_objects/#service-objects">this article</a> instead of a secondary interface as was covered in this walkthrough. Not only does it leverage existing models outside the KubeVirt ecosystem, but it helps to enable a uniform method for exposing content.</p>

<p>Interested in learning more, providing feedback or contributing? Head over to the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> GitHub repository to continue your journey and get involved.</p>

<p><a href="https://github.com/kubevirt/kubevirt.core">https://github.com/kubevirt/kubevirt.core</a></p>]]></content><author><name>Felix Matouschek, Andrew Block</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="Ansible" /><category term="ansible collection" /><category term="kubevirt.core" /><category term="iac" /><summary type="html"><![CDATA[This post explains how to manage KubeVirt VMs with the kubevirt.core Ansible collection.]]></summary></entry><entry><title type="html">NetworkPolicies for KubeVirt VMs secondary networks using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies.html" rel="alternate" type="text/html" title="NetworkPolicies for KubeVirt VMs secondary networks using OVN-Kubernetes" /><published>2023-07-24T00:00:00+00:00</published><updated>2023-07-24T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">NetworkPolicies</a> are constructs to control traffic flow at the IP
address or port level (OSI layers 3 or 4).
They allow the user to specify how a pod (or group of pods) is allowed to
communicate with other entities on the network. In simpler words: the user can
specify ingress from or egress to other workloads, using L3 / L4 semantics.</p>

<p>Keeping in mind <code class="language-plaintext highlighter-rouge">NetworkPolicy</code> is a Kubernetes construct - which only cares
about a single network interface - they are only usable for the cluster’s
default network interface. This leaves a considerable gap for Virtual Machine
users, since they are heavily invested in secondary networks.</p>

<p>The <a href="https://github.com/k8snetworkplumbingwg">k8snetworkplumbingwg</a> has addressed this limitation by providing a
<code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> CRD - it features the exact same API as <code class="language-plaintext highlighter-rouge">NetworkPolicy</code>
but can target <a href="https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/how-to-use.md#create-network-attachment-definition">network-attachment-definitions</a>.
<a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a> implements this API, and configures access control accordingly
for secondary networks in the cluster.</p>

<p>In this post we will see how we can govern access control for VMs using the
multi-network policy API. On our simple example, we’ll only allow into our VMs
for traffic ingressing from a particular CIDR range.</p>

<h2 id="current-limitations-of-multinetworkpolicies-for-vms">Current limitations of <code class="language-plaintext highlighter-rouge">MultiNetworkPolicies</code> for VMs</h2>
<p>Kubernetes <code class="language-plaintext highlighter-rouge">NetworkPolicy</code> has three types of policy peers:</p>
<ul>
  <li>namespace selectors: allows ingress-from, egress-to based on the peer’s namespace labels</li>
  <li>pod selectors: allows ingress-from, egress-to based on the peer’s labels</li>
  <li>ip block: allows ingress-from, egress-to based on the peer’s IP address</li>
</ul>

<p>While <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> allows these three types, when used with VMs we
recommend using <strong>only</strong> the <code class="language-plaintext highlighter-rouge">IPBlock</code> policy peer - both <code class="language-plaintext highlighter-rouge">namespace</code> and <code class="language-plaintext highlighter-rouge">pod</code>
selectors prevent the live-migration of Virtual Machines (these policy peers
require OVN-K managed IPAM, and currently the live-migration feature is only
available when IPAM is not enabled on the interfaces).</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, we will prepare a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li><a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a></li>
  <li><a href="https://github.com/kubevirt/kubevirt">KubeVirt</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multi-networkpolicy">Multi-Network policy API</a></li>
</ul>

<p>The <a href="#setup-demo-environment">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
upstream latest multus-cni, and the multi-network policy CRDs deployed.</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster (one control plane, and two worker
nodes), configured to use OVN-Kubernetes as the default cluster network,
configuring the multi-homing OVN-Kubernetes feature gate, and deploying
<code class="language-plaintext highlighter-rouge">multus-cni</code> in the cluster.</p>

<h3 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h3>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v1.0.0).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="limiting-ingress-to-a-kubevirt-vm">Limiting ingress to a KubeVirt VM</h3>
<p>In this example, we will configure a <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> allowing ingress into
our VMs only from a particular CIDR range - let’s say <code class="language-plaintext highlighter-rouge">10.200.0.0/30</code>.</p>

<p>Provision the following NAD (to allow our VMs to live-migrate, we do not define
a <code class="language-plaintext highlighter-rouge">subnet</code>):</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2net</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.4.0",</span>
            <span class="s">"name": "flatl2net",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology":"layer2",</span>
            <span class="s">"netAttachDefName": "default/flatl2net"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p>Let’s now provision our six VMs, with the following name to IP address
(statically configured via cloud-init) association:</p>
<ul>
  <li>vm1: 10.200.0.1</li>
  <li>vm2: 10.200.0.2</li>
  <li>vm3: 10.200.0.3</li>
  <li>vm4: 10.200.0.4</li>
  <li>vm5: 10.200.0.5</li>
  <li>vm6: 10.200.0.6</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm1</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm1</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm1</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.1/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm2</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm2</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm2</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.2/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm3</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm3</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm3</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.3/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm4</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm4</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm4</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm4</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.4/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm5</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm5</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm5</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm5</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.5/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm6</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm6</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm6</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm6</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.6/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
</code></pre></div></div>
<p><strong>NOTE:</strong> it is important to highlight all the Virtual Machines (and the
<code class="language-plaintext highlighter-rouge">network-attachment-definition</code>) are defined in the <code class="language-plaintext highlighter-rouge">default</code> namespace.</p>

<p>After this step, we should have the following deployment:</p>

<p><img src="/assets/2023-07-10-OVN-kubernetes-secondary-networks-policies/01-vms-provisioned.png" alt="image" /></p>

<p>Let’s check the VMs <code class="language-plaintext highlighter-rouge">vm1</code> and <code class="language-plaintext highlighter-rouge">vm4</code> can ping their peers in the same subnet.
For that we will
<a href="https://kubevirt.io/user-guide/virtual_machines/accessing_virtual_machines/#accessing-the-serial-console">connect to the VMs over their serial console</a>:</p>

<p>First, let’s check <code class="language-plaintext highlighter-rouge">vm1</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  virtctl console vm1
Successfully connected to vm1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.2 <span class="nt">-c</span> 4
PING 10.200.0.2 <span class="o">(</span>10.200.0.2<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>5.16 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.41 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>34.2 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>2.56 ms

<span class="nt">---</span> 10.200.0.2 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3005ms
rtt min/avg/max/mdev <span class="o">=</span> 1.406/10.841/34.239/13.577 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.77 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.46 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>5.47 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.74 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3007ms
rtt min/avg/max/mdev <span class="o">=</span> 1.459/3.109/5.469/1.627 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>
</code></pre></div></div>

<p>And from vm4:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  ~ virtctl console vm4
Successfully connected to vm4 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.1 <span class="nt">-c</span> 4
PING 10.200.0.1 <span class="o">(</span>10.200.0.1<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.20 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.62 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.44 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.951 ms

<span class="nt">---</span> 10.200.0.1 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 0.951/1.803/3.201/0.843 ms
<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.85 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.02 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.27 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.970 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3005ms
rtt min/avg/max/mdev <span class="o">=</span> 0.970/1.275/1.850/0.350 ms
</code></pre></div></div>

<p>We will now provision a <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> applying to all the VMs defined
above. To do this mapping correcly, the policy has to:</p>
<ul>
  <li>Be in the same namespace as the VM.</li>
  <li>Set <code class="language-plaintext highlighter-rouge">k8s.v1.cni.cncf.io/policy-for</code> annotation matching the secondary 
network used by the VM.</li>
  <li>Set <code class="language-plaintext highlighter-rouge">matchLabels</code> selector matching the labels set on VM’s
<code class="language-plaintext highlighter-rouge">spec.template.metadata</code>.</li>
</ul>

<p>This policy will allow ingress into these <code class="language-plaintext highlighter-rouge">access-control</code> labeled pods 
<strong>only if</strong> the traffic originates from within the <code class="language-plaintext highlighter-rouge">10.200.0.0/30</code> CIDR range
(IPs 10.200.0.1-3).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">---</span>
apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name:  ingress-ipblock
  annotations:
    k8s.v1.cni.cncf.io/policy-for: default/flatl2net
spec:
  podSelector:
    matchLabels:
        name: access-control
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.200.0.0/30
</code></pre></div></div>

<p>Taking into account our example, only
<code class="language-plaintext highlighter-rouge">vm1</code>, <code class="language-plaintext highlighter-rouge">vm2</code>, and <code class="language-plaintext highlighter-rouge">vm3</code> will be able to contact any of its peers, as pictured
by the following diagram:</p>

<p><img src="/assets/2023-07-10-OVN-kubernetes-secondary-networks-policies/02-no-access.png" alt="MultiNetworkPolicy is provisioned" /></p>

<p>Let’s try again the ping after provisioning the <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> object:</p>

<p>From <code class="language-plaintext highlighter-rouge">vm1</code> (inside the allowed ip block range):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.2 <span class="nt">-c</span> 4
PING 10.200.0.2 <span class="o">(</span>10.200.0.2<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>6.48 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>4.40 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.28 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.51 ms

<span class="nt">---</span> 10.200.0.2 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 1.283/3.418/6.483/2.154 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.81 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>2.67 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.68 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.63 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 1.630/2.446/3.808/0.888 ms
</code></pre></div></div>

<p>From <code class="language-plaintext highlighter-rouge">vm4</code> (<strong>outside</strong> the allowed ip block range):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.1 <span class="nt">-c</span> 4
PING 10.200.0.1 <span class="o">(</span>10.200.0.1<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.

<span class="nt">---</span> 10.200.0.1 ping statistics <span class="nt">---</span>
4 packets transmitted, 0 received, 100% packet loss, <span class="nb">time </span>3083ms

<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 0 received, 100% packet loss, <span class="nb">time </span>3089ms
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>
<p>In this post we’ve shown how <code class="language-plaintext highlighter-rouge">MultiNetworkPolicies</code> can be used to provide
access control to VMs with secondary network interfaces.</p>

<p>We have provided a comprehensive example on how a policy can be used to limit
ingress to our VMs only from desired sources, based on the client’s IP address.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><category term="NetworkPolicy" /><summary type="html"><![CDATA[This post explains how to configure NetworkPolicies for KubeVirt VMs secondary networks.]]></summary></entry><entry><title type="html">KubeVirt v1.0 has landed!</title><link href="https://kubevirt.io//2023/KubeVirt-v1-has-landed.html" rel="alternate" type="text/html" title="KubeVirt v1.0 has landed!" /><published>2023-07-11T00:00:00+00:00</published><updated>2023-07-11T00:00:00+00:00</updated><id>https://kubevirt.io//2023/KubeVirt-v1-has-landed</id><content type="html" xml:base="https://kubevirt.io//2023/KubeVirt-v1-has-landed.html"><![CDATA[<p>The KubeVirt community is proud to announce the release of <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.0.0">KubeVirt v1.0</a>! This release demonstrates the accomplishments of the community and user adoption over the years and represents an important milestone for everyone involved.</p>

<h2 id="a-brief-history">A brief history</h2>
<p>The KubeVirt project started in Red Hat at the end of 2016, with the question: Can virtual machines (VMs) run in containers and be deployed by Kubernetes?
It proved to be not only possible, but quickly emerged as a promising solution to the future of virtual machines in the container age.
KubeVirt joined the <a href="https://www.cncf.io/">CNCF</a> as a Sandbox project in September 2019, and an Incubating project in April 2022.
From a handful of people hacking away on a proof of concept, KubeVirt has grown into 45 active repositories, with the primary <a href="https://github.com/kubevirt/kubevirt">kubevirt/kubevirt</a> repo having 17k commits and 1k forks.</p>

<h2 id="what-does-v10-mean-to-the-community">What does v1.0 mean to the community?</h2>
<p>The v1.0 release signifies the incredible growth that the community has gone through in the past six years from an idea to a production-ready Virtual Machine Management solution. The next stage with v1.0 is the additional focus on maintaining APIs while continuing to grow the project. This has led KubeVirt to adopt community practices from Kubernetes in key parts of the project.</p>

<p>Leading up to this release we had a shift in release cadence: from monthly to 3 times a year, following the Kubernetes release model. This allows our developer community additional time to ensure stability and compatibility, our users more time to plan and comfortably upgrade, and also aligns our releases with Kubernetes to simplify maintenance and supportability.</p>

<p>The theme ‘aligning with Kubernetes’ is also felt through the other parts of the community, by following their governance processes; introducing SIGs to split test and review responsibilities, as well as a SIG release repo to handle everything related to a release; and regular <a href="https://calendar.google.com/calendar/u/0/embed?src=kubevirt@cncf.io">SIG meetings</a> that now include SIG scale and performance and SIG storage alongside our weekly Community meetings.</p>

<h2 id="whats-included-in-this-release">What’s included in this release?</h2>

<p>This release demonstrates the accomplishments of the community and user adoption over the past many months. The full list of feature and bug fixes can be found in our <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.0.0">release notes</a>, but we’ve also asked representatives from some of our SIGs for a summary.</p>

<h3 id="sig-scale">SIG-scale</h3>
<p>KubeVirt’s SIG-scale drives the performance and scalability initiatives in the community. Our focus for the v1.0 release was on sharing the performance results over the past 6 months. The benchmarks since December 2022 which cover the past two release - v0.59 (Mar 2023) and v1.0 (July 2023) are as follows:</p>

<p><a href="https://github.com/kubevirt/kubevirt/blob/release-1.0/docs/release-v1-perf-scale-benchmarks.md#performance-benchmarks-for-v1-release">Performance benchmarks for v1.0 release</a></p>

<p><a href="https://github.com/kubevirt/kubevirt/blob/release-1.0/docs/release-v1-perf-scale-benchmarks.md#scalability-benchmarks-for-v1-release">Scalability benchmarks for v1.0 release</a></p>

<p>Publishing these measurements provides the community and end-users visibility into the performance and scalability over multiple releases. In addition, these results help identify the effects of code changes so that community members can diagnose performance problems and regressions.</p>

<p>End-users can use the same tools and techniques SIG-scale uses to analyze performance and scalability in their own deployments. Since performance and scalability are mostly relative to the deployment stack, the same strategies should be used to further contextualize the community’s measurements.</p>

<h3 id="sig-storage">SIG-storage</h3>
<p>SIG-storage is focused on providing persistent storage to KubeVirt VMs and managing that storage throughout the lifecycle of the VM. This begins with provisioning and populating PVCs with bootable images but also includes features such as disk hotplug, snapshots, backup and restore, disaster recovery, and virtual machine export.</p>

<p>For v1.0, SIG-storage delivered the following features: providing a flexible VM export API, enabling persistent SCSI reservation, provisioning VMs from a retained snapshot, and setting out-of-the-box defaults for additional storage provisioners. Another major effort was to implement Volume Populator alternatives to the KubeVirt DataVolume API in order to better leverage platform capabilities. The SIG meets every 2 weeks and welcomes anyone to join us for interesting storage discussions.</p>

<h3 id="sig-compute">SIG-compute</h3>
<p>SIG-compute is focused on the core virtualization functionality of KubeVirt, but also encompasses features that don’t fit well into another SIG. Some examples of SIG-compute’s scope include the lifecycle of VMs, migration, as well as maintenance of the core API.</p>

<p>For v1.0, SIG-compute developed features for memory over-commit. This includes initial support for KSM and FreePageReporting. We added support for persistent vTPM, which makes it much easier to use BitLocker on Windows installs. Additionally, there’s now an initial implementation for CPU Hotplug (currently hidden behind a feature gate).</p>

<h3 id="sig-network">SIG-network</h3>
<p>SIG-network is committed to enhancing and maintaining all aspects of Virtual Machine network connectivity and management in KubeVirt.</p>

<p>For the v1.0 release, we have introduced hot plug and hot unplug (as alpha), which enables users to add and remove VM secondary network interfaces that use bridge binding on a running VM. Hot plug API stabilization and support for SR-IOV interfaces is under development for the next minor release.</p>

<h3 id="sig-infra">SIG-infra</h3>
<p>The effort to simplify the VirtualMachine UX is still ongoing and with the v1.0 release we were able to introduce the v1beta1 version of the instancetype.kubevirt.io API. In the future KubeVirt v1.1.0 release we are aiming to finally graduate the instancetype.kubevirt.io API to v1.</p>

<p>With the new version it is now possible to control the memory overcommit of virtual machines as a percentage within instance types. Resource requirements were added to preferences, which allows users to ensure that requirements of a workload are met. Also several new preference attributes have been added to cover more use cases.</p>

<p>Moreover, virtctl was extended to make use of the new instance type and preference features.</p>

<h2 id="what-next-for-kubevirt">What next for KubeVirt?</h2>
<p>From a development perspective, we will continue to introduce and improve features that make life easier for virtualization users in a manner that is as native to Kubernetes as possible. From a community perspective, we are improving our new contributor experience so that we can continue to grow and help new members learn and be a part of the cloud native ecosystem. In addition, with this milestone we can now shift our attention on becoming a CNCF Graduated project.</p>]]></content><author><name>KubeVirt Maintainers</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.0" /><category term="release" /><category term="community" /><category term="cncf" /><category term="milestone" /><category term="party time" /><summary type="html"><![CDATA[We are very pleased to announce the release of KubeVirt v1.0!]]></summary></entry><entry><title type="html">KubeVirt v1.0.0</title><link href="https://kubevirt.io//2023/changelog-v1.0.0.html" rel="alternate" type="text/html" title="KubeVirt v1.0.0" /><published>2023-07-06T00:00:00+00:00</published><updated>2023-07-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/changelog-v1.0.0</id><content type="html" xml:base="https://kubevirt.io//2023/changelog-v1.0.0.html"><![CDATA[<h2 id="v100">v1.0.0</h2>

<p>Released on: Thu Jul 6 17:39:42 2023 +0000</p>

<ul>
  <li>[PR #10037][kubevirt-bot] The VM controller now replicates spec interfaces MAC addresses to the corresponding interfaces in the VMI spec.</li>
  <li>[PR #9992][machadovilaca] Fix incorrect KubevirtVmHighMemoryUsage description</li>
  <li>[PR #9965][kubevirt-bot] Disable network interface hotplug/unplug for VMIs. It will be supported for VMs only.</li>
  <li>[PR #9931][kubevirt-bot] Fix for hotplug with WFFC SCI storage class which uses CDI populators</li>
  <li>[PR #9946][kubevirt-bot] On hotunplug - remove bridge, tap and dummy interface from virt-launcher and the caches (file and volatile) from the node.</li>
  <li>[PR #9757][enp0s3] Introduce CPU hotplug</li>
  <li>[PR #9811][machadovilaca] Remove unnecessary marketplace tool</li>
  <li>[PR #7742][Fuzzy-Math] Experimental support for AMD SEV-ES</li>
  <li>[PR #9799][vladikr] Introduce an ability to set memory overcommit percentage in instanceType spec</li>
  <li>[PR #8780][lyarwood] Add basic support for expressing minimum resource requirements for CPU and Memory within VirtualMachine{Preferences,ClusterPreferences}</li>
  <li>[PR #9812][mhenriks] Handle DataVolume PendingPopulation phase</li>
  <li>[PR #9858][fossedihelm] build virtctl for all os/architectures when <code class="language-plaintext highlighter-rouge">KUBEVIRT_RELEASE</code> env var is true</li>
  <li>[PR #9765][lyarwood] Allow to define preferred cpu features in VirtualMachine{Preferences,ClusterPreferences}</li>
  <li>[PR #9844][EdDev] Drop the <code class="language-plaintext highlighter-rouge">kubevirt.io/interface</code> resource name API for reserving domain resources for network interfaces.</li>
  <li>[PR #9841][ormergi] Support hot-unplug of network interfaces on VirtualMachine objects</li>
  <li>[PR #9851][lxs137] virt-api: portfowrad can handle IPv6 VM</li>
  <li>[PR #9845][lxs137] DHCPv6 server handle request without iana option</li>
  <li>[PR #9769][lyarwood] Allow to define the preferred subdomain in VirtualMachine{Preferences,ClusterPreferences}</li>
  <li>[PR #9246][jean-edouard] Fixed migration issue for VMIs that have RWX disks backed by filesystem storage classes.</li>
  <li>[PR #9808][jcanocan] DownwardMetrics: Rename AllocatedToVirtualServers metric to AllocatedToVirtualServers and add ResourceProcessorLimit metric</li>
  <li>[PR #9832][tiraboschi] build virtctl also for arm64 for linux, darwin and windows</li>
  <li>[PR #9744][lyarwood] Allow to define the preferred termination grace period in VirtualMachine{Preferences,ClusterPreferences}</li>
  <li>[PR #9828][rthallisey] Publish multiarch manifests with each release</li>
  <li>[PR #9761][lyarwood] Allow to define the preferred masquerade configuration in VirtualMachine{Preferences,ClusterPreferences}</li>
  <li>[PR #9768][jean-edouard] New CR option to enable auto CPU limits for virt-launcher on some namespaces</li>
  <li>[PR #9779][EdDev] Support hot-unplug of network interfaces on VMI objects</li>
  <li>[PR #9688][xpivarc] Users are warned about the usage of deprecated fields</li>
  <li>[PR #9798][rmohr] Add LiveMigrateIfPossible eviction strategy to allow admins to express a live migration preference instead of a live migration requirement for evictions.</li>
  <li>[PR #9764][fossedihelm] Cluster admins can enable ksm in a set of nodes via kv configuration</li>
  <li>[PR #9753][lyarwood] The following flags have been added to the <code class="language-plaintext highlighter-rouge">virtctl image-upload</code> command allowing users to associate a default instance type and/or preference with an image during upload. <code class="language-plaintext highlighter-rouge">--default-instancetype</code>,  <code class="language-plaintext highlighter-rouge">--default-instancetype-kind</code>, <code class="language-plaintext highlighter-rouge">--default-preference</code> and <code class="language-plaintext highlighter-rouge">--default-preference-kind</code>. <a href="https://kubevirt.io/user-guide/virtual_machines/instancetypes/#inferfromvolume">See the user-guide documentation</a> for more details on using the uploaded image with the <code class="language-plaintext highlighter-rouge">inferFromVolume</code> feature during <code class="language-plaintext highlighter-rouge">VirtualMachine</code> creation.</li>
  <li>[PR #9575][lyarwood] A new <code class="language-plaintext highlighter-rouge">v1beta1</code> version of the <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io</code> API and CRDs has been introduced.</li>
  <li>[PR #9738][Barakmor1] Add condition to migrations that indicates that migration was rejected by ResourceQuota</li>
  <li>[PR #9730][assafad] Add <code class="language-plaintext highlighter-rouge">kubevirt_vmi_memory_cached_bytes</code> metric</li>
  <li>[PR #9674][fossedihelm] Introduce cluster configuration <code class="language-plaintext highlighter-rouge">VirtualMachineOptions</code> to specify virtual machine behavior at cluster level</li>
  <li>[PR #9724][0xFelix] An alert which triggers when KubeVirt APIs marked as deprecated are used was added.</li>
  <li>[PR #9623][rmohr] Bump to apimachinery 1.26</li>
  <li>[PR #9747][lyarwood] action required - With the <code class="language-plaintext highlighter-rouge">v1.0.0</code> release of KubeVirt the storage version of all core <code class="language-plaintext highlighter-rouge">kubevirt.io</code> APIs will be moving to version <code class="language-plaintext highlighter-rouge">v1</code>. To accommodate the eventual removal of the <code class="language-plaintext highlighter-rouge">v1alpha3</code> version with KubeVirt &gt;=<code class="language-plaintext highlighter-rouge">v1.2.0</code> it is recommended that operators deploy the <a href="https://github.com/kubernetes-sigs/kube-storage-version-migrator"><code class="language-plaintext highlighter-rouge">kube-storage-version-migrator</code></a> tool within their environment. This will ensure any existing <code class="language-plaintext highlighter-rouge">v1alpha3</code> stored objects are migrated to <code class="language-plaintext highlighter-rouge">v1</code> well in advance of the removal of the underlying <code class="language-plaintext highlighter-rouge">v1alpha3</code> version.</li>
  <li>[PR #9268][ormergi] virt-launcher pods network interfaces name scheme is changed to hashed names (SHA256), based on the VMI spec network names.</li>
  <li>[PR #9746][EdDev] Introduce the <code class="language-plaintext highlighter-rouge">kubevirt.io/interface</code> resource name to reserve domain resources for network interfaces.</li>
  <li>[PR #9652][machadovilaca] Add kubevirt_number_of_vms recording rule</li>
  <li>[PR #9691][fossedihelm] ksm enabled nodes will have <code class="language-plaintext highlighter-rouge">kubevirt.io/ksm-enabled</code> label</li>
  <li>[PR #9628][lyarwood] * The <code class="language-plaintext highlighter-rouge">kubevirt.io/v1</code> <code class="language-plaintext highlighter-rouge">apiVersion</code> is now the default storage version for newly created objects</li>
  <li>[PR #8293][daghaian] Add multi-arch support to KubeVirt. This allows a single KubeVirt installation to run VMs on different node architectures in the same cluster.</li>
  <li>[PR #9686][maiqueb] Fix ownership of macvtap’s char devices on non-root pods</li>
  <li>[PR #9631][0xFelix] virtctl: Allow to infer instancetype or preference from specified volume when creating VMs</li>
  <li>[PR #9665][rmohr] Expose the final resolved qemu machine type on the VMI on status.machine</li>
  <li>[PR #9609][germag] Add support for running virtiofsd in an unprivileged container when sharing configuration volumes.</li>
  <li>[PR #9651][0xFelix] virtctl: Allow to specify memory of created VMs. Default to 512Mi if no instancetype was specified or is inferred.</li>
  <li>[PR #9640][jean-edouard] TSC-enabled VMs can now migrate to a node with a non-identical (but close-enough) frequency</li>
  <li>[PR #9629][0xFelix] virtctl: Allow to specify the boot order of volumes when creating VMs</li>
  <li>[PR #9632][toelke] * Add Genesis Cloud to the adopters list</li>
  <li>[PR #9572][fossedihelm] Enable freePageReporting for new non high performance vmi</li>
  <li>[PR #9435][rmohr] Ensure existence of all PVCs attached to the VMI before creating the VM target pod.</li>
  <li>[PR #8156][jean-edouard] TPM VM device can now be set to persistent</li>
  <li>[PR #8575][iholder101] QEMU-level migration parallelism (a.k.a. multifd) + Upgrade QEMU to 7.2.0-11.el9</li>
  <li>[PR #9603][qinqon] Adapt node-labeller.sh script to work at non kvm envs with emulation.</li>
  <li>[PR #9591][awels] BugFix: allow multiple NFS disks to be used/hotplugged</li>
  <li>[PR #9596][iholder101] Add “virtctl create clone” command</li>
  <li>[PR #9422][awels] Ability to specify cpu/mem request limit for supporting containers (hotplug/container disk/virtiofs/side car)</li>
  <li>[PR #9536][akalenyu] BugFix: virtualmachineclusterinstancetypes/preferences show up for get all -n <namespace></namespace></li>
  <li>[PR #9177][alicefr] Adding SCSI persistent reservation</li>
  <li>[PR #9470][machadovilaca] Enable libvirt GetDomainStats on paused VMs</li>
  <li>[PR #9407][assafad] Use env <code class="language-plaintext highlighter-rouge">RUNBOOK_URL_TEMPLATE</code> for the runbooks URL template</li>
  <li>[PR #9399][maiqueb] Compute the interfaces to be hotplugged based on the current domain info, rather than on the interface status.</li>
  <li>[PR #9491][orelmisan] API, AddInterfaceOptions: Rename NetworkName to NetworkAttachmentDefinitionName and InterfaceName to Name</li>
  <li>[PR #9327][jcanocan] DownwardMetrics: Swap KubeVirt build info with qemu version in VirtProductInfo field</li>
  <li>[PR #9478][xpivarc] Bug fix: Fixes case when migration is not retried if the migration Pod gets denied.</li>
  <li>[PR #9421][lyarwood] Requests to update the target <code class="language-plaintext highlighter-rouge">Name</code> of a <code class="language-plaintext highlighter-rouge">{Instancetype,Preference}Matcher</code> without also updating the <code class="language-plaintext highlighter-rouge">RevisionName</code> are now rejected.</li>
  <li>[PR #9367][machadovilaca] Add VM instancetype and preference label to vmi_phase_count metric</li>
  <li>[PR #9392][awels] virtctl supports retrieving vm manifest for VM export</li>
  <li>[PR #9442][EdDev] Remove the VMI Status interface <code class="language-plaintext highlighter-rouge">podConfigDone</code> field in favor of a new source option in <code class="language-plaintext highlighter-rouge">infoSource</code>.</li>
  <li>[PR #9376][ShellyKa13] Fix vmrestore with WFFC snapshotable storage class</li>
  <li>[PR #6852][maiqueb] Dev preview: Enables network interface hotplug for VMs / VMIs</li>
  <li>[PR #9300][xpivarc] Bug fix: API and virtctl invoked migration is not rejected when the VM is paused</li>
  <li>[PR #9189][xpivarc] Bug fix: DNS integration continues to work after migration</li>
  <li>[PR #9322][iholder101] Add guest-to-request memory headroom ratio.</li>
  <li>[PR #8906][machadovilaca] Alert if there are no available nodes to run VMs</li>
  <li>[PR #9320][darfux] node-labeller: Check arch on the handler side</li>
  <li>[PR #9127][fossedihelm] Use ECDSA instead of RSA for key generation</li>
  <li>[PR #9330][qinqon] Skip label kubevirt.io/migrationTargetNodeName from virtctl expose service selector</li>
  <li>[PR #9163][vladikr] fixes the requests/limits CPU number mismatch for VMs with isolatedEmulatorThread</li>
  <li>[PR #9250][vladikr] externally created mediated devices will not be deleted by virt-handler</li>
  <li>[PR #9193][qinqon] Add annotation for live migration and bridged pod interface</li>
  <li>[PR #9260][ShellyKa13] Fix bug of possible re-trigger of memory dump</li>
  <li>[PR #9241][akalenyu] BugFix: Guestfs image url not constructed correctly</li>
  <li>[PR #9220][orelmisan] client-go: Added context to VirtualMachine’s methods.</li>
  <li>[PR #9228][rumans] Bump virtiofs container limit</li>
  <li>[PR #9169][lyarwood] The <code class="language-plaintext highlighter-rouge">dedicatedCPUPlacement</code> attribute is once again supported within the <code class="language-plaintext highlighter-rouge">VirtualMachineInstancetype</code> and <code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code> CRDs after a recent bugfix improved <code class="language-plaintext highlighter-rouge">VirtualMachine</code> validations, ensuring defaults are applied before any attempt to validate.</li>
  <li>[PR #9159][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 9.0.0 and QEMU 7.2.0.</li>
  <li>[PR #8989][rthallisey] Integrate multi-architecture container manifests into the bazel make recipes</li>
  <li>[PR #9188][awels] Default RBAC for clone and export</li>
  <li>[PR #9145][awels] Show VirtualMachine name in the VMExport status</li>
  <li>[PR #8937][fossedihelm] Added foreground finalizer to  virtual machine</li>
  <li>[PR #9133][ShellyKa13] Fix addvolume not rejecting adding existing volume source, fix removevolume allowing to remove non hotpluggable volume</li>
  <li>[PR #9047][machadovilaca] Deprecate VM stuck in status alerts</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.0.0 changes]]></summary></entry><entry><title type="html">Secondary networks connected to the physical underlay for KubeVirt VMs using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet.html" rel="alternate" type="text/html" title="Secondary networks connected to the physical underlay for KubeVirt VMs using OVN-Kubernetes" /><published>2023-05-31T00:00:00+00:00</published><updated>2023-05-31T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>OVN (Open Virtual Network) is a series of daemons for the Open vSwitch that
translate virtual network configurations into OpenFlow. It provides virtual
networking capabilities for any type of workload on a virtualized platform
(virtual machines and containers) using the same API.</p>

<p>OVN provides a higher-layer of abstraction than Open vSwitch, working with
logical routers and logical switches, rather than flows.
More details can be found in the OVN architecture
<a href="https://man7.org/linux/man-pages/man7/ovn-architecture.7.html#DESCRIPTION">man page</a>.</p>

<p>In this post we will repeat the scenario of
<a href="https://kubevirt.io/2020/Multiple-Network-Attachments-with-bridge-CNI.html">its bridge CNI equivalent</a>,
using this SDN approach. This secondary network topology is akin to the one
described in the <a href="http://kubevirt.io/2023/OVN-kubernetes-secondary-networks.html">flatL2 topology</a>,
but allows connectivity to the physical underlay.</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, we will prepare a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li><a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a></li>
  <li><a href="https://github.com/kubevirt/kubevirt">KubeVirt</a></li>
</ul>

<p>The <a href="#environment-setup">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
and upstream latest multus-cni deployed.</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster, configured to use OVN-Kubernetes as
the default cluster network, configuring the multi-homing OVN-Kubernetes feature
gate, and deploying
<a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a> in the cluster.</p>

<h3 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h3>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v0.59.0).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="single-broadcast-domain">Single broadcast domain</h3>
<p>In this scenario we will see how traffic from a single localnet network can be
connected to a physical network in the host using a dedicated bridge.</p>

<p>This scenario does not use any VLAN encapsulation, thus is simpler, since the
network admin does not need to provision any VLANs in advance.</p>

<h4 id="configuring-the-underlay">Configuring the underlay</h4>
<p>When you’ve started the KinD cluster with the <code class="language-plaintext highlighter-rouge">--multi-network-enable</code> flag an
additional OCI network was created, and attached to each of the KinD nodes.</p>

<p>But still, further steps may be required, depending on the desired L2
configuration.</p>

<p>Let’s first create a dedicated OVS bridge, and attach the aforementioned
virtualized network to it:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>node <span class="k">in</span> <span class="si">$(</span>kubectl <span class="nt">-n</span> ovn-kubernetes get pods <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ovs-node <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.name}"</span><span class="si">)</span>
<span class="k">do
	</span>kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-br ovsbr1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-port ovsbr1 eth1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nb">set </span>open <span class="nb">.</span> external_ids:ovn-bridge-mappings<span class="o">=</span>physnet:breth0,localnet-network:ovsbr1
<span class="k">done</span>
</code></pre></div></div>

<p>The first two commands are self-evident: you create an OVS bridge, and attach
a port to it; the last one is not. In it, we’re using the
<a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.0/html/networking_guide/bridge-mappings">OVN bridge mapping</a>
API to configure which OVS bridge must be used for each physical network.
It creates a patch port between the OVN integration bridge - <code class="language-plaintext highlighter-rouge">br-int</code> - and the
OVS bridge you tell it to, and traffic will be forwarded to/from it with the
help of a
<a href="https://man7.org/linux/man-pages/man5/ovn-nb.5.html#Logical_Switch_Port_TABLE">localnet port</a>.</p>

<p><strong>NOTE:</strong> The provided mapping <strong>must</strong> match the <code class="language-plaintext highlighter-rouge">name</code> within the
<code class="language-plaintext highlighter-rouge">net-attach-def</code>.Spec.Config JSON, otherwise, the patch ports will not be
created.</p>

<p>You will also have to configure an IP address on the bridge for the
extra-network the kind script created. For that, you first need to identify the
bridge’s name. In the example below we’re providing a command for the podman
runtime:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>podman network inspect underlay <span class="nt">--format</span> <span class="s1">'{{ .NetworkInterface }}'</span>
podman3

ip addr add 10.128.0.1/24 dev podman3
</code></pre></div></div>

<p><strong>NOTE:</strong> for docker, please use the following command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip a | <span class="nb">grep</span> <span class="sb">`</span>docker network inspect underlay <span class="nt">--format</span> <span class="s1">'{{ index .IPAM.Config 0 "Gateway" }}'</span><span class="sb">`</span> | <span class="nb">awk</span> <span class="s1">'{print $NF}'</span>
br-0aeb0318f71f

ip addr add 10.128.0.1/24 dev br-0aeb0318f71f
</code></pre></div></div>

<p>Let’s also use an IP in the same subnet as the network subnet (defined in the
NAD). This IP address must be excluded from the IPAM pool (also on the NAD),
otherwise the OVN-Kubernetes IPAM may assign it to a workload.</p>

<h4 id="defining-the-ovn-kubernetes-networks">Defining the OVN-Kubernetes networks</h4>
<p>Once the underlay is configured, we can now provision the attachment configuration:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">localnet-network</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "localnet-network",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"subnets": "10.128.0.0/24",</span>
            <span class="s">"excludeSubnets": "10.128.0.1/32",</span>
            <span class="s">"netAttachDefName": "default/localnet-network"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p>It is required to list the gateway IP in the <code class="language-plaintext highlighter-rouge">excludedSubnets</code> attribute, thus
preventing OVN-Kubernetes from assigning that IP address to the workloads.</p>

<h4 id="spin-up-the-vms">Spin up the VMs</h4>
<p>These two VMs can be used for the single broadcast domain scenario (no VLANs).</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-server</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">localnet-network</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">dhcp4: true</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-client</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker2</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">localnet-network</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">dhcp4: true</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
</code></pre></div></div>

<h4 id="test-east--west-communication">Test East / West communication</h4>
<p>You can check east/west connectivity between both VMs via ICMP:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-server <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent, multus-status"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"10.128.0.2"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"10.128.0.2"</span>,
      <span class="s2">"fe80::e83d:16ff:fe76:c1bd"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"ea:3d:16:76:c1:bd"</span>,
    <span class="s2">"name"</span>: <span class="s2">"localnet"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-client
Successfully connected to vm-client console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ping 10.128.0.2
PING 10.128.0.2 <span class="o">(</span>10.128.0.2<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.128.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.808 ms
64 bytes from 10.128.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.478 ms
64 bytes from 10.128.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.536 ms
64 bytes from 10.128.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.507 ms

<span class="nt">---</span> 10.128.0.2 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3005ms
rtt min/avg/max/mdev <span class="o">=</span> 0.478/0.582/0.808/0.131 ms
</code></pre></div></div>

<h4 id="check-underlay-services">Check underlay services</h4>
<p>We can now start HTTP servers listening to the IPs attached on
the gateway:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 10.128.0.1 9000
</code></pre></div></div>

<p>And finally curl this from your client:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 10.128.0.1:9000
<span class="k">*</span>   Trying 10.128.0.1:9000...
<span class="k">*</span> Connected to 10.128.0.1 <span class="o">(</span>10.128.0.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 10.128.0.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:05:09 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
...
</code></pre></div></div>

<h3 id="multiple-physical-networks-pointing-to-the-same-ovs-bridge">Multiple physical networks pointing to the same OVS bridge</h3>
<p>This example will feature 2 physical networks, each with a different VLAN,
both pointing at the same OVS bridge.</p>

<h4 id="configuring-the-underlay-1">Configuring the underlay</h4>
<p>Again, the first thing to do is create a dedicated OVS bridge, and attach the
aforementioned virtualized network to it, while defining it as a trunk port
for two broadcast domains, with tags 10 and 20.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>node <span class="k">in</span> <span class="si">$(</span>kubectl <span class="nt">-n</span> ovn-kubernetes get pods <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ovs-node <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.name}"</span><span class="si">)</span>
<span class="k">do
	</span>kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-br ovsbr1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-port ovsbr1 eth1 <span class="nv">trunks</span><span class="o">=</span>10,20 <span class="nv">vlan_mode</span><span class="o">=</span>trunk
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nb">set </span>open <span class="nb">.</span> external_ids:ovn-bridge-mappings<span class="o">=</span>physnet:breth0,tenantblue:ovsbr1,tenantred:ovsbr1
<span class="k">done</span>
</code></pre></div></div>

<p>We must now configure the physical network; since the packets are leaving the
OVS bridge tagged with either the 10 or 20 VLAN, we must configure the physical
network where the virtualized nodes run to handle the tagged traffic.</p>

<p>For that we will create two VLANed interfaces, each with a different subnet; we
will need to know the name of the bridge the kind script created to implement
the extra network it required. Those VLAN interfaces also need to be configured
with an IP address: (for docker see previous example)</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>podman network inspect underlay <span class="nt">--format</span> <span class="s1">'{{ .NetworkInterface }}'</span>
podman3

<span class="c"># create the VLANs</span>
ip <span class="nb">link </span>add <span class="nb">link </span>podman3 name podman3.10 <span class="nb">type </span>vlan <span class="nb">id </span>10
ip addr add 192.168.123.1/24 dev podman3.10
ip <span class="nb">link set </span>dev podman3.10 up

ip <span class="nb">link </span>add <span class="nb">link </span>podman3 name podman3.20 <span class="nb">type </span>vlan <span class="nb">id </span>20
ip addr add 192.168.124.1/24 dev podman3.20
ip <span class="nb">link set </span>dev podman3.20 up
</code></pre></div></div>

<p><strong>NOTE:</strong> both the <code class="language-plaintext highlighter-rouge">tenantblue</code> and <code class="language-plaintext highlighter-rouge">tenantred</code> networks forward their traffic
to the <code class="language-plaintext highlighter-rouge">ovsbr1</code> OVS bridge.</p>

<h4 id="defining-the-ovn-kubernetes-networks-1">Defining the OVN-Kubernetes networks</h4>
<p>Let us now provision the attachment configuration for the two physical networks.
Notice they do not have a subnet defined, which means our workloads must
configure static IPs via cloud-init.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tenantred</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "tenantred",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"vlanID": 10,</span>
            <span class="s">"netAttachDefName": "default/tenantred"</span>
    <span class="s">}</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tenantblue</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "tenantblue",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"vlanID": 20,</span>
            <span class="s">"netAttachDefName": "default/tenantblue"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p><strong>NOTE:</strong> each of the <code class="language-plaintext highlighter-rouge">tenantblue</code> and <code class="language-plaintext highlighter-rouge">tenantred</code> networks tags their traffic
with a different VLAN, which must be listed on the port <code class="language-plaintext highlighter-rouge">trunks</code> configuration.</p>

<h4 id="spin-up-the-vms-1">Spin up the VMs</h4>
<p>These two VMs can be used for the OVS bridge sharing scenario (two physical
networks share the same OVS bridge, each on a different VLAN).</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-red-1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-red</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-red</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantred</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.123.10/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-red-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantred</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.123.20/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-blue-1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantblue</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.124.10/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-blue-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantblue</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.124.20/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
</code></pre></div></div>

<h4 id="test-east--west-communication-1">Test East / West communication</h4>
<p>You can check east/west connectivity between both <strong>red</strong> VMs via ICMP:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-red-2 <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.168.123.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.168.123.20"</span>,
      <span class="s2">"fe80::e83d:16ff:fe76:c1bd"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"ea:3d:16:76:c1:bd"</span>,
    <span class="s2">"name"</span>: <span class="s2">"flatl2-overlay"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-red-1
Successfully connected to vm-red-1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-red-1 ~]<span class="nv">$ </span>ping 192.168.123.20
PING 192.168.123.20 <span class="o">(</span>192.168.123.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.534 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.246 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.178 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.236 ms

<span class="nt">---</span> 192.168.123.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3028ms
rtt min/avg/max/mdev <span class="o">=</span> 0.178/0.298/0.534/0.138 ms
</code></pre></div></div>

<p>The same behavior can be seen on the VMs attached to the <strong>blue</strong> network:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-blue-2 <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.168.124.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.168.124.20"</span>,
      <span class="s2">"fe80::6cae:e4ff:fefc:bd02"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"6e:ae:e4:fc:bd:02"</span>,
    <span class="s2">"name"</span>: <span class="s2">"physnet-blue"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-blue-1
Successfully connected to vm-blue-1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-blue-1 ~]<span class="nv">$ </span>ping 192.168.124.20
PING 192.168.124.20 <span class="o">(</span>192.168.124.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.531 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.255 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.688 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.648 ms

<span class="nt">---</span> 192.168.124.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3047ms
rtt min/avg/max/mdev <span class="o">=</span> 0.255/0.530/0.688/0.169 ms
</code></pre></div></div>

<h3 id="accessing-the-underlay-services">Accessing the underlay services</h3>
<p>We can now start HTTP servers listening to the IPs attached on the VLAN
interfaces:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 192.168.123.1 9000 &amp;
python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 192.168.124.1 9000 &amp;
</code></pre></div></div>

<p>And finally curl this from your client (blue network):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-blue-1 ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 192.168.124.1:9000
<span class="k">*</span>   Trying 192.168.124.1:9000...
<span class="k">*</span> Connected to 192.168.124.1 <span class="o">(</span>192.168.124.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 192.168.124.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:05:09 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
...
</code></pre></div></div>

<p>And from the client connected to the red network:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-red-1 ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 192.168.123.1:9000
<span class="k">*</span>   Trying 192.168.123.1:9000...
<span class="k">*</span> Connected to 192.168.123.1 <span class="o">(</span>192.168.123.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 192.168.123.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:06:02 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
&lt; 
...
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>
<p>In this post we have seen how to use OVN-Kubernetes to create secondary
networks connected to the physical underlay, allowing both east/west
communication between VMs, and access to services running outside the
Kubernetes cluster.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><summary type="html"><![CDATA[This post explains how to configure secondary networks connected to the physical underlay for KubeVirt virtual machines.]]></summary></entry></feed>