<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2024-11-13T15:44:00+00:00</updated><id>https://kubevirt.io//feed.xml</id><title type="html">KubeVirt.io</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">KubeVirt v1.4.0</title><link href="https://kubevirt.io//2024/changelog-v1.4.0.html" rel="alternate" type="text/html" title="KubeVirt v1.4.0" /><published>2024-11-13T00:00:00+00:00</published><updated>2024-11-13T00:00:00+00:00</updated><id>https://kubevirt.io//2024/changelog-v1.4.0</id><content type="html" xml:base="https://kubevirt.io//2024/changelog-v1.4.0.html"><![CDATA[<h2 id="v140">v1.4.0</h2>

<p>Released on: Wed Nov 13 08:14:06 2024 +0000</p>

<ul>
  <li>[PR #13225][kubevirt-bot] backend-storage will now correctly use the default virtualization storage class</li>
  <li>[PR #13203][kubevirt-bot] BugFix: VMSnapshots broken on OpenShift</li>
  <li>[PR #13071][machadovilaca] Add kubevirt_vm_resource_limits metric</li>
  <li>[PR #13090][acardace] Allow live updating VMs’ tolerations</li>
  <li>[PR #12629][jean-edouard] backend-storage now supports RWO FS</li>
  <li>[PR #13086][lyarwood] A new <code class="language-plaintext highlighter-rouge">spec.configuration.instancetype.referencePolicy</code> configurable has been added to the <code class="language-plaintext highlighter-rouge">KubeVirt</code> CR with support for <code class="language-plaintext highlighter-rouge">reference</code> (default), <code class="language-plaintext highlighter-rouge">expand</code> and <code class="language-plaintext highlighter-rouge">expandAll</code> policies provided.</li>
  <li>[PR #13064][xpivarc] Fix cache corruption</li>
  <li>[PR #12967][xpivarc] BochsDisplayForEFIGuests is GAed, use  “kubevirt.io/vga-display-efi-x86” annotation on Kubevirt CR before upgrading in case you need retain compatibility.</li>
  <li>[PR #13078][qinqon] Add dynamic pod interface name feature gate</li>
  <li>[PR #13072][0xFelix] virtctl: virtctl create vm can now use the Access Credentials API to add credentials to a new VM</li>
  <li>[PR #13050][vamsikrishna-siddu] fix the cpu model issue for s390x.</li>
  <li>[PR #12802][machadovilaca] Add kubevirt_vmi_status_addresses metric</li>
  <li>[PR #13027][awels] BugFix: Stop creating tokenSecretRef when no volumes to export</li>
  <li>[PR #13001][awels] Relaxed check on modify VM spec during VM snapshot to only check disks/volumes</li>
  <li>[PR #13082][kubevirt-bot] Updated common-instancetypes bundles to v1.2.0</li>
  <li>[PR #12601][mhenriks] vmsnapshot: Enable status subresource for snapshot.kubevirt.io api group</li>
  <li>[PR #13018][orelmisan] Support Dynamic Primary Pod NIC Name</li>
  <li>[PR #13019][0xFelix] virtctl: The flags <code class="language-plaintext highlighter-rouge">--volume-clone-pvc</code>, <code class="language-plaintext highlighter-rouge">--volume-datasource</code> and <code class="language-plaintext highlighter-rouge">--volume-blank</code> are deprecated in favor of the <code class="language-plaintext highlighter-rouge">--volume-import</code> flag.</li>
  <li>[PR #13059][EdDev] Network hotplug feature is declared as GA.</li>
  <li>[PR #13024][EdDev] network binding plugin: Introduce a new <code class="language-plaintext highlighter-rouge">managedTap</code> <code class="language-plaintext highlighter-rouge">domainAttachmentType</code></li>
  <li>[PR #13060][EdDev] Network binding plugins feature is declared as Beta.</li>
  <li>[PR #13045][dasionov] Add ‘machine_type’ label for kubevirt_vm_info metric</li>
  <li>[PR #13030][alicefr] Removed the ManualRecoveryRequired field from the VolumeMigrationState and convert it to the VM condition ManualRecoveryRequired</li>
  <li>[PR #13053][0xFelix] virtctl: Users can specify a sysprep volume in VMs created with virtctl create vm</li>
  <li>[PR #12855][0xFelix] virtctl expose: Drop flag to set deprecated LoadBalancerIP option</li>
  <li>[PR #13008][0xFelix] virtctl: Allow creating a basic cloud-init config with virtctl create vm</li>
  <li>[PR #12829][0xFelix] fix: Proxies configured in kubeconfig are used in client-go for asynchronous subresources like VNC or Console</li>
  <li>[PR #12733][alromeros] Bugfix: Fix disk expansion logic by checking usable size instead of requested capacity</li>
  <li>[PR #13052][fossedihelm] Update code-generators to 1.31.1</li>
  <li>[PR #12882][brianmcarey] Build KubeVirt with go v1.22.8</li>
  <li>[PR #13040][awels] BugFix: Allow VMExport to work with VM columes that have dots in its name</li>
  <li>[PR #12729][fossedihelm] Update k8s dependencies to 0.31.0</li>
  <li>[PR #12867][jschintag] Fixed additional broken amd64 image in some image manifests</li>
  <li>[PR #12940][Barakmor1] Deprecate the DockerSELinuxMCS FeatureGate</li>
  <li>[PR #12943][Barakmor1] The <code class="language-plaintext highlighter-rouge">GPU</code> feature gate is now deprecated with the feature state graduated to <code class="language-plaintext highlighter-rouge">GA</code> and thus enabled by default</li>
  <li>[PR #12992][machadovilaca] Add a ‘outdated’ label to kubevirt_vmi_info metric</li>
  <li>[PR #12933][ShellyKa13] VM admitter: improve validation of vm spec datavolumetemplate</li>
  <li>[PR #12986][lyarwood] The <code class="language-plaintext highlighter-rouge">PreferredEfi</code> preference is now only applied when a user has not already enabled either <code class="language-plaintext highlighter-rouge">EFI</code> or <code class="language-plaintext highlighter-rouge">BIOS</code> within the underlying <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</li>
  <li>[PR #12117][Sreeja1725] Integrate kwok with sig-scale tests</li>
  <li>[PR #12716][Sreeja1725] Update kubevirt_rest_client_request_latency_seconds to count list calls if made using query params</li>
  <li>[PR #12578][dasionov] Mark Running field as deprecated</li>
  <li>[PR #12753][lyarwood] The <code class="language-plaintext highlighter-rouge">CommonInstancetypesDeploymentGate</code> feature gate and underlying feature are graduated to GA and now always enabled by default. A single new <code class="language-plaintext highlighter-rouge">KubeVirt</code> configurable is also introduced to allow cluster admins a way of explicitly disabling deployment when required.</li>
  <li>[PR #12645][avlitman] Add kubevirt_vmsnapshot_succeeded_timestamp_seconds metric</li>
  <li>[PR #11097][vamsikrishna-siddu] add s390x support for kubevirt builder</li>
  <li>[PR #12910][machadovilaca] Rename kubevirt_vm_resource_requests ‘vmi’ label to ‘name’</li>
  <li>[PR #12848][iholder101] Reduce default CompletionTimeoutPerGiB from 800s to 150s</li>
  <li>[PR #12861][ShellyKa13] bugfix: fix possible miss update of datavolumename on vmrestore restores</li>
  <li>[PR #12441][machadovilaca] Increase periodicity in domainstats migration metrics</li>
  <li>[PR #12718][machadovilaca] Add kubevirt_vm_info metric</li>
  <li>[PR #12599][xpivarc] MaxCpuSockets won’t block creation of VMs with more Sockets than MaxCpuSockets declare</li>
  <li>[PR #12857][akalenyu] BugFix: Fail to create VMExport via virtctl vmexport create</li>
  <li>[PR #12355][alicefr] Add the volume migration state in the VM status</li>
  <li>[PR #12726][awels] Concurrent addvolume/removevolume using virtctl no longer fail if concurrent modifications happen</li>
  <li>[PR #12835][ShellyKa13] bugfix: In case of err in vmrestore, leave VM without RestoreInProgress annotation allowing it to be started</li>
  <li>[PR #12809][dasionov] bug-fix: Ensure PDB associated with a VMI is deleted when it Reaches Succeeded or Failed phase</li>
  <li>[PR #12813][akalenyu] BugFix: can’t create export pod on OpenShift</li>
  <li>[PR #12786][0xFelix] virtctl: Created VMs can infer an instancetype or preference from PVC, Registry and Snapshot sources now.</li>
  <li>[PR #12764][ShellyKa13] bugfix: vmrestore create DVs before creation/update of restored VM</li>
  <li>[PR #10562][dhiller] Continue changes to Ginkgo V2 Serial runs</li>
  <li>[PR #12516][vamsikrishna-siddu] enable initial e2e tests for s390x.</li>
  <li>[PR #12739][lyarwood] A new <code class="language-plaintext highlighter-rouge">PreferredEfi</code> field has been added to preferences to express the preferred <code class="language-plaintext highlighter-rouge">EFI</code> configuration for a given <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</li>
  <li>[PR #12737][machadovilaca] Add evictable label to kubevirt_vmi_info</li>
  <li>[PR #12232][lyarwood] The <code class="language-plaintext highlighter-rouge">NUMA</code> feature gate is now deprecated with the feature state graduated to <code class="language-plaintext highlighter-rouge">GA</code> and thus enabled by default</li>
  <li>[PR #12582][mhenriks] vmsnapshot: when checking if a VM is running, ignore runStrategy</li>
  <li>[PR #12625][machadovilaca] Add kubevirt_vm_resource_requests for CPU resource</li>
  <li>[PR #12605][mhenriks] vmexport: enable status subresource for VirtualMachineExport</li>
  <li>[PR #12616][orenc1] replace <code class="language-plaintext highlighter-rouge">Update()</code> with <code class="language-plaintext highlighter-rouge">Patch()</code> for <code class="language-plaintext highlighter-rouge">test VirtualMachineInstancesPerNode</code></li>
  <li>[PR #12557][codingben] Optionally create data source using virtctl image upload.</li>
  <li>[PR #12547][mhenriks] virt-api: skip clone auth check when DataVolume already exists</li>
  <li>[PR #12613][orelmisan] Bridge binding: Static routes to subnets containing the pod’s NIC IP address are passed to the VM.</li>
  <li>[PR #12594][tiraboschi] [tests] introduce a decorator for Periodic_only tests</li>
  <li>[PR #12593][machadovilaca] Add kubevirt_vm_resource_requests metric for memory resource</li>
  <li>[PR #12617][Acedus] grpc from go.mod is now correctly shipped in release images</li>
  <li>[PR #12638][akalenyu] BugFix: “Cannot allocate memory” warnings for containerdisk VMs</li>
  <li>[PR #12395][alicefr] Add new condition for VMIStorageLiveMigratable</li>
  <li>[PR #12419][nunnatsa] Add timeout to validation webhooks</li>
  <li>[PR #12592][awels] Fixed issue emitting created secret events when not actually creating secrets during VMExport setup</li>
  <li>[PR #12584][brianmcarey] Build KubeVirt with go v1.22.6</li>
  <li>[PR #12575][Barakmor1] Advise users to use RunStrategy in virt-api messages</li>
  <li>[PR #12466][orenc1] tests/vm_tests.go: replace Update() with Patch()</li>
  <li>[PR #12548][kubevirt-bot] Updated common-instancetypes bundles to v1.1.0</li>
  <li>[PR #12476][jschintag] Enable live-migration and node labels on s390x</li>
  <li>[PR #12194][mhenriks] VM supports kubevirt.io/immediate-data-volume-creation: “false” which delays creating DataVolumeTemplates until VM is started</li>
  <li>[PR #11802][matthewei] Adding newMacAddresses validatewebhook for  VMCloneAPI</li>
  <li>[PR #11754][nickolaev] Adding support for the <code class="language-plaintext highlighter-rouge">igb</code> network interface model</li>
  <li>[PR #12254][jkinred] * Reduced the severity of log messages when a <code class="language-plaintext highlighter-rouge">VolumeSnapshotClass</code> is not found. When snapshots are not enabled for a volume, the reason will still be displayed in the <code class="language-plaintext highlighter-rouge">status.volumeSnapshotStatuses</code> field of a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> resource.</li>
  <li>[PR #12460][mhenriks] virt-api: unencode authorization extra headers</li>
  <li>[PR #12451][fossedihelm] Fix: eviction requests to completed virt-launcher pods cannot trigger a live migration</li>
  <li>[PR #11881][lyarwood] The <code class="language-plaintext highlighter-rouge">expand-spec</code> subresource API now applies defaults to the returned <code class="language-plaintext highlighter-rouge">VirtualMachine</code> to ensure the <code class="language-plaintext highlighter-rouge">VirtualMachineInstanceSpec</code> within is closer to the eventual version used when starting the original <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</li>
  <li>[PR #12452][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 10.5.0 and QEMU 9.0.0.</li>
  <li>[PR #12425][fudancoder] fix some comments</li>
  <li>[PR #12354][qinqon] Use optional interface at passt binding sidecar</li>
  <li>[PR #12268][fossedihelm] Drop <code class="language-plaintext highlighter-rouge">ForceRestart</code> and <code class="language-plaintext highlighter-rouge">ForceStop</code> methods from client-go</li>
  <li>[PR #12235][orelmisan] Network binding plugins: Enable the ability to specify compute memory overhead</li>
  <li>[PR #12209][orenc1] Fix wrong KubeVirtVMIExcessiveMigrations alert calculation in an upgrade scenario.</li>
  <li>[PR #12261][fossedihelm] Fix: persistent tpm can be used with vmis containing dots in their name</li>
  <li>[PR #12247][Sreeja1725] Add perf-scale benchmarks for release v1.3</li>
  <li>[PR #12181][akalenyu] BugFix: Grant namespace admin RBAC to passthrough a client USB to a VMI</li>
  <li>[PR #12096][machadovilaca] Fix missing performance metrics for VMI resources</li>
  <li>[PR #11856][Sreeja1725] Add unit tests to check for API backward compatibility</li>
  <li>[PR #12116][Sreeja1725] Add CPU/Memory utilization of components metrics to kubevirt benchmarks</li>
  <li>[PR #12195][awels] Virt export route has an edge termination of redirect</li>
  <li>[PR #12212][acardace] enable only for VMs with memory &gt;= 1Gi</li>
  <li>[PR #12053][vladikr] Only a single vgpu display option with ramfb will be configured per VMI.</li>
  <li>[PR #12193][acardace] fix RerunOnFailure stuck in Provisioning</li>
  <li>[PR #12186][kubevirt-bot] Updated common-instancetypes bundles to v1.0.1</li>
  <li>[PR #12180][0xFelix] VMs with a single socket and NetworkInterfaceMultiqueue enabled require a restart to hotplug additional CPU sockets.</li>
  <li>[PR #11927][lyarwood] All <code class="language-plaintext highlighter-rouge">preferredCPUTopology</code> constants prefixed with <code class="language-plaintext highlighter-rouge">Prefer</code> have been deprecated and will be removed in a future version of the <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io</code> API.</li>
  <li>[PR #12169][lyarwood] <code class="language-plaintext highlighter-rouge">PreferredDiskDedicatedIoThread</code> is now only applied to <code class="language-plaintext highlighter-rouge">virtio</code> disk devices</li>
  <li>[PR #12125][ksimon1] chore: bump virtio-win image version to 0.1.248</li>
  <li>[PR #12128][acardace] Memory Hotplug fixes and stabilization</li>
  <li>[PR #11911][alromeros] Bugfix: Implement retry mechanism in export server and vmexport</li>
  <li>[PR #11982][RamLavi] Introduce validatingAdmissionPolicy to restrict node patches on virt-handler</li>
  <li>[PR #12119][acardace] Fix VMPools when <code class="language-plaintext highlighter-rouge">LiveUpdate</code> as <code class="language-plaintext highlighter-rouge">vmRolloutStrategy</code> is used.</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.4.0 changes]]></summary></entry><entry><title type="html">You wanted more? It’s KubeVirt v1.4!</title><link href="https://kubevirt.io//2024/KubeVirt-v1-4.html" rel="alternate" type="text/html" title="You wanted more? It’s KubeVirt v1.4!" /><published>2024-11-12T00:00:00+00:00</published><updated>2024-11-12T00:00:00+00:00</updated><id>https://kubevirt.io//2024/KubeVirt-v1-4</id><content type="html" xml:base="https://kubevirt.io//2024/KubeVirt-v1-4.html"><![CDATA[<p>The KubeVirt Community is proud to announce the release of <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.4.0">v1.4</a>. This release aligns with <a href="https://kubernetes.io/blog/2024/08/13/kubernetes-v1-31-release/">Kubernetes v1.31</a> and is the sixth KubeVirt release to follow the Kubernetes release cadence.</p>

<p>What’s 1/3 of one thousand? Because that’s how many people have <a href="https://kubevirt.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.3.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=All&amp;var-country_name=All&amp;var-companies=All">contributed in some way</a> to this release, with 90 of those 333 people <a href="https://kubevirt.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.3.0%20-%20now&amp;var-metric=commits&amp;var-repogroup_name=All&amp;var-country_name=All&amp;var-companies=All">contributing commits to our repos</a>.</p>

<p>You can read the full <a href="https://kubevirt.io/user-guide/release_notes/#v140">release notes</a> in our user-guide, but we have included some highlights in this blog.</p>

<p>For those of you at KubeCon this week, be sure to check out our <a href="https://sched.co/1hoy6">maintainer talk</a> where our project maintainers will be going into these and other recent enhancements in KubeVirt.</p>

<h3 id="feature-ga">Feature GA</h3>
<p>This release marks the graduation of a number of features to GA; deprecating the feature gate and now enabled by default:</p>

<ul>
  <li><a href="https://kubevirt.io/user-guide/network/hotplug_interfaces/#hotplug-network-interfaces">Network hotplug</a>: Add network interfaces to, and remove them from, running virtual machines.</li>
  <li><a href="https://kubevirt.io/user-guide/user_workloads/instancetypes/">Common Instance types</a>: Simplify virtual machine creation with a predefined set of resource, performance, and runtime settings. We have also introduced a single configurable for cluster admins to explicitly disable this feature if required.</li>
  <li><a href="https://deploy-preview-840--kubevirt-user-guide.netlify.app/compute/numa/">NUMA</a>: Improving performance by mapping host NUMA topology to virtual machine topology.</li>
  <li><a href="https://deploy-preview-840--kubevirt-user-guide.netlify.app/compute/host-devices/#host-devices-assignment">GPU assignment</a>: An oldie but a goodie: Assign GPUs and vGPUs to virtual machines.</li>
</ul>

<p>This version of KubeVirt includes upgraded virtualization technology based on <a href="https://www.libvirt.org/news.html#v10-5-0-2024-07-01">libvirt 10.5.0</a> and <a href="https://www.qemu.org/2024/04/23/qemu-9-0-0/">QEMU 9.0.0</a>. Other KubeVirt-specific features of this release include the following:</p>

<h3 id="virtualization">Virtualization</h3>
<p>In the interest of security, we have restricted the <a href="https://github.com/kubevirt/kubevirt/pull/11982">ability of virt-handler</a> to patch nodes, and removed privileges for the cluster. You can also now <a href="https://github.com/kubevirt/kubevirt/pull/13090">live-update tolerations</a> to a running VM.</p>

<p>Our KubeVirt command line tool, virtctl, also received some love and <a href="https://kubevirt.io/user-guide/release_notes/ba#sig-compute">improved functionality</a> for VM creation, image upload, and source inference.</p>

<h3 id="networking">Networking</h3>
<p>The networking binding plugins have matured to Beta, and we have a new domain attachment type,<a href="https://github.com/kubevirt/kubevirt/pull/13024"><code class="language-plaintext highlighter-rouge">managedTap</code></a>, and the ability to <a href="https://github.com/kubevirt/kubevirt/pull/12235">reserve memory overhead</a> for binding plugins. <a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network binding plugins</a> enable vendors to provide their own VM-to-network plumbing alongside KubeVirt.</p>

<p>We also added support for the <code class="language-plaintext highlighter-rouge">igb</code> network interface model.</p>

<h3 id="storage">Storage</h3>
<p>If you’ve ever wanted to migrate your virtual machine volume from one storage type to another then you’ll be interested in our <a href="https://kubevirt.io/user-guide/storage/volume_migration/">volume migration</a> feature.</p>

<h3 id="scale-and-performance">Scale and Performance</h3>
<p>Our SIG scale and performance team have added performance benchmarks for resource utilization of virt-controller and virt-api components. Furthermore, the test-suite was enhanced by <a href="https://github.com/kubevirt/kubevirt/pull/12117">integrating KWOK with SIG-scale tests</a> to simulate nodes and VMIs to test KubeVirt performance while using minimum resources in test infrastructure. A comprehensive list of performance and scale benchmarks for the release is available <a href="https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md">here</a>.</p>

<h3 id="thanks">Thanks!</h3>
<p>A lot of work from a huge amount of people go into these releases. Some contributions are small, such as raising a bug or attending our community meeting, and others are massive, like working on a feature or reviewing PRs. Whatever your part: we thank you.</p>

<p>And if you’re interested in contributing to the project and being a part of the next release, please check out our <a href="https://kubevirt.io/user-guide/contributing/">contributing guide</a> and our <a href="https://github.com/kubevirt/community/blob/main/membership_policy.md">community membership guidelines</a>.</p>]]></content><author><name>KubeVirt Maintainers</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.4" /><category term="release" /><category term="community" /><category term="cncf" /><category term="milestone" /><category term="party time" /><summary type="html"><![CDATA[Introducing the KubeVirt v1.4 release]]></summary></entry><entry><title type="html">KubeVirt v1.3.0</title><link href="https://kubevirt.io//2024/changelog-v1.3.0.html" rel="alternate" type="text/html" title="KubeVirt v1.3.0" /><published>2024-07-17T00:00:00+00:00</published><updated>2024-07-17T00:00:00+00:00</updated><id>https://kubevirt.io//2024/changelog-v1.3.0</id><content type="html" xml:base="https://kubevirt.io//2024/changelog-v1.3.0.html"><![CDATA[<h2 id="v130">v1.3.0</h2>

<p>Released on: Wed Jul 17 15:09:44 2024 +0000</p>

<ul>
  <li>[PR #12319][Sreeja1725] Add v1.3.0 perf and scale benchmarks data</li>
  <li>[PR #12330][kubevirt-bot] Fix wrong KubeVirtVMIExcessiveMigrations alert calculation in an upgrade scenario.</li>
  <li>[PR #12328][acardace] enable only for VMs with memory &gt;= 1Gi</li>
  <li>[PR #12272][Sreeja1725] Add unit tests to check for API backward compatibility</li>
  <li>[PR #12296][orelmisan] Network binding plugins: Enable the ability to specify compute memory overhead</li>
  <li>[PR #12279][kubevirt-bot] Fix: persistent tpm can be used with vmis containing dots in their name</li>
  <li>[PR #12226][kubevirt-bot] Virt export route has an edge termination of redirect</li>
  <li>[PR #12240][kubevirt-bot] Updated common-instancetypes bundles to v1.0.1</li>
  <li>[PR #12249][kubevirt-bot] Fix missing performance metrics for VMI resources</li>
  <li>[PR #12237][vladikr] Only a single vgpu display option with ramfb will be configured per VMI.</li>
  <li>[PR #12122][kubevirt-bot] Fix VMPools when <code class="language-plaintext highlighter-rouge">LiveUpdate</code> as <code class="language-plaintext highlighter-rouge">vmRolloutStrategy</code> is used.</li>
  <li>[PR #12201][kubevirt-bot] fix RerunOnFailure stuck in Provisioning</li>
  <li>[PR #12151][kubevirt-bot] Bugfix: Implement retry mechanism in export server and vmexport</li>
  <li>[PR #12171][kubevirt-bot] <code class="language-plaintext highlighter-rouge">PreferredDiskDedicatedIoThread</code> is now only applied to <code class="language-plaintext highlighter-rouge">virtio</code> disk devices</li>
  <li>[PR #12146][kubevirt-bot] Memory Hotplug fixes and stabilization</li>
  <li>[PR #12185][kubevirt-bot] VMs with a single socket and NetworkInterfaceMultiqueue enabled require a restart to hotplug additional CPU sockets.</li>
  <li>[PR #12132][kubevirt-bot] Introduce validatingAdmissionPolicy to restrict node patches on virt-handler</li>
  <li>[PR #12109][acardace] Support Memory Hotplug with Hugepages</li>
  <li>[PR #12009][xpivarc] By enabling NodeRestriction feature gate, Kubevirt now authorize virt-handler’s requests to modify VMs.</li>
  <li>[PR #11681][lyarwood] The <code class="language-plaintext highlighter-rouge">CommonInstancetypesDeployment</code> feature and gate are retrospectively moved to Beta from the 1.2.0 release.</li>
  <li>[PR #12025][fossedihelm] Add descheduler compatibility</li>
  <li>[PR #12097][fossedihelm] Bump k8s deps to 0.30.0</li>
  <li>[PR #12089][jean-edouard] Less privileged virt-operator ClusterRole</li>
  <li>[PR #12064][akalenyu] BugFix: Graceful deletion skipped for any delete call to the VM (not VMI) resource</li>
  <li>[PR #10490][jschintag] Add support for building and running kubevirt on s390x.</li>
  <li>[PR #12079][EdDev] Network hotplug feature is declared as Beta.</li>
  <li>[PR #11455][lyarwood] <code class="language-plaintext highlighter-rouge">LiveUpdates</code>  of VMs using instance types are now supported with the same caveats as when making changes to a vanilla VM.</li>
  <li>[PR #12000][machadovilaca] Create kubevirt_vmi_launcher_memory_overhead_bytes metric</li>
  <li>[PR #11915][ormergi] The ‘passt’ core network binding is discontinued and removed.</li>
  <li>[PR #12016][acardace] fix starting VM with Manual RunStrategy</li>
  <li>[PR #11533][alicefr] Implement volume migration and introduce the migration updateVolumesStrategy field</li>
  <li>[PR #11934][assafad] Add kubevirt_vmi_last_connection_timestamp_seconds metric</li>
  <li>[PR #11956][mhenriks] Introduce export.kibevirt.io/v1beta1</li>
  <li>[PR #11996][ShellyKa13] BugFix: Fix restore panic in case of volumesnapshot missing</li>
  <li>[PR #11957][mhenriks] snapshot: Ignore unfreeze error if VMSnapshot deleting</li>
  <li>[PR #11906][machadovilaca] Create kubevirt_vmi_info metric</li>
  <li>[PR #11969][iholder101] Infra components control-plane nodes NoSchedule toleration</li>
  <li>[PR #11955][mhenriks] Introduce snapshot.kibevirt.io/v1beta1</li>
  <li>[PR #11883][orelmisan] Restart of a VM is required when the CPU socket count is reduced</li>
  <li>[PR #11835][talcoh2x] add Intel Gaudi to adopters.</li>
  <li>[PR #11344][aerosouund] Refactor device plugins to use a base plugin and define a common interface</li>
  <li>[PR #11973][fossedihelm] Bug fix: Correctly reflect RestartRequired condition</li>
  <li>[PR #11963][acardace] Fix RerunOnFailure RunStrategy</li>
  <li>[PR #11962][lyarwood] <code class="language-plaintext highlighter-rouge">VirtualMachines</code> referencing an instance type are now allowed when the <code class="language-plaintext highlighter-rouge">LiveUpdate</code> feature is enabled and will trigger the <code class="language-plaintext highlighter-rouge">RestartRequired</code> condition if the reference within the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> is changed.</li>
  <li>[PR #11942][ido106] Update virtctl to use v1beta1 endpoint for both regular and async image upload</li>
  <li>[PR #11648][kubevirt-bot] Updated common-instancetypes bundles to v1.0.0</li>
  <li>[PR #11659][iholder101] Require scheduling infra components onto control-plane nodes</li>
  <li>[PR #10545][lyarwood] <code class="language-plaintext highlighter-rouge">ControllerRevisions</code> containing instance types and preferences are now upgraded to their latest available version when the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> owning them is resync’d by <code class="language-plaintext highlighter-rouge">virt-controller</code>.</li>
  <li>[PR #11901][EdDev] The ‘macvtap’ core network binding is discontinued and removed.</li>
  <li>[PR #11922][alromeros] Bugfix: Fix VM manifest rendering in export controller</li>
  <li>[PR #11908][victortoso] sidecar-shim: allow stderr log from binary hooks</li>
  <li>[PR #11729][lyarwood] <code class="language-plaintext highlighter-rouge">spreadOptions</code> have been introduced to preferences in order to allow for finer grain control of the <code class="language-plaintext highlighter-rouge">preferSpread</code> <code class="language-plaintext highlighter-rouge">preferredCPUTopology</code>. This includes the ability to now spread vCPUs across guest visible sockets, cores and threads.</li>
  <li>[PR #11655][acardace] Allow to hotplug vcpus for VMs with CPU requests and/or limits set</li>
  <li>[PR #11701][EdDev] The SLIRP core binding is deprecated and removed.</li>
  <li>[PR #11773][jean-edouard] Persistent TPM/UEFI will use the default storage class if none is specified in the CR.</li>
  <li>[PR #11846][victortoso] SMBios sidecar can be built out-of-tree</li>
  <li>[PR #11788][ormergi] The network-info annotation is now used for mapping between SR-IOV network and the underlying device PCI address</li>
  <li>[PR #11700][alicefr] Add the updateVolumeStrategy field</li>
  <li>[PR #11256][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 10.0.0 and QEMU 8.2.0.</li>
  <li>[PR #11482][brianmcarey] Build KubeVirt with go v1.22.2</li>
  <li>[PR #11641][alicefr] Add kubevirt.io/testWorkloadUpdateMigrationAbortion annotation and a mechanism to abort workload updates</li>
  <li>[PR #11770][alicefr] Fix the live updates for volumes and disks</li>
  <li>[PR #11790][aburdenthehand] Re-adding Cloudflare to our ADOPTERS list</li>
  <li>[PR #11718][fossedihelm] Fix: SEV methods in client-go now satisfy the proxy server configuration, if provided</li>
  <li>[PR #11685][fossedihelm] Updated go version of the client-go to 1.21</li>
  <li>[PR #11618][AlonaKaplan] Extend network binding plugin to support device-info DownwardAPI.</li>
  <li>[PR #11283][assafad] Collect VMI OS info from the Guest agent as <code class="language-plaintext highlighter-rouge">kubevirt_vmi_phase_count</code> metric labels</li>
  <li>[PR #11676][machadovilaca] Rename rest client metrics to include kubevirt prefix</li>
  <li>[PR #11557][avlitman] New memory statistics added named kubevirt_memory_delta_from_requested_bytes</li>
  <li>[PR #11678][Vicente-Cheng] Improve the handling of ordinal pod interface name for upgrade</li>
  <li>[PR #11653][EdDev] Build the <code class="language-plaintext highlighter-rouge">passt</code>custom CNI binary statically, for the <code class="language-plaintext highlighter-rouge">passt</code> network binding plugin.</li>
  <li>[PR #11294][machadovilaca] Fix kubevirt_vm_created_total being broken down by virt-api pod</li>
  <li>[PR #11307][machadovilaca] Add e2e tests for metrics</li>
  <li>[PR #11479][vladikr] virtual machines instance will no longer be stuck in an irrecoverable state after an interrupted postcopy migration. Instead, these will fail and could be restarted again.</li>
  <li>[PR #11416][dhiller] emission of k8s logs when using programmatic focus with <code class="language-plaintext highlighter-rouge">FIt</code></li>
  <li>[PR #11272][dharmit] Make ‘image’ field in hook sidecar annotation optional.</li>
  <li>[PR #11500][iholder101] Support HyperV Passthrough: automatically use all available HyperV features</li>
  <li>[PR #11484][jcanocan] Reduce the downwardMetrics server maximum number of request per second to 1.</li>
  <li>[PR #11498][acardace] Allow to hotplug memory for VMs with memory limits set</li>
  <li>[PR #11470][brianmcarey] Build KubeVirt with Go version 1.21.8</li>
  <li>[PR #11312][alromeros] Improve handling of export resources in virtctl vmexport</li>
  <li>[PR #11367][alromeros] Bugfix: Allow vmexport download redirections by printing logs into stderr</li>
  <li>[PR #11219][alromeros] Bugfix: Improve handling of IOThreads with incompatible buses</li>
  <li>[PR #11149][0xFelix] virtctl: It is possible to import volumes from GCS when creating a VM now</li>
  <li>[PR #11404][avlitman] KubeVirtComponentExceedsRequestedCPU and KubeVirtComponentExceedsRequestedMemory alerts are deprecated; they do not indicate a genuine issue.</li>
  <li>[PR #11331][anjuls] add cloudraft to adopters.</li>
  <li>[PR #11387][alaypatel07] add perf-scale benchmarks for release v1.2</li>
  <li>[PR #11095][ShellyKa13] Expose volumesnapshot error in vmsnapshot object</li>
  <li>[PR #11372][xpivarc] Bug-fix: Fix nil panic if VM update fails</li>
  <li>[PR #11267][mhenriks] BugFix: Ensure DataVolumes created by virt-controller (DataVolumeTemplates) are recreated and owned by the VM in the case of DR and backup/restore.</li>
  <li>[PR #10900][KarstenB] BugFix: Fixed incorrect APIVersion of APIResourceList</li>
  <li>[PR #11306][fossedihelm] fix(ksm): set the <code class="language-plaintext highlighter-rouge">kubevirt.io/ksm-enabled</code> node label to true if the ksm is managed by KubeVirt, instead of reflect the actual ksm value.</li>
  <li>[PR #11330][jean-edouard] More information in the migration state of VMI / migration objects</li>
  <li>[PR #11264][machadovilaca] Fix perfscale buckets error</li>
  <li>[PR #11183][dhiller] Extend OWNERS for sig-buildsystem</li>
  <li>[PR #11058][fossedihelm] fix(vmclone): delete vmclone resource when the target vm is deleted</li>
  <li>[PR #11265][xpivarc] Bug fix: VM controller doesn’t corrupt its cache anymore</li>
  <li>[PR #11205][akalenyu] Fix migration breaking in case the VM has an rng device after hotplugging a block volume on cgroupsv2</li>
  <li>[PR #11051][alromeros] Bugfix: Improve error reporting when fsfreeze fails</li>
  <li>[PR #11156][nunnatsa] Move some verification from the VMI create validation webhook to the CRD</li>
  <li>[PR #11146][RamLavi] node-labeller: Remove obsolete functionalities</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.3.0 changes]]></summary></entry><entry><title type="html">KubeVirt Summit 2024 CfP is open!</title><link href="https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP.html" rel="alternate" type="text/html" title="KubeVirt Summit 2024 CfP is open!" /><published>2024-03-19T00:00:00+00:00</published><updated>2024-03-19T00:00:00+00:00</updated><id>https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP</id><content type="html" xml:base="https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP.html"><![CDATA[<p>We are very pleased to announce the details for this year’s KubeVirt Summit!!</p>

<h2 id="what-is-kubevirt-summit">What is KubeVirt Summit?</h2>

<p>KubeVirt Summit is our annual online conference, now in its fourth year, in which the entire broader community meets to showcase technical architecture, new features, proposed changes, and in-depth tutorials.
We have two tracks to cater for developer talks, and another for end users to share their deployment journey with KubeVirt and their use case(s) at scale. And there’s no reason why a talk can’t be both :)</p>

<h2 id="when-is-it">When is it?</h2>

<p>The event will take place online over two half-days:</p>

<ul>
  <li>Dates: <del>June 25 and 26</del> <strong>June 24 and 25</strong>, 2024</li>
  <li>Time: TBD 
(In the past we have aimed for 1200-1700 UTC but may modify these times slightly depending on our speaker timezones)</li>
</ul>

<h2 id="how-to-submit-a-proposal">How to submit a proposal?</h2>

<ul>
  <li>Please submit through this <a href="https://docs.google.com/forms/d/e/1FAIpQLSeELmfpD_20kZnrciXkdSdDS_MLFLN9xSaZDKptNPjg3JGLaA/viewform">Googleform</a></li>
  <li>CfP closes: <strong>May 20</strong>, 2024</li>
  <li>Schedule will announced at the end of May</li>
</ul>

<p>Do consider proposing a session, and help make our fourth Summit as valuable as possible. We welcome a range of session types, any of which can be simple and intended for beginners or face-meltingly technical. Check out <a href="https://www.youtube.com/playlist?list=PLnLpXX8KHIYwe_V5pCXfXVDs-lY5dX55Q">last year’s talks</a> for some ideas.</p>

<h2 id="have-questions">Have questions?</h2>

<p>Our <a href="/summit/">KubeVirt Summit 2024</a> page will continue to evolve with details as we get closer.</p>

<p>You can also reach out on our <a href="https://kubernetes.slack.com/messages/virtualization">virtualization</a> Slack channel (in the Kubernetes workspace).</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our mailing list, slack channels, weekly meetings, and more, all list in our <a href="https://github.com/kubevirt/community">community repo</a>.</p>

<p>Good luck!</p>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's fourth annual dedicated online event]]></summary></entry><entry><title type="html">KubeVirt v1.2.0</title><link href="https://kubevirt.io//2024/changelog-v1.2.0.html" rel="alternate" type="text/html" title="KubeVirt v1.2.0" /><published>2024-03-05T00:00:00+00:00</published><updated>2024-03-05T00:00:00+00:00</updated><id>https://kubevirt.io//2024/changelog-v1.2.0</id><content type="html" xml:base="https://kubevirt.io//2024/changelog-v1.2.0.html"><![CDATA[<h2 id="v120">v1.2.0</h2>

<p>Released on: Tue Mar 5 20:25:04 2024 +0000</p>

<ul>
  <li>[PR #11318][fossedihelm] fix(vmclone): delete vmclone resource when the target vm is deleted</li>
  <li>[PR #11393][kubevirt-bot] Bug-fix: Fix nil panic if VM update fails</li>
  <li>[PR #11354][kubevirt-bot] Fix perfscale buckets error</li>
  <li>[PR #11378][fossedihelm] fix(ksm): set the <code class="language-plaintext highlighter-rouge">kubevirt.io/ksm-enabled</code> node label to true if the ksm is managed by KubeVirt, instead of reflect the actual ksm value.</li>
  <li>[PR #11271][kubevirt-bot] Bug fix: VM controller doesn’t corrupt its cache anymore</li>
  <li>[PR #11242][kubevirt-bot] Fix migration breaking in case the VM has an rng device after hotplugging a block volume on cgroupsv2</li>
  <li>[PR #11144][0xFelix] virtctl: Specifying size when creating a VM and using –volume-import to clone a PVC or a VolumeSnapshot is optional now</li>
  <li>[PR #11054][jean-edouard] New cluster-wide <code class="language-plaintext highlighter-rouge">vmRolloutStrategy</code> setting to define whether changes to VMs should either be always staged or live-updated when possible.</li>
  <li>[PR #11064][AlonaKaplan] Introduce a new API to mark a binding plugin as migratable.</li>
  <li>[PR #11122][brianmcarey] Update runc dependency to v1.1.12</li>
  <li>[PR #10982][machadovilaca] Refactor monitoring metrics</li>
  <li>[PR #11069][ormergi] Bug fix: Packet drops during the initial phase of VM live migration https://issues.redhat.com/browse/CNV-28040</li>
  <li>[PR #10961][jcanocan] Reduced VM rescheduling time on node failure</li>
  <li>[PR #11065][fossedihelm] fix(vmclone): Generate VM patches from vmsnapshotcontent, instead of current VM</li>
  <li>[PR #10888][fossedihelm] [Bugfix] Clone VM with WaitForFirstConsumer binding mode PVC now works.</li>
  <li>[PR #11068][brianmcarey] Update container base image to use current stable debian 12 base</li>
  <li>[PR #11047][jschintag] Fix potential crash when trying to list USB devices on host without any</li>
  <li>[PR #10970][alromeros] Expose fs disk information via GuestOsInfo</li>
  <li>[PR #11050][fossedihelm] restrict default cluster role to authenticated only users</li>
  <li>[PR #11025][0xFelix] Allow unprivileged users read-only access to VirtualMachineCluster{Instancetypes,Preferences} by default.</li>
  <li>[PR #10853][machadovilaca] Refactor monitoring collectors</li>
  <li>[PR #11001][fossedihelm] Allow <code class="language-plaintext highlighter-rouge">kubevirt.io:default</code> clusterRole to get,list kubevirts</li>
  <li>[PR #10905][tiraboschi] Aggregate DVs conditions on VMI (and so VM)</li>
  <li>[PR #10963][alromeros] Bugfix: Reject volume exports when no output is specified</li>
  <li>[PR #10962][machadovilaca] Update monitoring file structure</li>
  <li>[PR #10981][AlonaKaplan] Report IP of interfaces using network binding plugin.</li>
  <li>[PR #10922][kubevirt-bot] Updated common-instancetypes bundles to v0.4.0</li>
  <li>[PR #10914][brianmcarey] KubeVirt is now built with go 1.21.5</li>
  <li>[PR #10846][RamLavi] Change vm.status.PrintableStatus default value to “Stopped”</li>
  <li>[PR #10787][matthewei] # Create a manifest for a clone with template label filters:</li>
  <li>[PR #10918][orelmisan] VMClone: Emit an event in case restore creation fails</li>
  <li>[PR #10916][orelmisan] Fix the value of VMI <code class="language-plaintext highlighter-rouge">Status.GuestOSInfo.Version</code></li>
  <li>[PR #10924][AlonaKaplan] Deprecate macvtap</li>
  <li>[PR #10898][matthewei] vmi status’s guestOsInfo adds <code class="language-plaintext highlighter-rouge">Machine</code></li>
  <li>[PR #10866][AlonaKaplan] Raise an error in case passt feature gate or API are used.</li>
  <li>[PR #10879][brianmcarey] Built with golang 1.20.12</li>
  <li>[PR #10872][RamLavi] IsolateEmulatorThread: Add cluster-wide parity completion setting</li>
  <li>[PR #10700][machadovilaca] Refactor monitoring alerts</li>
  <li>[PR #10839][RamLavi] Change second emulator thread assign strategy to best-effort.</li>
  <li>[PR #10863][dhiller] Remove year from generated code copyright</li>
  <li>[PR #10747][acardace] Fix KubeVirt for CRIO 1.28 by using checksums to verify containerdisks when migrating VMIs</li>
  <li>[PR #10860][akalenyu] BugFix: Double cloning with filter fails</li>
  <li>[PR #10567][awels] Attachment pod creation is now rate limited</li>
  <li>[PR #10845][orelmisan] Reject VirtualMachineClone creation when target name is equal to source name</li>
  <li>[PR #10840][acardace] Requests/Limits can now be configured when using CPU/Memory hotplug</li>
  <li>[PR #10418][machadovilaca] Add total VMs created metric</li>
  <li>[PR #10800][AlonaKaplan] Support macvtap as a binding plugin</li>
  <li>[PR #10753][victortoso] Fixes device permission when using USB host passthrough</li>
  <li>[PR #10774][victortoso] Windows offline activation with ACPI SLIC table</li>
  <li>[PR #10783][RamLavi] Support multiple CPUs in Housekeeping cgroup</li>
  <li>[PR #10809][orelmisan] Source virt-launcher: Log migration info by default</li>
  <li>[PR #10046][victortoso] Add v1alpha3 for hooks</li>
  <li>[PR #10651][machadovilaca] Refactor monitoring  recording-rules</li>
  <li>[PR #10732][AlonaKaplan] Extend kubvirt CR by adding domain attachment option to the network binding plugin API.</li>
  <li>[PR #10244][hshitomi] Added “adm” subcommand under “virtctl”, and “log-verbosity” subcommand under “adm”. The log-verbosity command is:</li>
  <li>[PR #10658][matthewei] 1. Support “Clone API” to filter VirtualMachine.spec.template.annotation and VirtualMachine.spec.template.label</li>
  <li>[PR #10593][RamLavi] Fixes SMT Alignment Error in virt-launcher pod by optimizing isolateEmulatorThread feature (BZ#2228103).</li>
  <li>[PR #10720][awels] Restored hotplug attachment pod request/limit to original value</li>
  <li>[PR #10657][germag] Exposing Filesystem Persistent Volumes (PVs)  to the VM using unprivilege virtiofsd.</li>
  <li>[PR #10637][dharmit] Functional tests for sidecar hook with ConfigMap</li>
  <li>[PR #10598][alicefr] Add PVC option to the hook sidecars for supplying additional debugging tools</li>
  <li>[PR #10526][cfilleke]</li>
  <li>[PR #10699][qinqon] virt-launcher: fix qemu non root log path</li>
  <li>[PR #10689][akalenyu] BugFix: cgroupsv2 device allowlist is bound to virt-handler internal state/block disk device overwritten on hotplug</li>
  <li>[PR #10693][machadovilaca] Remove MigrateVmiDiskTransferRateMetric</li>
  <li>[PR #10615][orelmisan] Remove leftover NonRoot feature gate</li>
  <li>[PR #10529][alromeros] Allow LUN disks to be hotplugged</li>
  <li>[PR #10582][orelmisan] Remove leftover NonRootExperimental feature gate</li>
  <li>[PR #10596][mhenriks] Disable HTTP/2 to mitigate CVE-2023-44487</li>
  <li>[PR #10570][machadovilaca] Fix LowKVMNodesCount not firing</li>
  <li>[PR #10571][tiraboschi] vmi memory footprint increase by 35M when guest serial console logging is turned on (default on).</li>
  <li>[PR #10425][ormergi] Introduce network binding plugin for Passt networking, interfacing with Kubevirt new network binding plugin API.</li>
  <li>[PR #10479][dharmit] Ability to run scripts through hook sidecar</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.2.0 changes]]></summary></entry><entry><title type="html">Announcing KubeVirt v1.1</title><link href="https://kubevirt.io//2023/Announcing-KubeVirt-v1-1.html" rel="alternate" type="text/html" title="Announcing KubeVirt v1.1" /><published>2023-11-07T00:00:00+00:00</published><updated>2023-11-07T00:00:00+00:00</updated><id>https://kubevirt.io//2023/Announcing-KubeVirt-v1-1</id><content type="html" xml:base="https://kubevirt.io//2023/Announcing-KubeVirt-v1-1.html"><![CDATA[<p>The KubeVirt Community is very pleased to announce the release of KubeVirt v1.1. This comes 17 weeks after our celebrated v1.0 release, and follows the predictable schedule we moved to three releases ago to follow the Kubernetes release cadence.</p>

<p>You can read the full <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.1.0">v1.1 release notes here</a>, but we’ve asked the KubeVirt SIGs to summarize their largest successes, as well as one of the community members from Arm to list their integration accomplishments for this release.</p>

<h2 id="sig-compute">SIG-compute</h2>
<p>SIG-compute covers the core functionality of KubeVirt. This includes scheduling VMs, the API, and all KubeVirt operators.</p>

<p>For the v1.1 release, we have added quite a few features. This includes memory hotplug, as a follow up to CPU hotplug, which was part of the 1.0 release. Basic KSM support was already part of KubeVirt, but we have now extended that with more tuning parameters and KubeVirt can also dynamically configure KSM based on system pressure. We’ve added persistent NVRAM support (requires that a VM use UEFI) so that settings are preserved across reboots.</p>

<p>We’ve also added host-side USB passthrough support, so that USB devices on a cluster node can be made available to workloads. KubeVirt can now automatically apply limits to a VM running in a namespace with quotas. We’ve also added refinements to VM cloning, as well as the ability to create clones using the virtctl CLI tool. And you can now stream guest’s console logs.</p>

<p>Finally, on the confidential computing front, we now have an API for SEV attestation.</p>

<h2 id="sig-infra">SIG-infra</h2>
<p>SIG-infra takes care of KubeVirt’s own infrastructure, user workloads and other user-focused integrations through automation and the reduction of complexity wherever possible, providing a quality experience for end users.</p>

<p>In this release, two major instance type-related features were added to KubeVirt. The first feature is the deployment of Common InstanceTypes by the virt-operator. This provides users with a useful set of InstanceTypes and Preferences right out of the box and allows them to easily create virtual machines tailored to the needs of their workloads. For now this feature remains behind a feature gate, but in future versions we aim to enable the deployment by default.</p>

<p>Secondly, the inference of InstanceTypes and Preferences has been enabled by default when creating virtual machines with virtctl. This feature was already present in the previous release, but users still needed to explicitly enable it. Now it is enabled by default, being as transparent as possible so as to not let the creation of virtual machines fail if inference should not be possible. This significantly improves usability, as the command line for creating virtual machines is now even simpler.</p>

<h2 id="sig-network">SIG-network</h2>
<p>SIG-network is committed to enhancing and maintaining all aspects of Virtual Machine network connectivity and management in KubeVirt.</p>

<p>For the v1.1 release, we have re-designed the interface hot plug/unplug API, while adding hotplug support for SR-IOV interfaces. On top of that, we have added a network binding option allowing the community to extend the KubeVirt network configuration in the pod by injecting custom CNI plugins to configure the networking stack, and a sidecar to configure the libvirt domain. The existing <code class="language-plaintext highlighter-rouge">slirp</code> network configuration has been extracted from the code and re-designed as one such network binding, and can be used by the community as an example on how to extend KubeVirt bindings.</p>

<h2 id="sig-scale">SIG-scale</h2>
<p>SIG-scale continues to track scale and performance across releases.  The v1.1 testing lanes ran on Kubernetes 1.27 and we observed a slight performance improvement from Kubernetes.  There’s no other notable performance or scale changes in KubeVirt v1.1 as our focus has been on improving our tracking.</p>

<h4 id="vmicreationtorunningsecondsp95">vmiCreationToRunningSecondsP95</h4>
<ul>
  <li>The gray dotted line in the graph is Feb 1, 2023, denoting release of v0.59</li>
  <li>The blue dotted line in the graph is March 1, 2023, denoting release of v0.60</li>
  <li>The green dotted line in the graph is July 6, 2023, denoting release of v1.0.0</li>
  <li>The red dotted line in the graph is September 6, 2023, denoting change in k8s provider from v1.25 to v1.27</li>
</ul>

<p><img src="/assets/2023-11-07-Announcing-KubeVirt-v1-1/vmi-p95-Creation-to-Running.png" alt="Alt text" />
<img src="/assets/2023-11-07-Announcing-KubeVirt-v1-1/vm-p95-Creation-to-Running.png" alt="Alt text" /></p>

<p>Full v1.1 data source: <a href="https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md">https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md</a></p>

<h2 id="sig-storage">SIG-storage</h2>
<p>SIG-storage is focused on providing persistent storage to KubeVirt VMs and managing that storage throughout the lifecycle of the VM. This begins with provisioning and populating PVCs with bootable images but also includes features such as disk hotplug, snapshots, backup and restore, disaster recovery, and virtual machine export.</p>

<p>For this release we aimed to draw closer to Kubernetes principles when it comes to managing storage artifacts. Introducing CDI volume populators, which is CDI’s implementation of importing/uploading/cloning data to PVCs using the <code class="language-plaintext highlighter-rouge">dataSourceRef</code> field. This follows the Kubernetes way of populating PVCs and enables us to populate PVCs directly without the need for DataVolumes, an important but bespoke object that has served the KubeVirt use case for many years.</p>

<p>Speaking of DataVolumes, they will no longer be garbage collected by default, something that violated a fundamental principle of Kubernetes (even though it was very useful for our use case).</p>

<p>And, finally, we can now use snapshots to store operating system “golden images”, to serve as the base image for cloning.</p>

<h2 id="kubevirt-and-arm">KubeVirt and Arm</h2>
<p>We are excited to announce the successful integration of KubeVirt on Arm64 platforms. Here are some key accomplishments:</p>
<ol>
  <li><strong>Building and Compiling</strong>: We have released multi-architecture KubeVirt component images and binaries, while also allowing cross-compiling Arm64 architecture images and binaries on x86_64 platforms.</li>
  <li><strong>Core Functionality</strong>: Our dedicated efforts have focused on enabling the core functionality of KubeVirt on Arm64 platforms.</li>
  <li><strong>Testing Integration</strong>: Quality assurance is of paramount importance. We have integrated unit tests and end-to-end tests on Arm64 servers into the pull request (PR) pre-submit process. This guarantees that KubeVirt maintains its reliability and functionality on Arm64.</li>
  <li><strong>Comprehensive Documentation</strong>: To provide valuable insights into KubeVirt’s capabilities on Arm64 platforms, we have compiled extensive documentation. Explore the status of <a href="https://kubevirt.io/user-guide/operations/feature_gate_status_on_Arm64/">feature gates</a> and dive into <a href="https://kubevirt.io/user-guide/virtual_machines/device_status_on_Arm64/">device status documentation</a>.</li>
  <li><strong>Hybrid Cluster Compatibility Preview</strong>: Hybrid x86_64 and Arm64 clusters can work together now as a preview feature. Try it out and provide feedback.</li>
</ol>

<p>We are thrilled to declare that KubeVirt now offers tier-one support on Arm64 platforms. This milestone represents a culmination of collaborative efforts, unwavering dedication, and a commitment to innovation within the KubeVirt community. KubeVirt is no longer just an option; it has evolved to become a first-class citizen on Arm64 platforms.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Thank you to everyone in the KubeVirt Community who contributed to this release, whether you pitched in on any of the features listed above, helped out with any of the other features or maintenance improvements listed in our release notes, or made any number of non-code contributions to our website, user guide or meetings.</p>]]></content><author><name>KubeVirt Community</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.1.0" /><category term="release" /><category term="community" /><category term="cncf" /><summary type="html"><![CDATA[We are very pleased to announce the release of KubeVirt v1.1!]]></summary></entry><entry><title type="html">KubeVirt v1.1.0</title><link href="https://kubevirt.io//2023/changelog-v1.1.0.html" rel="alternate" type="text/html" title="KubeVirt v1.1.0" /><published>2023-11-06T00:00:00+00:00</published><updated>2023-11-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/changelog-v1.1.0</id><content type="html" xml:base="https://kubevirt.io//2023/changelog-v1.1.0.html"><![CDATA[<h2 id="v110">v1.1.0</h2>

<p>Released on: Mon Nov 6 16:28:56 2023 +0000</p>

<ul>
  <li>[PR #10669][kubevirt-bot] Introduce network binding plugin for Passt networking, interfacing with Kubevirt new network binding plugin API.</li>
  <li>[PR #10646][jean-edouard] The dedicated migration network should now always be properly detected by virt-handler</li>
  <li>[PR #10602][kubevirt-bot] Fix LowKVMNodesCount not firing</li>
  <li>[PR #10566][fossedihelm] Add 100Mi of memory overhead for vmi with dedicatedCPU or that wants GuaranteedQos</li>
  <li>[PR #10568][ormergi] Network binding plugin API support CNIs, new integration point on virt-launcher pod creation.</li>
  <li>[PR #10496][fossedihelm] Automatically set cpu limits when a resource quota with cpu limits is associated to the creation namespace and the <code class="language-plaintext highlighter-rouge">AutoResourceLimits</code> FeatureGate is enabled</li>
  <li>[PR #10309][lyarwood] cluster-wide <a href="https://github.com/kubevirt/common-instancetypes"><code class="language-plaintext highlighter-rouge">common-instancetypes</code></a> resources can now deployed by <code class="language-plaintext highlighter-rouge">virt-operator</code> using the <code class="language-plaintext highlighter-rouge">CommonInstancetypesDeploymentGate</code> feature gate.</li>
  <li>[PR #10543][0xFelix] Clear VM guest memory when ignoring inference failures</li>
  <li>[PR #9590][xuzhenglun] fix embed version info of virt-operator</li>
  <li>[PR #10532][alromeros] Add –volume-mode flag in image-upload</li>
  <li>[PR #10515][iholder101] Bug-fix: Stop copying VMI spec to VM during snapshots</li>
  <li>[PR #10320][victortoso] sidecar-shim implements PreCloudInitIso hook</li>
  <li>[PR #10463][0xFelix] VirtualMachines: Introduce InferFromVolumeFailurePolicy in Instancetype- and PreferenceMatchers</li>
  <li>[PR #10393][iholder101] [Bugfix] [Clone API] Double-cloning is now working as expected.</li>
  <li>[PR #10486][assafad] Deprecation notice for the metrics listed in the PR. Please update your systems to use the new metrics names.</li>
  <li>[PR #10438][lyarwood] A new <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io:view</code> <code class="language-plaintext highlighter-rouge">ClusterRole</code> has been introduced that can be bound to users via a <code class="language-plaintext highlighter-rouge">ClusterRoleBinding</code> to provide read only access to the cluster scoped <code class="language-plaintext highlighter-rouge">VirtualMachineCluster{Instancetype,Preference}</code> resources.</li>
  <li>[PR #10477][jean-edouard] Dynamic KSM enabling and configuration</li>
  <li>[PR #10110][tiraboschi] Stream guest serial console logs from a dedicated container</li>
  <li>[PR #10015][victortoso] Implements USB host passthrough in permittedHostDevices of KubeVirt CRD</li>
  <li>[PR #10184][acardace] Add memory hotplug feature</li>
  <li>[PR #10044][machadovilaca] Add operator-observability package</li>
  <li>[PR #10489][maiqueb] Remove the network-attachment-definition <code class="language-plaintext highlighter-rouge">list</code> and <code class="language-plaintext highlighter-rouge">watch</code> verbs from virt-controller’s RBAC</li>
  <li>[PR #10450][0xFelix] virtctl: Enable inference in create vm subcommand by default</li>
  <li>[PR #10447][fossedihelm] Add a Feature Gate to KV CR to automatically set memory limits when a resource quota with memory limits is associated to the creation namespace</li>
  <li>[PR #10253][rmohr] Stop trying to create unused directory /var/run/kubevirt-ephemeral-disk in virt-controller</li>
  <li>[PR #10231][kvaps] Propogate public-keys to cloud-init NoCloud meta-data</li>
  <li>[PR #10400][alromeros] Add new vmexport flags to download raw images, either directly (–raw) or by decompressing (–decompress) them</li>
  <li>[PR #9673][germag] DownwardMetrics: Expose DownwardMetrics through virtio-serial channel.</li>
  <li>[PR #10086][vladikr] allow live updating VM affinity and node selector</li>
  <li>[PR #10050][victortoso] Updating the virt stack: QEMU 8.0.0, libvirt to 9.5.0, edk2 20230524,</li>
  <li>[PR #10370][benjx1990] N/A</li>
  <li>[PR #10391][awels] BugFix: VMExport now works in a namespace with quotas defined.</li>
  <li>[PR #10386][liuzhen21] KubeSphere added to the adopter’s file!</li>
  <li>[PR #10380][alromeros] Bugfix: Allow image-upload to recover from PendingPopulation phase</li>
  <li>[PR #10366][ormergi] Kubevirt now delegates Slirp networking configuration to Slirp network binding plugin.  In case you haven’t registered Slirp network binding plugin image yet (i.e.: specify in Kubevirt config) the following default image would be used: <code class="language-plaintext highlighter-rouge">quay.io/kubevirt/network-slirp-binding:20230830_638c60fc8</code>. On next release (v1.2.0) no default image will be set and registering an image would be mandatory.</li>
  <li>[PR #10167][0xFelix] virtctl: Apply namespace to created manifests</li>
  <li>[PR #10148][alromeros] Add port-forward functionalities to vmexport</li>
  <li>[PR #9821][sradco] Deprecation notice for the metrics listed in the PR. Please update your systems to use the new metrics names.</li>
  <li>[PR #10272][ormergi] Introduce network binding plugin for Slirp networking, interfacing with Kubevirt new network binding plugin API.</li>
  <li>[PR #10284][AlonaKaplan] Introduce an API for network binding plugins. The feature is behind “NetworkBindingPlugins” gate.</li>
  <li>[PR #10275][awels] Ensure new hotplug attachment pod is ready before deleting old attachment pod</li>
  <li>[PR #9231][victortoso] Introduces sidecar-shim container image</li>
  <li>[PR #10254][rmohr] Don’t mark the KubeVirt “Available” condition as false on up-to-date and ready but misscheduled virt-handler pods.</li>
  <li>[PR #10185][AlonaKaplan] Add support to migration based SRIOV hotplug.</li>
  <li>[PR #10182][iholder101] Stop considering nodes without <code class="language-plaintext highlighter-rouge">kubevirt.io/schedulable</code> label when finding lowest TSC frequency on the cluster</li>
  <li>[PR #10138][machadovilaca] Change kubevirt_vmi_*_usage_seconds from Gauge to Counter</li>
  <li>[PR #10173][rmohr]</li>
  <li>[PR #10101][acardace] Deprecate <code class="language-plaintext highlighter-rouge">spec.config.machineType</code> in KubeVirt CR.</li>
  <li>[PR #10020][akalenyu] Use auth API for DataVolumes, stop importing kubevirt.io/containerized-data-importer</li>
  <li>[PR #10107][PiotrProkop] Expose kubevirt_vmi_vcpu_delay_seconds_total reporting amount of seconds VM spent in  waiting in the queue instead of running.</li>
  <li>[PR #10099][iholder101] Bugfix: target virt-launcher pod hangs when migration is cancelled.</li>
  <li>[PR #10056][jean-edouard] UEFI guests now use Bochs display instead of VGA emulation</li>
  <li>[PR #10070][machadovilaca] Remove affinities label from kubevirt_vmi_cpu_affinity and use sum as value</li>
  <li>[PR #10165][awels] BugFix: deleting hotplug attachment pod will no longer detach volumes that were not removed.</li>
  <li>[PR #9878][jean-edouard] The EFI NVRAM can now be configured to persist across reboots</li>
  <li>[PR #9932][lyarwood] <code class="language-plaintext highlighter-rouge">ControllerRevisions</code> containing <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io</code> <code class="language-plaintext highlighter-rouge">CRDs</code> are now decorated with labels detailing specific metadata of the underlying stashed object</li>
  <li>[PR #10039][simonyangcj] fix guaranteed qos of virt-launcher pod broken when use virtiofs</li>
  <li>[PR #10116][ormergi] Existing detached interfaces with ‘absent’ state will be cleared from VMI spec.</li>
  <li>[PR #9982][fabiand] Introduce a support lifecycle and Kubernetes target version.</li>
  <li>[PR #10118][akalenyu] Change exportserver default UID to succeed exporting CDI standalone PVCs (not attached to VM)</li>
  <li>[PR #10106][acardace] Add boot-menu wait time when starting the VM as paused.</li>
  <li>[PR #10058][alicefr] Add field errorPolicy for disks</li>
  <li>[PR #10004][AlonaKaplan] Hoyplug/unplug interfaces should be done by updating the VM spec template. virtctl and REST API endpoints were removed.</li>
  <li>[PR #10067][iholder101] Bug fix: <code class="language-plaintext highlighter-rouge">virtctl create clone</code> marshalling and replacement of <code class="language-plaintext highlighter-rouge">kubectl</code> with <code class="language-plaintext highlighter-rouge">kubectl virt</code></li>
  <li>[PR #9989][alaypatel07] Add perf scale benchmarks for VMIs</li>
  <li>[PR #10001][machadovilaca] Fix kubevirt_vmi_phase_count not being created</li>
  <li>[PR #9896][ormergi] The VM controller now replicates spec interfaces MAC addresses to the corresponding interfaces in the VMI spec.</li>
  <li>[PR #9840][dhiller] Increase probability for flake checker script to find flakes</li>
  <li>[PR #9988][enp0s3] always deploy the outdated VMI workload alert</li>
  <li>[PR #7708][VirrageS] <code class="language-plaintext highlighter-rouge">nodeSelector</code> and <code class="language-plaintext highlighter-rouge">schedulerName</code> fields have been added to VirtualMachineInstancetype spec.</li>
  <li>[PR #7197][vasiliy-ul] Experimantal support of SEV attestation via the new API endpoints</li>
  <li>[PR #9958][AlonaKaplan] Disable network interface hotplug/unplug for VMIs. It will be supported for VMs only.</li>
  <li>[PR #9882][dhiller] Add some context for initial contributors about automated testing and draft pull requests.</li>
  <li>[PR #9935][xpivarc] Bug fix - correct logging in container disk</li>
  <li>[PR #9552][phoracek] gRPC client now works correctly with non-Go gRPC servers</li>
  <li>[PR #9918][ShellyKa13] Fix for hotplug with WFFC SCI storage class which uses CDI populators</li>
  <li>[PR #9737][AlonaKaplan] On hotunplug - remove bridge, tap and dummy interface from virt-launcher and the caches (file and volatile) from the node.</li>
  <li>[PR #9861][rmohr] Fix the possibility of data corruption when requestin a force-restart via “virtctl restart”</li>
  <li>[PR #9818][akrejcir] Added “virtctl credentials” commands to dynamically change SSH keys in a VM, and to set user’s password.</li>
  <li>[PR #9872][alromeros] Bugfix: Allow lun disks to be mapped to DataVolume sources</li>
  <li>[PR #9073][machadovilaca] Fix incorrect KubevirtVmHighMemoryUsage description</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.1.0 changes]]></summary></entry><entry><title type="html">Running KubeVirt with Cluster Autoscaler</title><link href="https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes.html" rel="alternate" type="text/html" title="Running KubeVirt with Cluster Autoscaler" /><published>2023-09-06T00:00:00+00:00</published><updated>2023-09-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes</id><content type="html" xml:base="https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>For this article, we’ll learn about the process of setting up
<a href="https://kubevirt.io/">KubeVirt</a> with <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">Cluster
Autoscaler</a>
on EKS. In addition, we’ll be using bare metal nodes to host KubeVirt VMs.</p>

<h2 id="required-base-knowledge">Required Base Knowledge</h2>

<p>This article will talk about how to make various software systems work together
but introducing each one in detail is outside of its scope. Thus, you must already:</p>

<ol>
  <li>Know how to administer a Kubernetes cluster;</li>
  <li>Be familiar with AWS, specifically IAM and EKS; and</li>
  <li>Have some experience with KubeVirt.</li>
</ol>

<h2 id="companion-code">Companion Code</h2>

<p>All the code used in this article may also be found at
<a href="https://github.com/relaxdiego/kubevirt-cas-baremetal">github.com/relaxdiego/kubevirt-cas-baremetal</a>.</p>

<h2 id="set-up-the-cluster">Set Up the Cluster</h2>

<h3 id="shared-environment-variables">Shared environment variables</h3>

<p>First let’s set some environment variables:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The name of the EKS cluster we're going to create</span>
<span class="nb">export </span><span class="nv">RD_CLUSTER_NAME</span><span class="o">=</span>my-cluster

<span class="c"># The region where we will create the cluster</span>
<span class="nb">export </span><span class="nv">RD_REGION</span><span class="o">=</span>us-west-2

<span class="c"># Kubernetes version to use</span>
<span class="nb">export </span><span class="nv">RD_K8S_VERSION</span><span class="o">=</span>1.27

<span class="c"># The name of the keypair that we're going to inject into the nodes. You</span>
<span class="c"># must create this ahead of time in the correct region.</span>
<span class="nb">export </span><span class="nv">RD_EC2_KEYPAIR_NAME</span><span class="o">=</span>eks-my-cluster
</code></pre></div></div>

<h3 id="prepare-the-clusteryaml-file">Prepare the cluster.yaml file</h3>

<p>Using <a href="https://eksctl.io/">eksctl</a>, prepare an EKS cluster config:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eksctl create cluster <span class="se">\</span>
    <span class="nt">--dry-run</span> <span class="se">\</span>
    <span class="nt">--name</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--nodegroup-name</span> ng-infra <span class="se">\</span>
    <span class="nt">--node-type</span> m5.xlarge <span class="se">\</span>
    <span class="nt">--nodes</span> 2 <span class="se">\</span>
    <span class="nt">--nodes-min</span> 2 <span class="se">\</span>
    <span class="nt">--nodes-max</span> 2 <span class="se">\</span>
    <span class="nt">--node-labels</span> <span class="nv">workload</span><span class="o">=</span>infra <span class="se">\</span>
    <span class="nt">--region</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--ssh-access</span> <span class="se">\</span>
    <span class="nt">--ssh-public-key</span> <span class="k">${</span><span class="nv">RD_EC2_KEYPAIR_NAME</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--version</span> <span class="k">${</span><span class="nv">RD_K8S_VERSION</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--vpc-nat-mode</span> HighlyAvailable <span class="se">\</span>
    <span class="nt">--with-oidc</span> <span class="se">\</span>
<span class="o">&gt;</span> cluster.yaml
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">--dry-run</code> means the command will not actually create the cluster but will
instead output a config to stdout which we then write to <code class="language-plaintext highlighter-rouge">cluster.yaml</code>.</p>

<p>Open the file and look at what it has produced.</p>

<blockquote>
  <p>For more info on the schema used by <code class="language-plaintext highlighter-rouge">cluster.yaml</code>, see the <a href="https://eksctl.io/usage/schema/">Config file
schema</a> page from eksctl.io</p>
</blockquote>

<p>This cluster will start out with a node group that we will use to host our
“infra” services. This is why we are using the cheaper <code class="language-plaintext highlighter-rouge">m5.xlarge</code> rather than
a baremetal instance type. However, we also need to ensure that none of our VMs
will ever be scheduled in these nodes. Thus we need to taint them. In the
generated <code class="language-plaintext highlighter-rouge">cluster.yaml</code> file, append the following taint to the only node
group in the <code class="language-plaintext highlighter-rouge">managedNodeGroups</code> list:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">managedNodeGroups</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">amiFamily</span><span class="pi">:</span> <span class="s">AmazonLinux2</span>
  <span class="s">...</span>
  <span class="na">taints</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">CriticalAddonsOnly</span>
      <span class="na">effect</span><span class="pi">:</span> <span class="s">NoSchedule</span>
</code></pre></div></div>

<h3 id="create-the-cluster">Create the cluster</h3>

<p>We can now create the cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eksctl create cluster <span class="nt">--config-file</span> cluster.yaml
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2023-08-20 07:59:14 [ℹ]  eksctl version ...
2023-08-20 07:59:14 [ℹ]  using region us-west-2 ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2a ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2b ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2c ...
...
2023-08-20 08:14:06 [ℹ]  kubectl command should work with ...
2023-08-20 08:14:06 [✔]  EKS cluster "my-cluster" in "us-west-2" is ready
</code></pre></div></div>

<p>Once the command is done, you should be able to query the the kube API. For
example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get nodes
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                      STATUS   ROLES    AGE     VERSION
ip-XXX.compute.internal   Ready    &lt;none&gt;   32m     v1.27.4-eks-2d98532
ip-YYY.compute.internal   Ready    &lt;none&gt;   32m     v1.27.4-eks-2d98532
</code></pre></div></div>

<h3 id="create-the-node-groups">Create the Node Groups</h3>

<p>As per <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">this section of the Cluster Autoscaler
docs</a>:</p>

<blockquote>
  <p>If you’re using Persistent Volumes, your deployment needs to run in the same
AZ as where the EBS volume is, otherwise the pod scheduling could fail if it
is scheduled in a different AZ and cannot find the EBS volume. To overcome
this, either use a single AZ ASG for this use case, or an ASG-per-AZ while
enabling <code class="language-plaintext highlighter-rouge">--balance-similar-node-groups</code>.</p>
</blockquote>

<p>Based on the above, we will create a node group for each of the availability
zones (AZs) that was declared in <code class="language-plaintext highlighter-rouge">cluster.yaml</code> so that the Cluster Autoscaler will
always bring up a node in the AZ where a VM’s EBS-backed PV is located.</p>

<p>To do that, we will first prepare a template that we can then feed to
<code class="language-plaintext highlighter-rouge">envsubst</code>. Save the following in <code class="language-plaintext highlighter-rouge">node-group.yaml.template</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="c1"># See: Config File Schema &lt;https://eksctl.io/usage/schema/&gt;</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">eksctl.io/v1alpha5</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterConfig</span>

<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">${RD_CLUSTER_NAME}</span>
  <span class="na">region</span><span class="pi">:</span> <span class="s">${RD_REGION}</span>

<span class="na">managedNodeGroups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
    <span class="na">amiFamily</span><span class="pi">:</span> <span class="s">AmazonLinux2</span>
    <span class="na">instanceType</span><span class="pi">:</span> <span class="s">c5.metal</span>
    <span class="na">availabilityZones</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">${EKS_AZ}</span>
    <span class="na">desiredCapacity</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">maxSize</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">minSize</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">alpha.eksctl.io/cluster-name</span><span class="pi">:</span> <span class="s">my-cluster</span>
      <span class="na">alpha.eksctl.io/nodegroup-name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
      <span class="na">workload</span><span class="pi">:</span> <span class="s">vm</span>
    <span class="na">privateNetworking</span><span class="pi">:</span> <span class="no">false</span>
    <span class="na">ssh</span><span class="pi">:</span>
      <span class="na">allow</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">publicKeyPath</span><span class="pi">:</span> <span class="s">${RD_EC2_KEYPAIR_NAME}</span>
    <span class="na">volumeSize</span><span class="pi">:</span> <span class="m">500</span>
    <span class="na">volumeIOPS</span><span class="pi">:</span> <span class="m">10000</span>
    <span class="na">volumeThroughput</span><span class="pi">:</span> <span class="m">750</span>
    <span class="na">volumeType</span><span class="pi">:</span> <span class="s">gp3</span>
    <span class="na">propagateASGTags</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">tags</span><span class="pi">:</span>
      <span class="na">alpha.eksctl.io/nodegroup-name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
      <span class="na">alpha.eksctl.io/nodegroup-type</span><span class="pi">:</span> <span class="s">managed</span>
      <span class="na">k8s.io/cluster-autoscaler/my-cluster</span><span class="pi">:</span> <span class="s">owned</span>
      <span class="na">k8s.io/cluster-autoscaler/enabled</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
      <span class="c1"># The following tags help CAS determine that this node group is able</span>
      <span class="c1"># to satisfy the label and resource requirements of the KubeVirt VMs.</span>
      <span class="c1"># See: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/label/kubevirt.io/schedulable</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
</code></pre></div></div>

<p>The last few tags bears additional emphasis. They are required because when a
virtual machine is created, it will have the following requirements:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">requests</span><span class="pi">:</span>
  <span class="na">devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>

<span class="na">nodeSelectors</span><span class="pi">:</span> <span class="s">kubevirt.io/schedulable=true</span>
</code></pre></div></div>

<p>However, at least when scaling from zero for the first time, CAS will have no
knowledge of this information unless the correct AWS tags are added to the node
group. This is why we have the following added to the managed node group’s
tags:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/label/kubevirt.io/schedulable</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
</code></pre></div></div>

<blockquote>
  <p>For more information on these tags, see <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup">Auto-Discovery
Setup</a>.</p>
</blockquote>

<h3 id="create-the-vm-node-groups">Create the VM Node Groups</h3>

<p>We can now create the node group:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yq .availabilityZones[] cluster.yaml <span class="nt">-r</span> | <span class="se">\</span>
    xargs <span class="nt">-I</span><span class="o">{}</span> bash <span class="nt">-c</span> <span class="s2">"
        export EKS_AZ={};
        envsubst &lt; node-group.yaml.template | </span><span class="se">\</span><span class="s2">
        eksctl create nodegroup --config-file -
    "</span>
</code></pre></div></div>

<h2 id="deploy-kubevirt">Deploy KubeVirt</h2>

<blockquote>
  <p>The following was adapted from <a href="https://kubevirt.io/quickstart_cloud/">KubeVirt quickstart with cloud
providers</a>.</p>
</blockquote>

<p>Deploy the KubeVirt operator:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/v1.0.0/kubevirt-operator.yaml
</code></pre></div></div>

<p>So that the operator will know how to deploy KubeVirt, let’s add the <code class="language-plaintext highlighter-rouge">KubeVirt</code>
resource:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: kubevirt.io/v1
kind: KubeVirt
metadata:
  name: kubevirt
  namespace: kubevirt
spec:
  certificateRotateStrategy: {}
  configuration:
    developerConfiguration:
      featureGates: []
  customizeComponents: {}
  imagePullPolicy: IfNotPresent
  workloadUpdateStrategy: {}
  infra:
    nodePlacement:
      nodeSelector:
        workload: infra
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
</span><span class="no">EOF
</span></code></pre></div></div>

<blockquote>
  <p>Notice how we are specifically configuring KubeVirt itself to tolerate the
<code class="language-plaintext highlighter-rouge">CriticalAddonsOnly</code> taint. This is so that the KubeVirt services themselves
can be scheduled in the infra nodes instead of the bare metal nodes which we
want to scale down to zero when there are no VMs.</p>
</blockquote>

<p>Wait until KubeVirt is in a <code class="language-plaintext highlighter-rouge">Deployed</code> state:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get <span class="nt">-n</span> kubevirt <span class="nt">-o</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.phase}"</span> <span class="se">\</span>
	kubevirt.kubevirt.io/kubevirt
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Deployed
</code></pre></div></div>

<p>Double check that all KubeVirt components are healthy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-n</span> kubevirt
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                 READY   STATUS    RESTARTS       AGE
pod/virt-api-674467958c-5chhj        1/1     Running   0              98d
pod/virt-api-674467958c-wzcmk        1/1     Running   0              5d
pod/virt-controller-6768977b-49wwb   1/1     Running   0              98d
pod/virt-controller-6768977b-6pfcm   1/1     Running   0              5d
pod/virt-handler-4hztq               1/1     Running   0              5d
pod/virt-handler-x98x5               1/1     Running   0              98d
pod/virt-operator-85f65df79b-lg8xb   1/1     Running   0              5d
pod/virt-operator-85f65df79b-rp8p5   1/1     Running   0              98d
</code></pre></div></div>

<h2 id="deploy-a-vm-to-test">Deploy a VM to test</h2>

<blockquote>
  <p>The following is copied from
<a href="https://kubevirt.io/user-guide/virtual_machines/accessing_virtual_machines/#static-ssh-public-key-injection-via-cloud-init">kubevirt.io</a>.</p>
</blockquote>

<p>First create a secret from your public key:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create secret generic my-pub-key <span class="nt">--from-file</span><span class="o">=</span><span class="nv">key1</span><span class="o">=</span>~/.ssh/id_rsa.pub
</code></pre></div></div>

<p>Next, create the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a VM referencing the Secret using propagation method configDrive</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: testvm
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          rng: {}
        resources:
          requests:
            memory: 1024M
      terminationGracePeriodSeconds: 0
      accessCredentials:
      - sshPublicKey:
          source:
            secret:
              secretName: my-pub-key
          propagationMethod:
            configDrive: {}
      volumes:
      - containerDisk:
          image: quay.io/containerdisks/fedora:latest
        name: containerdisk
      - cloudInitConfigDrive:
          userData: |-
            #cloud-config
            password: fedora
            chpasswd: { expire: False }
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Check that the test VM is running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get vm
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME        AGE     STATUS               READY
testvm      30s     Running              True
</code></pre></div></div>

<p>Delete the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl delete testvm
</code></pre></div></div>

<h2 id="set-up-cluster-autoscaler">Set Up Cluster Autoscaler</h2>

<h3 id="prepare-the-permissions-for-cluster-autoscaler">Prepare the permissions for Cluster Autoscaler</h3>

<p>So that CAS can set the desired capacity of each node group dynamically, we
must grant it limited access to certain AWS resources. The first step to this
is to define the IAM policy.</p>

<blockquote>
  <p>This section is based off of the “Create an IAM policy and role” section of
the <a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html">AWS
Autoscaling</a>
documentation.</p>
</blockquote>

<h3 id="create-the-cluster-specific-policy-document">Create the cluster-specific policy document</h3>

<p>Prepare the policy document by rendering the following file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> policy.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "autoscaling:SetDesiredCapacity",
                "autoscaling:TerminateInstanceInAutoScalingGroup"
            ],
            "Resource": "*"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeAutoScalingInstances",
                "autoscaling:DescribeAutoScalingGroups",
                "ec2:DescribeLaunchTemplateVersions",
                "autoscaling:DescribeTags",
                "autoscaling:DescribeLaunchConfigurations",
                "ec2:DescribeInstanceTypes"
            ],
            "Resource": "*"
        }
    ]
}
</span><span class="no">EOF
</span></code></pre></div></div>

<p>The above should be enough for CAS to do its job. Next, create the policy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws iam create-policy <span class="se">\</span>
    <span class="nt">--policy-name</span> eks-<span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span>-<span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span><span class="nt">-ClusterAutoscalerPolicy</span> <span class="se">\</span>
    <span class="nt">--policy-document</span> file://policy.json
</code></pre></div></div>

<blockquote>
  <p>IMPORTANT: Take note of the returned policy ARN. You will need that below.</p>
</blockquote>

<h3 id="create-the-iam-role-and-k8s-service-account-pair">Create the IAM role and k8s service account pair</h3>

<p>The Cluster Autoscaler needs a service account in the k8s cluster that’s
associated with an IAM role that consumes the policy document we created in the
previous section. This is normally a two-step process but can be created in a
single command using <code class="language-plaintext highlighter-rouge">eksctl</code>:</p>

<blockquote>
  <p>For more information on what <code class="language-plaintext highlighter-rouge">eksctl</code> is doing under the covers, see <a href="https://eksctl.io/usage/iamserviceaccounts/#how-it-works">How It
Works</a> from the
<code class="language-plaintext highlighter-rouge">eksctl</code> documentation for IAM Roles for Service Accounts.</p>
</blockquote>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RD_POLICY_ARN</span><span class="o">=</span><span class="s2">"&lt;Get this value from the last command's output&gt;"</span>

eksctl create iamserviceaccount <span class="se">\</span>
	<span class="nt">--cluster</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--region</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--namespace</span><span class="o">=</span>kube-system <span class="se">\</span>
	<span class="nt">--name</span><span class="o">=</span>cluster-autoscaler <span class="se">\</span>
	<span class="nt">--attach-policy-arn</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_POLICY_ARN</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--override-existing-serviceaccounts</span> <span class="se">\</span>
	<span class="nt">--approve</span>
</code></pre></div></div>

<p>Double check that the <code class="language-plaintext highlighter-rouge">cluster-autoscaler</code> service account has been correctly
annotated with the IAM role that was created by <code class="language-plaintext highlighter-rouge">eksctl</code> in the same step:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get sa cluster-autoscaler <span class="nt">-n</span> kube-system <span class="nt">-ojson</span> | <span class="se">\</span>
	jq <span class="nt">-r</span> <span class="s1">'.metadata.annotations | ."eks.amazonaws.com/role-arn"'</span>
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>arn:aws:iam::365499461711:role/eksctl-my-cluster-addon-iamserviceaccount-...
</code></pre></div></div>

<p>Check from the AWS Console if the above role contains the policy that we created
earlier.</p>

<h3 id="deploy-cluster-autoscaler">Deploy Cluster Autoscaler</h3>

<p>First, find the most recent Cluster Autoscaler version that has the same MAJOR
and MINOR version as the kubernetes cluster you’re deploying to.</p>

<p>Get the kube cluster’s version:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl version <span class="nt">-ojson</span> | jq <span class="nt">-r</span> .serverVersion.gitVersion
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v1.27.4-eks-2d98532
</code></pre></div></div>

<p>Choose the appropriate version for CAS. You can get the latest Cluster
Autoscaler versions from its <a href="https://github.com/kubernetes/autoscaler/releases?q=cluster-autoscaler+1&amp;expanded=true">Github Releases
Page</a>.</p>

<p>Example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">CLUSTER_AUTOSCALER_VERSION</span><span class="o">=</span>1.27.3
</code></pre></div></div>

<p>Next, deploy the cluster autoscaler using the deployment template that I
prepared in the <a href="https://github.com/relaxdiego/kubevirt-cas-baremetal">companion
repo</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>envsubst &lt; &lt;<span class="o">(</span>curl https://raw.githubusercontent.com/relaxdiego/kubevirt-cas-baremetal/main/cas-deployment.yaml.template<span class="o">)</span> | <span class="se">\</span>
  kubectl apply <span class="nt">-f</span> -
</code></pre></div></div>

<p>Check the cluster autoscaler status:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get deploy,pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>cluster-autoscaler <span class="nt">-n</span> kube-system
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cluster-autoscaler   1/1     1            1           4m1s

NAME                                      READY   STATUS    RESTARTS   AGE
pod/cluster-autoscaler-6c58bd6d89-v8wbn   1/1     Running   0          60s
</code></pre></div></div>

<p>Tail the <code class="language-plaintext highlighter-rouge">cluster-autoscaler</code> pod’s logs to see what’s happening:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> kube-system logs <span class="nt">-f</span> deployment.apps/cluster-autoscaler
</code></pre></div></div>

<p>Below are example log entries from Cluster Autoscaler terminating an unneeded
node:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>node ip-XXXX.YYYY.compute.internal may be removed
...
ip-XXXX.YYYY.compute.internal was unneeded for 1m3.743475455s
</code></pre></div></div>

<p>Once the timeout has been reached (default: 10 minutes), CAS will scale down
the group:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scale-down: removing empty node ip-XXXX.YYYY.compute.internal
Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"kube-system", ...
Successfully added ToBeDeletedTaint on node ip-XXXX.YYYY.compute.internal
Terminating EC2 instance: i-ZZZZ
DeleteInstances was called: ...
</code></pre></div></div>

<blockquote>
  <p>For more information on how Cluster Autoscaler scales down a node group, see
<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work">How does scale-down
work?</a>
from the project’s FAQ.</p>
</blockquote>

<p>When you try to get the list of nodes, you should see the bare metal nodes
tainted such that they are no longer schedulable:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME       STATUS                     ROLES    AGE    VERSION
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready                      &lt;none&gt;   112m   v1.27.3-eks-a5565ad
ip-XXXX    Ready                      &lt;none&gt;   112m   v1.27.3-eks-a5565ad
</code></pre></div></div>

<p>In a few more minutes, the nodes will be deleted.</p>

<p>To try the scale up, just deploy a VM.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Expanding Node Group eks-ng-eacf8ebb ...
Best option to resize: eks-ng-eacf8ebb
Estimated 1 nodes needed in eks-ng-eacf8ebb
Final scale-up plan: [{eks-ng-eacf8ebb 0-&gt;1 (max: 3)}]
Scale-up: setting group eks-ng-eacf8ebb size to 1
Setting asg eks-ng-eacf8ebb size to 1
</code></pre></div></div>

<h2 id="done">Done</h2>

<p>At this point you should have a working, auto-scaling EKS cluster that can host
VMs on bare metal nodes. If you have any questions, ask them
<a href="https://github.com/relaxdiego/relaxdiego.github.com/discussions/new?category=general">here</a>.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html">Amazon EKS Autoscaling</a></li>
  <li><a href="https://aws.plainenglish.io/cluster-autoscaler-amazon-eks-7ffaa24e5938">Cluster Autoscaler in Plain English</a></li>
  <li><a href="https://aws.github.io/aws-eks-best-practices/">AWS EKS Best Practices Guide</a></li>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html">IAM roles for service accounts</a></li>
  <li><a href="https://eksctl.io/usage/iamserviceaccounts/">eksctl create iamserviceaccount</a></li>
</ul>]]></content><author><name>Mark Maglana, Jonathan Kinred, Paul Myjavec</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="Cluster Autoscaler" /><category term="AWS" /><category term="EKS" /><summary type="html"><![CDATA[This post explains how to set up KubeVirt with Cluster Autoscaler on EKS]]></summary></entry><entry><title type="html">Managing KubeVirt VMs with Ansible</title><link href="https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible.html" rel="alternate" type="text/html" title="Managing KubeVirt VMs with Ansible" /><published>2023-09-05T00:00:00+00:00</published><updated>2023-09-05T00:00:00+00:00</updated><id>https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible</id><content type="html" xml:base="https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Infrastructure teams managing virtual machines (VMs) and the end users of these systems make use of a variety of tools as part of their day-to-day world. One such tool that is shared amongst these two groups is Ansible, an agentless automation tool for the enterprise. To simplify both the adoption and usage of KubeVirt as well as to integrate seamlessly into existing workflows, the KubeVirt community is excited to introduce the release of the first version of the KubeVirt collection for <a href="https://docs.ansible.com/ansible/latest/index.html">Ansible</a>, called <code class="language-plaintext highlighter-rouge">kubevirt.core</code>, which includes a number of tools that you do not want to miss.</p>

<p>This article will review some of the features and their use associated with this initial release.</p>

<p>Note: There is also a video version of this blog, which can be found on the <a href="https://youtu.be/GVROaPgJD_8">KubeVirt YouTube channel</a>.</p>

<h2 id="motivation">Motivation</h2>

<p>Before diving into the featureset of the collection itself, let’s review why the collection was created in the first place.</p>

<p>While adopting KubeVirt and Kubernetes has the potential to disrupt the workflows of teams that typically manage VM infrastructure, including the end users themselves, many of the same paradigms remain:</p>

<ul>
  <li>Kubernetes and the resources associated with KubeVirt can be represented in a declarative fashion.</li>
  <li>In many cases, communicating with KubeVirt VMs makes use of the same protocols and schemes as non-Kubernetes-based environments.</li>
  <li>The management of VMs still represents a challenge.</li>
</ul>

<p>For these reasons and more, it is only natural that a tool, like Ansible, is introduced within the KubeVirt community. Not only can it help manage KubeVirt and Kubernetes resources, like <code class="language-plaintext highlighter-rouge">VirtualMachines</code>, but also to enable the extensive Ansible ecosystem for managing guest configurations.</p>

<h2 id="included-capabilities">Included capabilities</h2>

<p>As part of the initial release, an <a href="https://docs.ansible.com/ansible/latest/plugins/inventory.html">Ansible Inventory plugin</a> and management module is included. They are available in the same distribution location containing Ansible automation content, <a href="https://galaxy.ansible.com/kubevirt/core">Ansible Galaxy</a>. The resources encompassing the collection itself are detailed in the following sections.</p>

<h3 id="inventory">Inventory</h3>

<p>To work with KubeVirt VMs in Ansible, they need to be available in Ansible’s hosts <a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html">inventory</a>. Since KubeVirt is already using the Kubernetes API to manage VMs, it would be nice to leverage this API to discover hosts with Ansible too. This is where the <a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_dynamic_inventory.html">dynamic inventory</a> of the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection comes into play.</p>

<p>The dynamic inventory capability allows you to query the Kubernetes API for available VMs in a given namespace or namespaces, along with additional filtering options, such as labels. To allow Ansible to find the right connection parameters for a VM, the network name of a secondary interface can also be specified.</p>

<p>Under the hood, the dynamic inventory uses either your default kubectl credentials or credentials specified in the inventory parameters to establish the connection with a cluster.</p>

<h3 id="managing-vms">Managing VMs</h3>

<p>While working with existing VMs is already quite useful, it would be even better to control the entire lifecycle of KubeVirt <code class="language-plaintext highlighter-rouge">VirtualMachines</code> from Ansible. This is made possible by the <code class="language-plaintext highlighter-rouge">kubevirt_vm</code> module provided by the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection.</p>

<p>The <code class="language-plaintext highlighter-rouge">kubevirt_vm</code> module is a thin wrapper around the <a href="https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html">kubernetes.core.k8s</a> module and it allows you to control the essential fields of a KubeVirt <code class="language-plaintext highlighter-rouge">VirtualMachine</code>’s specification. In true Ansible fashion, this module tries to be as idempotent as possible and only makes changes to objects within Kubernetes if necessary. With its <code class="language-plaintext highlighter-rouge">wait</code> feature, it is possible to delay further tasks until a VM was successfully created or updated and the VM is in the ready state or was successfully deleted.</p>

<h2 id="getting-started">Getting started</h2>

<p>Now that we’ve provided an introduction to the featureset, it is time to illustrate how you can get up to speed using the collection including a few examples to showcase the capabilities provided by the collection.</p>

<h3 id="prerequisites">Prerequisites</h3>

<p>Please note that as a prerequisite, Ansible needs to be installed and configured along with a working Kubernetes cluster with KubeVirt and the <a href="https://github.com/kubevirt/cluster-network-addons-operator">KubeVirt Cluster Network Addons Operator</a>. The cluster also needs to have a <a href="https://kubevirt.io/user-guide/virtual_machines/interfaces_and_networks/#bridge">secondary network configured</a>, which can be attached to VMs so that the machine can be reached from the Ansible control node.</p>

<h3 id="items-covered">Items covered</h3>

<ol>
  <li>Installing the collection from Ansible Galaxy</li>
  <li>Creating a Namespace and a Secret with an SSH public key</li>
  <li>Creating a VM</li>
  <li>Listing available VMs</li>
  <li>Executing a command on the VM</li>
  <li>Removing the previously created resources</li>
</ol>

<h3 id="walkthrough">Walkthrough</h3>

<p>First, install the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection from Ansible Galaxy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-galaxy collection <span class="nb">install </span>kubevirt.core
</code></pre></div></div>

<p>This will also install the <code class="language-plaintext highlighter-rouge">kubernetes.core</code> collection as a dependency.</p>

<p>Second, create a new Namespace and a Secret containing a public key for SSH authentication:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-f</span> my-key
kubectl create namespace kubevirt-ansible
kubectl create secret generic my-pub-key <span class="nt">--from-file</span><span class="o">=</span><span class="nv">key1</span><span class="o">=</span>my-key.pub <span class="nt">-n</span> kubevirt-ansible
</code></pre></div></div>

<p>With the collection now installed and the public key pair created, create a file called <code class="language-plaintext highlighter-rouge">play-create.yml</code> containing an Ansible playbook to deploy a new VM called <code class="language-plaintext highlighter-rouge">testvm</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span> <span class="s">localhost</span>
  <span class="na">connection</span><span class="pi">:</span> <span class="s">local</span>
  <span class="na">tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Create VM</span>
    <span class="na">kubevirt.core.kubevirt_vm</span><span class="pi">:</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">present</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">testvm</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">test</span>
      <span class="na">instancetype</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">u1.medium</span>
      <span class="na">preference</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
      <span class="na">spec</span><span class="pi">:</span>
        <span class="na">domain</span><span class="pi">:</span>
          <span class="na">devices</span><span class="pi">:</span>
            <span class="na">interfaces</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
              <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">secondary-network</span>
              <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">networks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
          <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">secondary-network</span>
          <span class="na">multus</span><span class="pi">:</span>
            <span class="na">networkName</span><span class="pi">:</span> <span class="s">secondary-network</span>
        <span class="na">accessCredentials</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">sshPublicKey</span><span class="pi">:</span>
            <span class="na">source</span><span class="pi">:</span>
              <span class="na">secret</span><span class="pi">:</span>
                <span class="na">secretName</span><span class="pi">:</span> <span class="s">my-pub-key</span>
            <span class="na">propagationMethod</span><span class="pi">:</span>
              <span class="na">configDrive</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/containerdisks/fedora:latest</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
        <span class="pi">-</span> <span class="na">cloudInitConfigDrive</span><span class="pi">:</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s"># The default username is: fedora</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
      <span class="na">wait</span><span class="pi">:</span> <span class="s">yes</span>
</code></pre></div></div>

<p>Run the playbook by executing the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook play-create.yml
</code></pre></div></div>

<p>Once the playbook completes successfully, the defined VM will be running in the <code class="language-plaintext highlighter-rouge">kubevirt-ansible</code> namespace, which can be confirmed by querying for <code class="language-plaintext highlighter-rouge">VirtualMachines</code> in this namespace:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get VirtualMachine <span class="nt">-n</span> kubevirt-ansible
</code></pre></div></div>

<p>With the VM deployed, it is eligible for use in Ansible automation activities. Let’s illustrate how it can be queried and added to an Ansible inventory dynamically using the plugin provided by the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection.</p>

<p>Create a file called <code class="language-plaintext highlighter-rouge">inventory.kubevirt.yml</code> containing the following content:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">plugin</span><span class="pi">:</span> <span class="s">kubevirt.core.kubevirt</span>
<span class="na">connections</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">namespaces</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">kubevirt-ansible</span>
  <span class="na">network_name</span><span class="pi">:</span> <span class="s">secondary-network</span>
  <span class="na">label_selector</span><span class="pi">:</span> <span class="s">app=test</span>
</code></pre></div></div>

<p>Use the <code class="language-plaintext highlighter-rouge">ansible-inventory</code> command to confirm the VM becomes added to the Ansible inventory:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-inventory <span class="nt">-i</span> inventory.kubevirt.yml <span class="nt">--list</span>
</code></pre></div></div>

<p>Next, make use of the host by querying for all of the facts exposed by the VM using the setup module:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible <span class="nt">-i</span> inventory.kubevirt.yml <span class="nt">-u</span> fedora <span class="nt">--key-file</span> my-key all <span class="nt">-m</span> setup
</code></pre></div></div>

<p>Complete the lifecycle of the VM by destroying the previously created <code class="language-plaintext highlighter-rouge">VirtualMachine</code> and <code class="language-plaintext highlighter-rouge">Namespace</code>. Create a file called <code class="language-plaintext highlighter-rouge">play-delete.yml</code> containing the following playbook:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span> <span class="s">localhost</span>
  <span class="na">tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Delete VM</span>
    <span class="na">kubevirt.core.kubevirt_vm</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">testvm</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">absent</span>
      <span class="na">wait</span><span class="pi">:</span> <span class="s">yes</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Delete namespace</span>
    <span class="na">kubernetes.core.k8s</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">api_version</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">kind</span><span class="pi">:</span> <span class="s">Namespace</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">absent</span>
</code></pre></div></div>

<p>Run the playbook to remove the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook play-delete.yml
</code></pre></div></div>

<p>More information including the full list of parameters and options can be found within the collection documentation:</p>

<p><a href="https://kubevirt.io/kubevirt.core">https://kubevirt.io/kubevirt.core</a></p>

<h2 id="what-next">What next?</h2>

<p>This has been a brief introduction to the concepts and usage of the newly released <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection. Nevertheless, we hope that it helped to showcase the integration now available between KubeVirt and Ansible, including how easy it is to manage KubeVirt assets. A next potential iteration could be to expose a VM via a Kubernetes <code class="language-plaintext highlighter-rouge">Service</code> using one of the methods described in <a href="https://kubevirt.io/user-guide/virtual_machines/service_objects/#service-objects">this article</a> instead of a secondary interface as was covered in this walkthrough. Not only does it leverage existing models outside the KubeVirt ecosystem, but it helps to enable a uniform method for exposing content.</p>

<p>Interested in learning more, providing feedback or contributing? Head over to the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> GitHub repository to continue your journey and get involved.</p>

<p><a href="https://github.com/kubevirt/kubevirt.core">https://github.com/kubevirt/kubevirt.core</a></p>]]></content><author><name>Felix Matouschek, Andrew Block</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="Ansible" /><category term="ansible collection" /><category term="kubevirt.core" /><category term="iac" /><summary type="html"><![CDATA[This post explains how to manage KubeVirt VMs with the kubevirt.core Ansible collection.]]></summary></entry><entry><title type="html">NetworkPolicies for KubeVirt VMs secondary networks using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies.html" rel="alternate" type="text/html" title="NetworkPolicies for KubeVirt VMs secondary networks using OVN-Kubernetes" /><published>2023-07-24T00:00:00+00:00</published><updated>2023-07-24T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">NetworkPolicies</a> are constructs to control traffic flow at the IP
address or port level (OSI layers 3 or 4).
They allow the user to specify how a pod (or group of pods) is allowed to
communicate with other entities on the network. In simpler words: the user can
specify ingress from or egress to other workloads, using L3 / L4 semantics.</p>

<p>Keeping in mind <code class="language-plaintext highlighter-rouge">NetworkPolicy</code> is a Kubernetes construct - which only cares
about a single network interface - they are only usable for the cluster’s
default network interface. This leaves a considerable gap for Virtual Machine
users, since they are heavily invested in secondary networks.</p>

<p>The <a href="https://github.com/k8snetworkplumbingwg">k8snetworkplumbingwg</a> has addressed this limitation by providing a
<code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> CRD - it features the exact same API as <code class="language-plaintext highlighter-rouge">NetworkPolicy</code>
but can target <a href="https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/how-to-use.md#create-network-attachment-definition">network-attachment-definitions</a>.
<a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a> implements this API, and configures access control accordingly
for secondary networks in the cluster.</p>

<p>In this post we will see how we can govern access control for VMs using the
multi-network policy API. On our simple example, we’ll only allow into our VMs
for traffic ingressing from a particular CIDR range.</p>

<h2 id="current-limitations-of-multinetworkpolicies-for-vms">Current limitations of <code class="language-plaintext highlighter-rouge">MultiNetworkPolicies</code> for VMs</h2>
<p>Kubernetes <code class="language-plaintext highlighter-rouge">NetworkPolicy</code> has three types of policy peers:</p>
<ul>
  <li>namespace selectors: allows ingress-from, egress-to based on the peer’s namespace labels</li>
  <li>pod selectors: allows ingress-from, egress-to based on the peer’s labels</li>
  <li>ip block: allows ingress-from, egress-to based on the peer’s IP address</li>
</ul>

<p>While <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> allows these three types, when used with VMs we
recommend using <strong>only</strong> the <code class="language-plaintext highlighter-rouge">IPBlock</code> policy peer - both <code class="language-plaintext highlighter-rouge">namespace</code> and <code class="language-plaintext highlighter-rouge">pod</code>
selectors prevent the live-migration of Virtual Machines (these policy peers
require OVN-K managed IPAM, and currently the live-migration feature is only
available when IPAM is not enabled on the interfaces).</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, we will prepare a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li><a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a></li>
  <li><a href="https://github.com/kubevirt/kubevirt">KubeVirt</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multi-networkpolicy">Multi-Network policy API</a></li>
</ul>

<p>The <a href="#setup-demo-environment">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
upstream latest multus-cni, and the multi-network policy CRDs deployed.</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster (one control plane, and two worker
nodes), configured to use OVN-Kubernetes as the default cluster network,
configuring the multi-homing OVN-Kubernetes feature gate, and deploying
<code class="language-plaintext highlighter-rouge">multus-cni</code> in the cluster.</p>

<h3 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h3>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v1.0.0).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="limiting-ingress-to-a-kubevirt-vm">Limiting ingress to a KubeVirt VM</h3>
<p>In this example, we will configure a <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> allowing ingress into
our VMs only from a particular CIDR range - let’s say <code class="language-plaintext highlighter-rouge">10.200.0.0/30</code>.</p>

<p>Provision the following NAD (to allow our VMs to live-migrate, we do not define
a <code class="language-plaintext highlighter-rouge">subnet</code>):</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2net</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.4.0",</span>
            <span class="s">"name": "flatl2net",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology":"layer2",</span>
            <span class="s">"netAttachDefName": "default/flatl2net"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p>Let’s now provision our six VMs, with the following name to IP address
(statically configured via cloud-init) association:</p>
<ul>
  <li>vm1: 10.200.0.1</li>
  <li>vm2: 10.200.0.2</li>
  <li>vm3: 10.200.0.3</li>
  <li>vm4: 10.200.0.4</li>
  <li>vm5: 10.200.0.5</li>
  <li>vm6: 10.200.0.6</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm1</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm1</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm1</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.1/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm2</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm2</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm2</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.2/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm3</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm3</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm3</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.3/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm4</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm4</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm4</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm4</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.4/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm5</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm5</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm5</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm5</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.5/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm6</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm6</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm6</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm6</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.6/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
</code></pre></div></div>
<p><strong>NOTE:</strong> it is important to highlight all the Virtual Machines (and the
<code class="language-plaintext highlighter-rouge">network-attachment-definition</code>) are defined in the <code class="language-plaintext highlighter-rouge">default</code> namespace.</p>

<p>After this step, we should have the following deployment:</p>

<p><img src="/assets/2023-07-10-OVN-kubernetes-secondary-networks-policies/01-vms-provisioned.png" alt="image" /></p>

<p>Let’s check the VMs <code class="language-plaintext highlighter-rouge">vm1</code> and <code class="language-plaintext highlighter-rouge">vm4</code> can ping their peers in the same subnet.
For that we will
<a href="https://kubevirt.io/user-guide/virtual_machines/accessing_virtual_machines/#accessing-the-serial-console">connect to the VMs over their serial console</a>:</p>

<p>First, let’s check <code class="language-plaintext highlighter-rouge">vm1</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  virtctl console vm1
Successfully connected to vm1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.2 <span class="nt">-c</span> 4
PING 10.200.0.2 <span class="o">(</span>10.200.0.2<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>5.16 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.41 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>34.2 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>2.56 ms

<span class="nt">---</span> 10.200.0.2 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3005ms
rtt min/avg/max/mdev <span class="o">=</span> 1.406/10.841/34.239/13.577 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.77 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.46 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>5.47 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.74 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3007ms
rtt min/avg/max/mdev <span class="o">=</span> 1.459/3.109/5.469/1.627 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>
</code></pre></div></div>

<p>And from vm4:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  ~ virtctl console vm4
Successfully connected to vm4 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.1 <span class="nt">-c</span> 4
PING 10.200.0.1 <span class="o">(</span>10.200.0.1<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.20 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.62 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.44 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.951 ms

<span class="nt">---</span> 10.200.0.1 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 0.951/1.803/3.201/0.843 ms
<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.85 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.02 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.27 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.970 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3005ms
rtt min/avg/max/mdev <span class="o">=</span> 0.970/1.275/1.850/0.350 ms
</code></pre></div></div>

<p>We will now provision a <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> applying to all the VMs defined
above. To do this mapping correcly, the policy has to:</p>
<ul>
  <li>Be in the same namespace as the VM.</li>
  <li>Set <code class="language-plaintext highlighter-rouge">k8s.v1.cni.cncf.io/policy-for</code> annotation matching the secondary 
network used by the VM.</li>
  <li>Set <code class="language-plaintext highlighter-rouge">matchLabels</code> selector matching the labels set on VM’s
<code class="language-plaintext highlighter-rouge">spec.template.metadata</code>.</li>
</ul>

<p>This policy will allow ingress into these <code class="language-plaintext highlighter-rouge">access-control</code> labeled pods 
<strong>only if</strong> the traffic originates from within the <code class="language-plaintext highlighter-rouge">10.200.0.0/30</code> CIDR range
(IPs 10.200.0.1-3).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">---</span>
apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name:  ingress-ipblock
  annotations:
    k8s.v1.cni.cncf.io/policy-for: default/flatl2net
spec:
  podSelector:
    matchLabels:
        name: access-control
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.200.0.0/30
</code></pre></div></div>

<p>Taking into account our example, only
<code class="language-plaintext highlighter-rouge">vm1</code>, <code class="language-plaintext highlighter-rouge">vm2</code>, and <code class="language-plaintext highlighter-rouge">vm3</code> will be able to contact any of its peers, as pictured
by the following diagram:</p>

<p><img src="/assets/2023-07-10-OVN-kubernetes-secondary-networks-policies/02-no-access.png" alt="MultiNetworkPolicy is provisioned" /></p>

<p>Let’s try again the ping after provisioning the <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> object:</p>

<p>From <code class="language-plaintext highlighter-rouge">vm1</code> (inside the allowed ip block range):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.2 <span class="nt">-c</span> 4
PING 10.200.0.2 <span class="o">(</span>10.200.0.2<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>6.48 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>4.40 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.28 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.51 ms

<span class="nt">---</span> 10.200.0.2 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 1.283/3.418/6.483/2.154 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.81 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>2.67 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.68 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.63 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 1.630/2.446/3.808/0.888 ms
</code></pre></div></div>

<p>From <code class="language-plaintext highlighter-rouge">vm4</code> (<strong>outside</strong> the allowed ip block range):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.1 <span class="nt">-c</span> 4
PING 10.200.0.1 <span class="o">(</span>10.200.0.1<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.

<span class="nt">---</span> 10.200.0.1 ping statistics <span class="nt">---</span>
4 packets transmitted, 0 received, 100% packet loss, <span class="nb">time </span>3083ms

<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 0 received, 100% packet loss, <span class="nb">time </span>3089ms
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>
<p>In this post we’ve shown how <code class="language-plaintext highlighter-rouge">MultiNetworkPolicies</code> can be used to provide
access control to VMs with secondary network interfaces.</p>

<p>We have provided a comprehensive example on how a policy can be used to limit
ingress to our VMs only from desired sources, based on the client’s IP address.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><category term="NetworkPolicy" /><summary type="html"><![CDATA[This post explains how to configure NetworkPolicies for KubeVirt VMs secondary networks.]]></summary></entry></feed>