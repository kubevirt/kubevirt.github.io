<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2022-08-17T23:03:11+00:00</updated><id>https://kubevirt.io//feed.xml</id><title type="html">KubeVirt.io</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">Simplifying KubeVirt’s `VirtualMachine` UX with Instancetypes and Preferences</title><link href="https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes.html" rel="alternate" type="text/html" title="Simplifying KubeVirt’s `VirtualMachine` UX with Instancetypes and Preferences" /><published>2022-08-12T00:00:00+00:00</published><updated>2022-08-12T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes.html"><![CDATA[<p>KubeVirt’s <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> API contains many advanced options for tuning a virtual machine’s resources and performance that go beyond what typical users need to be aware of. Users have until now been unable to simply define the storage/network they want assigned to their VM and then declare in broad terms what quality of resources and kind of performance they need for their VM. Instead, the user has to be keenly aware how to request specific compute resources alongside all of the performance tunings available on the <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> API and how those tunings impact their guest’s operating system in order to get a desired result.</p>

<p>A common pattern for IaaS is to have abstractions separating the resource sizing and performance of a workload from the user-defined values related to launching their custom application. This pattern is evident across all the major cloud providers (also known as hyperscalers) as well as open source IaaS projects like OpenStack. AWS has <a href="https://aws.amazon.com/ec2/instance-types/">instance types</a>, GCP has <a href="https://cloud.google.com/compute/docs/machine-types#custom_machine_types">machine types</a>, Azure has <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes">instance VM sizes</a>, and OpenStack has <a href="https://docs.openstack.org/nova/latest/user/flavors.html">flavors</a>.</p>

<p>Let’s take AWS for example to help visualize what this abstraction enables. Launching an EC2 instance only requires a few top level arguments; the disk image, instance type, keypair, security group, and subnet:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws ec2 run-instances <span class="nt">--image-id</span> ami-xxxxxxxx <span class="se">\</span>
                        <span class="nt">--count</span> 1 <span class="se">\</span>
                        <span class="nt">--instance-type</span> c4.xlarge <span class="se">\</span>
                        <span class="nt">--key-name</span> MyKeyPair <span class="se">\</span>
                        <span class="nt">--security-group-ids</span> sg-903004f8 <span class="se">\</span>
                        <span class="nt">--subnet-id</span> subnet-6e7f829e
</code></pre></div></div>

<p>When creating the EC2 instance the user doesn’t define the amount of resources, what processor to use, how to optimize the performance of the instance, or what hardware to schedule the instance on. Instead, all of that information is wrapped up in that single <code class="language-plaintext highlighter-rouge">--instance-type c4.xlarge</code> CLI argument. <code class="language-plaintext highlighter-rouge">c4</code> denotes a specific performance profile version, in this case from the <code class="language-plaintext highlighter-rouge">Compute Optimized</code> family and <code class="language-plaintext highlighter-rouge">xlarge</code> denotes a specific amount of compute resources provided by the instance type, in this case 4 vCPUs, 7.5 GiB of RAM, 750 Mbps EBS bandwidth, etc.</p>

<p>While hyperscalers can provide predefined types with performance profiles and compute resources already assigned IaaS and virtualization projects such as OpenStack and KubeVirt can only provide the raw abstractions for operators, admins, and even vendors to then create instances of these abstractions specific to each deployment.</p>

<h2 id="instancetype-api">Instancetype API</h2>

<p>The recently renamed instancetype API and associated <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> aim to address this by providing KubeVirt users with a set of APIs and abstractions that allow them to make fewer choices when creating a <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> while still ending up with a working, performant guest at runtime.</p>

<h2 id="virtualmachineinstancetype">VirtualMachineInstancetype</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-instancetype</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">memory</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="s">128Mi</span>
</code></pre></div></div>

<p>KubeVirt now provides two instancetype based <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a>, a cluster wide <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineclusterinstancetype"><code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code></a> and a namespaced <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetype"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetype</code></a>. These <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> encapsulate the following resource related characteristics of a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> through a shared <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetypespec"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetypeSpec</code></a>:</p>

<ul>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_cpuinstancetype">CPU</a> : Required number of vCPUs presented to the guest</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_memoryinstancetype">Memory</a> : Required amount of memory presented to the guest</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_gpu">GPUs</a> : Optional list of vGPUs to passthrough</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_hostdevice">HostDevices</a>: Optional list of HostDevices to passthrough</li>
  <li><a href="`string`">IOThreadsPolicy</a> : Optional IOThreadsPolicy to be used</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_launchsecurity">LaunchSecurity</a>: Optional LaunchSecurity to be used</li>
</ul>

<p>Anything provided within an instancetype cannot be overridden within a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a>. For example, <code class="language-plaintext highlighter-rouge">CPU</code> and <code class="language-plaintext highlighter-rouge">Memory</code> are both required attributes of an instancetype. If a user makes any requests for <code class="language-plaintext highlighter-rouge">CPU</code> or <code class="language-plaintext highlighter-rouge">Memory</code> resources within their <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a>, the instancetype will conflict and the request will be rejected.</p>

<h2 id="virtualmachinepreference">VirtualMachinePreference</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-preference</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">devices</span><span class="pi">:</span>
    <span class="na">preferredDiskBus</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredInterfaceModel</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>

<p>KubeVirt also provides two further preference based <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a>, again a cluster-wide <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineclusterpreference"><code class="language-plaintext highlighter-rouge">VirtualMachineClusterPreference</code></a> and namespaced <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreference"><code class="language-plaintext highlighter-rouge">VirtualMachinePreference</code></a>. These <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> encapsulate the preferred value of any remaining attributes of a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> required to run a given workload, again this is through a shared <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreferencespec"><code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code></a>.</p>

<p>Unlike instancetypes, preferences only represent the preferred values and as such can be overridden by values in the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> provided by the user.</p>

<h2 id="virtualmachineinstancetypepreferencematcher">VirtualMachine{Instancetype,Preference}Matcher</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-vm</span>
<span class="na">spec</span><span class="pi">:</span>
<span class="pi">[</span><span class="nv">..</span><span class="pi">]</span>
  <span class="na">instancetype</span><span class="pi">:</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-instancetype</span>
  <span class="na">preference</span><span class="pi">:</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-preference</span>
<span class="pi">[</span><span class="nv">..</span><span class="pi">]</span>
</code></pre></div></div>

<p>The previous instancetype and preference CRDs are matched to a given <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> through the use of a matcher. Each matcher consists of the following:</p>

<ul>
  <li>Name (string): Name of the resource being referenced</li>
  <li>Kind (string):  Optional, defaults to the cluster wide CRD kinds of <code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code> or <code class="language-plaintext highlighter-rouge">VirtualMachineClusterPreference</code></li>
  <li>RevisionName (string) : Optional, name of a <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/controller-revision-v1/">ControllerRevision</a> containing a copy of the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetypespec"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetypeSpec</code></a> or <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreferencespec"><code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code></a> taken when the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> is first started.</li>
</ul>

<h2 id="virtualmachineinstancepreset-deprecation">VirtualMachineInstancePreset Deprecation</h2>

<p>The new instancetype API and CRDs conflict somewhat with the existing <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachineinstancepreset"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancePreset</code></a> CRD. The approach taken by the CRD has also been removed in core k8s so, as advertised on the <a href="https://groups.google.com/g/kubevirt-dev/c/eM7JaDV_EU8">mailing list</a>, I have started the <a href="https://github.com/kubevirt/kubevirt/pull/8069">process of deprecating</a> <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachineinstancepreset"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancePreset</code></a> in favor of the Instancetype CRDs listed above.</p>

<h2 id="examples">Examples</h2>

<p>The following example is taken from the <a href="https://kubevirt.io/user-guide/virtual_machines/instancetypes/">KubeVirt User Guide</a>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">$ cat &lt;&lt; EOF | kubectl apply -f -</span> 
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cmedium</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">memory</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="s">1Gi</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">devices</span><span class="pi">:</span>
    <span class="na">preferredDiskBus</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredInterfaceModel</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredRng</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">features</span><span class="pi">:</span>
    <span class="na">preferredAcpi</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">preferredSmm</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">firmware</span><span class="pi">:</span>
    <span class="na">preferredUseEfi</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">preferredUseSecureBoot</span><span class="pi">:</span> <span class="no">true</span>    
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">instancetype</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">cmedium</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">virtualMachineInstancetype</span>
  <span class="na">preference</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">virtualMachinePreference</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/containerdisks/fedora:latest</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">users:</span>
              <span class="s">- name: admin</span>
                <span class="s">sudo: ALL=(ALL) NOPASSWD:ALL</span>
                <span class="s">ssh_authorized_keys:</span>
                  <span class="s">- ssh-rsa AAAA...</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>We can compare the original <code class="language-plaintext highlighter-rouge">VirtualMachine</code> spec with that of the running <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> to confirm our instancetype and preferences have been applied using the following <code class="language-plaintext highlighter-rouge">diff</code> command:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>diff <span class="nt">--color</span> <span class="nt">-u</span> &lt;<span class="o">(</span> kubectl get vms/fedora <span class="nt">-o</span> json | jq .spec.template.spec<span class="o">)</span> &lt;<span class="o">(</span> kubectl get vmis/fedora <span class="nt">-o</span> json | jq .spec<span class="o">)</span>
<span class="o">[</span>..]
 <span class="o">{</span>
   <span class="s2">"domain"</span>: <span class="o">{</span>
-    <span class="s2">"devices"</span>: <span class="o">{}</span>,
+    <span class="s2">"cpu"</span>: <span class="o">{</span>
+      <span class="s2">"cores"</span>: 1,
+      <span class="s2">"model"</span>: <span class="s2">"host-model"</span>,
+      <span class="s2">"sockets"</span>: 1,
+      <span class="s2">"threads"</span>: 1
+    <span class="o">}</span>,
+    <span class="s2">"devices"</span>: <span class="o">{</span>
+      <span class="s2">"disks"</span>: <span class="o">[</span>
+        <span class="o">{</span>
+          <span class="s2">"disk"</span>: <span class="o">{</span>
+            <span class="s2">"bus"</span>: <span class="s2">"virtio"</span>
+          <span class="o">}</span>,
+          <span class="s2">"name"</span>: <span class="s2">"containerdisk"</span>
+        <span class="o">}</span>,
+        <span class="o">{</span>
+          <span class="s2">"disk"</span>: <span class="o">{</span>
+            <span class="s2">"bus"</span>: <span class="s2">"virtio"</span>
+          <span class="o">}</span>,
+          <span class="s2">"name"</span>: <span class="s2">"cloudinit"</span>
+        <span class="o">}</span>
+      <span class="o">]</span>,
+      <span class="s2">"interfaces"</span>: <span class="o">[</span>
+        <span class="o">{</span>
+          <span class="s2">"bridge"</span>: <span class="o">{}</span>,
+          <span class="s2">"model"</span>: <span class="s2">"virtio"</span>,
+          <span class="s2">"name"</span>: <span class="s2">"default"</span>
+        <span class="o">}</span>
+      <span class="o">]</span>,
+      <span class="s2">"rng"</span>: <span class="o">{}</span>
+    <span class="o">}</span>,
+    <span class="s2">"features"</span>: <span class="o">{</span>
+      <span class="s2">"acpi"</span>: <span class="o">{</span>
+        <span class="s2">"enabled"</span>: <span class="nb">true</span>
+      <span class="o">}</span>,
+      <span class="s2">"smm"</span>: <span class="o">{</span>
+        <span class="s2">"enabled"</span>: <span class="nb">true</span>
+      <span class="o">}</span>
+    <span class="o">}</span>,
+    <span class="s2">"firmware"</span>: <span class="o">{</span>
+      <span class="s2">"bootloader"</span>: <span class="o">{</span>
+        <span class="s2">"efi"</span>: <span class="o">{</span>
+          <span class="s2">"secureBoot"</span>: <span class="nb">true</span>
+        <span class="o">}</span>
+      <span class="o">}</span>,
+      <span class="s2">"uuid"</span>: <span class="s2">"98f07cdd-96da-5880-b6c7-1a5700b73dc4"</span>
+    <span class="o">}</span>,
     <span class="s2">"machine"</span>: <span class="o">{</span>
       <span class="s2">"type"</span>: <span class="s2">"q35"</span>
     <span class="o">}</span>,
-    <span class="s2">"resources"</span>: <span class="o">{}</span>
+    <span class="s2">"memory"</span>: <span class="o">{</span>
+      <span class="s2">"guest"</span>: <span class="s2">"1Gi"</span>
+    <span class="o">}</span>,
+    <span class="s2">"resources"</span>: <span class="o">{</span>
+      <span class="s2">"requests"</span>: <span class="o">{</span>
+        <span class="s2">"memory"</span>: <span class="s2">"1Gi"</span>
+      <span class="o">}</span>
+    <span class="o">}</span>
   <span class="o">}</span>,
+  <span class="s2">"networks"</span>: <span class="o">[</span>
+    <span class="o">{</span>
+      <span class="s2">"name"</span>: <span class="s2">"default"</span>,
+      <span class="s2">"pod"</span>: <span class="o">{}</span>
+    <span class="o">}</span>
+  <span class="o">]</span>,
   <span class="s2">"volumes"</span>: <span class="o">[</span>
     <span class="o">{</span>
       <span class="s2">"containerDisk"</span>: <span class="o">{</span>
-        <span class="s2">"image"</span>: <span class="s2">"quay.io/containerdisks/fedora:latest"</span>
+        <span class="s2">"image"</span>: <span class="s2">"quay.io/containerdisks/fedora:latest"</span>,
+        <span class="s2">"imagePullPolicy"</span>: <span class="s2">"Always"</span>
       <span class="o">}</span>,
       <span class="s2">"name"</span>: <span class="s2">"containerdisk"</span>
     <span class="o">}</span>,
</code></pre></div></div>

<h2 id="future-work">Future work</h2>

<p>There’s still plenty of work required before the API and CRDs can move from their current <code class="language-plaintext highlighter-rouge">alpha</code> version to <code class="language-plaintext highlighter-rouge">beta</code>. We have a specific <a href="https://github.com/kubevirt/kubevirt/issues/8235"><code class="language-plaintext highlighter-rouge">kubevirt/kubevirt</code> issue tracking our progress to <code class="language-plaintext highlighter-rouge">beta</code></a>. As set out there and in the <a href="https://github.com/kubevirt/community/blob/main/docs/api-graduation-guidelines.md">KubeVirt community API Graduation Phase Expecations</a>, part of this work is to seek feedback from the wider community so please do feel free to chime in there with any and all feedback on the API and CRDs.</p>

<p>You can also track our work on this API through the <a href="https://github.com/kubevirt/kubevirt/labels/area%2Finstancetype"><code class="language-plaintext highlighter-rouge">area/instancetype</code> tag</a> or my <a href="https://blog.yarwood.me.uk/tags/instancetypes/">personal blog</a> where I will be posting <a href="https://blog.yarwood.me.uk/2022/07/21/kubevirt_instancetype_update_2/">regular updates</a> and <a href="https://blog.yarwood.me.uk/2022/08/03/kubevirt_instancetype_demo_2/">demos</a> for instancetypes.</p>]]></content><author><name>Lee Yarwood</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="instancetypes" /><category term="preferences" /><category term="VirtualMachine" /><category term="VirtualMachineInstancetype" /><category term="VirtualMachinePreference" /><summary type="html"><![CDATA[An introduction to Instancetypes and preferences in KubeVirt]]></summary></entry><entry><title type="html">KubeVirt: installing Microsoft Windows 11 from an ISO</title><link href="https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html" rel="alternate" type="text/html" title="KubeVirt: installing Microsoft Windows 11 from an ISO" /><published>2022-08-02T00:00:00+00:00</published><updated>2022-08-02T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html"><![CDATA[<p>This blog post describes a simple way to deploy a Windows 11 VM with KubeVirt, using an installation ISO as a starting point.<br />
Although only tested with Windows 11, the steps described here should also work to deploy other recent versions of Windows.</p>

<h2 id="pre-requisites">Pre-requisites</h2>

<ul>
  <li>You’ll need a Kubernetes cluster with worker node(s) that have at least 6GB of available memory</li>
  <li><a href="https://kubevirt.io/user-guide">KubeVirt</a> and <a href="https://github.com/kubevirt/containerized-data-importer/blob/main/README.md">CDI</a> both deployed on the cluster</li>
  <li>A storage backend, such as <a href="https://ceph.com/">Rook Ceph</a></li>
  <li>A Windows iso. One can be found at <a href="https://www.microsoft.com/software-download/windows11">https://www.microsoft.com/software-download/windows11</a></li>
</ul>

<p>A suitable test cluster can easily be deployed thanks to KubeVirtCI by running the following commands from the <a href="https://github.com/kubevirt/kubevirt">KubeVirt source repository</a>:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBEVIRT_MEMORY_SIZE</span><span class="o">=</span>8192M
<span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBEVIRT_STORAGE</span><span class="o">=</span>rook-ceph-default
<span class="nv">$ </span>make cluster-up <span class="o">&amp;&amp;</span> make cluster-sync
</code></pre></div></div>

<h2 id="preparation">Preparation</h2>

<p>Before the virtual machine can be created, we need to setup storage volumes for the ISO and the drive, and write the appropriate VM(I) yaml.</p>

<ol>
  <li>
    <p>Uploading the ISO to a PVC</p>

    <p>KubeVirt provides a simple tool that is able to do that for us: <code class="language-plaintext highlighter-rouge">virtctl</code>.<br />
Here’s the command to upload the ISO, just replace <code class="language-plaintext highlighter-rouge">/storage/win11.iso</code> with the path to your Windows 11 ISO:
<code class="language-plaintext highlighter-rouge">virtctl image-upload pvc win11cd-pvc --size 6Gi --image-path=/storage/win11.iso --insecure</code></p>
  </li>
  <li>
    <p>Creating a persistent volume to use as the Windows drive</p>

    <p>This will depend on the storage configuration of your cluster.
The following yaml, to apply to the cluster using <code class="language-plaintext highlighter-rouge">kubectl create</code>, should work just fine on a KubeVirtCI cluster:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">task-pv-volume</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">local</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">hostpath</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">15Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/tmp/hostImages/win11"</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>Microsoft actually <a href="https://docs.microsoft.com/en-us/windows/whats-new/windows-11-requirements">recommends</a> at least 64GB of storage.
But, unlike some other requirements, the installer will accept smaller disks.
This is convenient when testing with KubeVirtCI, as nodes only have about 20GB of free space.
However, please bear in mind that such a small drive should only be used for testing purposes, and might lead to instabilities.</p>


</div></div>
<ol>
  <li>
    <p>Creating a persistent volume claim (PVC) for the drive</p>

    <p>Once again, your milage may vary, but the following PVC yaml works fine on KubeVirtCI:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">disk-windows</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">15Gi</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">hostpath</span>
</code></pre></div>    </div>

    <p>The name of PVC, <code class="language-plaintext highlighter-rouge">disk-windows</code> here, will be used in the yaml of the VM(I) as the main volume.</p>
  </li>
  <li>
    <p>Creating the VM(I) yaml file</p>

    <p>KubeVirt already includes an example <a href="https://github.com/kubevirt/kubevirt/blob/main/examples/vmi-windows.yaml">Windows VMI yaml file</a>, which we’ll use as a starting point here for convenience.<br />
Using a VMI yaml is more than enough for testing purposes, however for more serious applications you might want to consider changing it into a VM.</p>

    <p>First, in the yaml above, bump the memory up to 4Gi, which is a hard requirement of Windows 11. (Windows 10 is happy with 2Gi).</p>

    <p>Then, let’s add the ISO created above.
Add is as a cdrom in the disks section:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
    <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
</code></pre></div>    </div>
    <p>And the corresponding volume at the bottom:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">win11cd-pvc</span>
</code></pre></div>    </div>
    <p>Note that the names should match, and that the <code class="language-plaintext highlighter-rouge">claimName</code> is what we used in the <code class="language-plaintext highlighter-rouge">virtctl</code> command above.</p>

    <p>Here is what the VMI looks like after those changes:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstance</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">special</span><span class="pi">:</span> <span class="s">vmi-windows</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vmi-windows</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">domain</span><span class="pi">:</span>
    <span class="na">clock</span><span class="pi">:</span>
      <span class="na">timer</span><span class="pi">:</span>
        <span class="na">hpet</span><span class="pi">:</span>
          <span class="na">present</span><span class="pi">:</span> <span class="no">false</span>
        <span class="na">hyperv</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">pit</span><span class="pi">:</span>
          <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">delay</span>
        <span class="na">rtc</span><span class="pi">:</span>
          <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">catchup</span>
      <span class="na">utc</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">cpu</span><span class="pi">:</span>
      <span class="na">cores</span><span class="pi">:</span> <span class="m">2</span>
    <span class="na">devices</span><span class="pi">:</span>
      <span class="na">disks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
          <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
      <span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
          <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
      <span class="na">interfaces</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">model</span><span class="pi">:</span> <span class="s">e1000</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
      <span class="na">tpm</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">features</span><span class="pi">:</span>
      <span class="na">acpi</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">apic</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">hyperv</span><span class="pi">:</span>
        <span class="na">relaxed</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">spinlocks</span><span class="pi">:</span>
          <span class="na">spinlocks</span><span class="pi">:</span> <span class="m">8191</span>
        <span class="na">vapic</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">smm</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">firmware</span><span class="pi">:</span>
      <span class="na">bootloader</span><span class="pi">:</span>
        <span class="na">efi</span><span class="pi">:</span>
          <span class="na">secureBoot</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">uuid</span><span class="pi">:</span> <span class="s">5d307ca9-b3ef-428c-8861-06e72d69f223</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s">4Gi</span>
  <span class="na">networks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
    <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">disk-windows</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">win11cd-pvc</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>When customizing this VMI definition or creating your own, please keep in mind that the TPM device and the UEFI firmware with SecureBoot are both hard requirements of Windows 11.
Not having them will cause the Windows 11 installation to fail early. Please also note that the SMM CPU feature is required for UEFI + SecureBoot.
However, they can all be omitted in the case of a Windows 10 VM(I).
Finally, we do not currently support TPM persistence, so any secret stored in the emulated TPM will be lost next time you boot the VMI.
For example, do not enable BitLocker, as it will fail to find the encryption key next boot and you will have to manually enter the (55 characters!) recovery key each boot.</p>


</div></div>
<h2 id="windows-installation">Windows installation</h2>

<p>You should now be able to create the VMI and start the Windows installation process.<br />
Just use kubectl to start the VMI created above: <code class="language-plaintext highlighter-rouge">kubectl create -f vmi-windows.yaml</code>.<br />
Shortly after, open a VNC session to it using <code class="language-plaintext highlighter-rouge">virtctl vnc vmi-windows</code> (keep trying until the VMI is running and the VNC session pops up).<br />
You should now see the boot screen, and shortly after a prompt to “Press any key to boot from CD or DVD…”. You have a few seconds to do so or the VM will fail to boot.
Then just follow the steps to install Windows.</p>

<h2 id="virtio-drivers-installation-optional">VirtIO drivers installation (optional)</h2>

<p>Once Windows is installed, it’s a good ideas to install the <a href="http://www.linux-kvm.org/page/Virtio">VirtIO</a> drivers inside the VM, as they can drastically improve performance.
The latest version can be downloaded <a href="https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/latest-virtio/">here</a>.
<code class="language-plaintext highlighter-rouge">virtio-win-gt-x64.msi</code> is the simplest package to install, as you just have to run it as Administrator.</p>

<p>Alternatively, KubeVirt has a containerdisk image that can be mounted inside the VM.<br />
To use it, just add a simple cdrom disk to the VMI, like:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
    <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>
<p>and the volume:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirt/virtio-container-disk</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>
<p>When using KubeVirtCI, a local copy of the image is also available at <code class="language-plaintext highlighter-rouge">registry:5000/kubevirt/virtio-container-disk:devel</code>.</p>

<h2 id="further-performance-improvements">Further performance improvements</h2>

<p>Windows is quite resource-hungry, and you might find that the VM created above is too slow, even with the VirtIO drivers installed.<br />
Here are a few steps you can take to improve things:</p>
<ul>
  <li>Increasing the RAM is always a good idea, if you have enough available of course.</li>
  <li>Increasing the number of CPUs, and/or using CPUManager to assign dedicated CPU to the VM should also help a lot.</li>
  <li>Once the VirtIO drivers are installed, the main drive can also be switched from <code class="language-plaintext highlighter-rouge">sata</code> to <code class="language-plaintext highlighter-rouge">virtio</code>, and the attached CDROMs can be removed.</li>
</ul>]]></content><author><name>Jed Lejosne</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="Microsoft Windows kubernetes" /><category term="Microsoft Windows container" /><category term="Windows" /><summary type="html"><![CDATA[This blog post describes how to create a Microsoft Windows 11 virtual machine with KubeVirt]]></summary></entry><entry><title type="html">KubeVirt v0.55.0</title><link href="https://kubevirt.io//2022/changelog-v0.55.0.html" rel="alternate" type="text/html" title="KubeVirt v0.55.0" /><published>2022-07-14T00:00:00+00:00</published><updated>2022-07-14T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.55.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.55.0.html"><![CDATA[<h2 id="v0550">v0.55.0</h2>

<p>Released on: Thu Jul 14 16:33:25 2022 +0000</p>

<ul>
  <li>[PR #7336][iholder-redhat] Introduce clone CRD, controller and API</li>
  <li>[PR #7791][iholder-redhat] Introduction of an initial deprecation policy</li>
  <li>[PR #7875][lyarwood] <code class="language-plaintext highlighter-rouge">ControllerRevisions</code> of any <code class="language-plaintext highlighter-rouge">VirtualMachineFlavorSpec</code> or <code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code> are stored during the initial start of a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> and used for subsequent restarts ensuring changes to the original <code class="language-plaintext highlighter-rouge">VirtualMachineFlavor</code> or <code class="language-plaintext highlighter-rouge">VirtualMachinePreference</code> do not modify the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> and the <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> it creates.</li>
  <li>[PR #8011][fossedihelm] Increase virt-launcher memory overhead</li>
  <li>[PR #7963][qinqon] Bump alpine_with_test_tooling</li>
  <li>[PR #7881][ShellyKa13] Enable memory dump to be included in VMSnapshot</li>
  <li>[PR #7926][qinqon] tests: Move main clean function to global AfterEach and create a VM per each infra_test.go Entry.</li>
  <li>[PR #7845][janeczku] Fixed a bug that caused <code class="language-plaintext highlighter-rouge">make generate</code> to fail when API code comments contain backticks. (#7844, @janeczku)</li>
  <li>[PR #7932][marceloamaral] Addition of kubevirt_vmi_migration_phase_transition_time_from_creation_seconds metric to monitor how long it takes to transition a VMI Migration object to a specific phase from creation time.</li>
  <li>[PR #7879][marceloamaral] Faster VM phase transitions thanks to an increased virt-controller QPS/Burst</li>
  <li>[PR #7807][acardace] make cloud-init ‘instance-id’ persistent across reboots</li>
  <li>[PR #7928][iholder-redhat] bugfix: node-labeller now removes “host-model-cpu.node.kubevirt.io/” and “host-model-required-features.node.kubevirt.io/” prefixes</li>
  <li>[PR #7841][jean-edouard] Non-root VMs will now migrate to root VMs after a cluster disables non-root.</li>
  <li>[PR #7933][akalenyu] BugFix: Fix vm restore in case of restore size bigger then PVC requested size</li>
  <li>[PR #7919][lyarwood] Device preferences are now applied to any default network interfaces or missing volume disks added to a <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> at runtime.</li>
  <li>[PR #7910][qinqon] tests: Create the expected readiness probe instead of liveness</li>
  <li>[PR #7732][acardace] Prevent virt-handler from starting a migration twice</li>
  <li>[PR #7594][alicefr] Enable to run libguestfs-tools pod to run as noroot user</li>
  <li>[PR #7811][raspbeep] User now gets information about the type of commands which the guest agent does not support.</li>
  <li>[PR #7590][awels] VMExport allows filesystem PVCs to be exported as either disks or directories.</li>
  <li>[PR #7683][alicefr] Add –command and –local-ssh-opts” options to virtctl ssh to execute remote command using local ssh method</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.55.0 changes]]></summary></entry><entry><title type="html">KubeVirt at KubeCon EU 2022</title><link href="https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022.html" rel="alternate" type="text/html" title="KubeVirt at KubeCon EU 2022" /><published>2022-06-28T00:00:00+00:00</published><updated>2022-06-28T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022.html"><![CDATA[<p>KubeCon EU was in Valencia, Spain this year from May 16-20. For many of the 7000+ physical attendees, it was their first in-person conference in several years. With luck, it was the first of many more to come, as KubeCon is a rare opportunity to learn about, from, and with a rich variety of adopters, communities, and vendors that make up the open source and cloud native ecosystem.</p>

<p>The KubeVirt community presented two sessions, both on Wednesday May 18th:</p>
<ol>
  <li>A Virtual Open Office Hours session, and</li>
  <li>A Maintainer Track session: ‘It’s All for the Users. More Durable, Secure, and Pluggable. KubeVirt v0.53’</li>
</ol>

<h2 id="virtual-open-office-hours">Virtual Open Office Hours</h2>

<p>This was a 45-minute project virtual session, hosted by the CNCF. This was on the Bevy platform (which will be familiar to KubeVirt Summit attendees from the past two years) and we had five lovely people from the KubeVirt community ready with a variety of demos and presentations and to answer questions from attendees:+
Alice Frosi, Itamar Holder, Miguel Duarte de Mora Barroso, Luboslav Pivarc, and Bartosz Rybacki</p>

<p>This was an opportunity for KubeCon attendees (virtual and physical) to ask questions and discuss any topics, and our presenters covered the following: Introduction to KubeVirt, live migration, Istio integration, and CDI hotplug/resize.
Despite some initial technical issues and improvised changes, this session went really well. We had about ~25 consistent attendees, and we received a good range of Q&amp;A and interaction with the attendees on all topics presented. It was a very solid 45 minutes.
Unfortunately, due to a miscommunication, there is no recording of this session.</p>

<p>A huge thanks to the presenters for their time and collaboration in preparing for this.</p>

<h2 id="maintainer-track">Maintainer Track</h2>

<p>Later that day, on the Maintainer Track, Alice also gave an in-depth breakdown of a whole slew of new KubeVirt features and showed a demo with the KubeVirt Cluster API: deploying Kubernetes on top of Kubernetes.
You can watch <a href="https://youtu.be/L9H0pz5PpKo">the CNCF recording here</a>, and download the <a href="https://kccnceu2022.sched.com/event/ytu1">demo video and slides</a> that are available from the schedule.</p>

<p>There was a healthy amount of questions, both during Q&amp;A and after the talk. The participants were particularly interested to know how to prepare and customize VM disks with KubeVirt, how to run Windows VM, especially combined with GPUs, and how to expose the Kubernetes API service of a deployed cluster to the KubeVirt cluster API provider outside of the KubeVirt VM. There were additional questions on the status of TPM support and VM migration when the hosting node goes down.</p>

<h2 id="thank-you">Thank you!</h2>
<p>Big thanks again to our presenters: Alice Frosi, Itamar Holder, Miguel Duarte de Mora Barroso, Luboslav Pivarc, and Bartosz Rybacki.
And everyone who attended the sessions, listened, and asked great questions.</p>

<h2 id="want-to-see-more-from-kubecon-eu-2022">Want to see more from KubeCon EU 2022?</h2>

<p>If you’re interested in seeing more photos and recordings from the event:</p>
<ul>
  <li><a href="https://www.flickr.com/photos/143247548@N03/albums/72177720298987342">CNCF’s Photo album (Flickr) of the event</a>.</li>
  <li><a href="https://www.youtube.com/c/cloudnativefdn">The CNCF video recordings of the sessions on Youtube</a>.</li>
  <li>And <a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/schedule/">the event schedule</a> to help you find sessions.</li>
</ul>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><category term="KubeCon" /><summary type="html"><![CDATA[A short report on the two sessions KubeVirt presented at KubeCon EU 2022]]></summary></entry><entry><title type="html">KubeVirt v0.54.0</title><link href="https://kubevirt.io//2022/changelog-v0.54.0.html" rel="alternate" type="text/html" title="KubeVirt v0.54.0" /><published>2022-06-08T00:00:00+00:00</published><updated>2022-06-08T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.54.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.54.0.html"><![CDATA[<h2 id="v0540">v0.54.0</h2>

<p>Released on: Wed Jun 8 14:15:43 2022 +0000</p>

<ul>
  <li>[PR #7757][orenc1] new alert for excessive number of VMI migrations in a period of time.</li>
  <li>[PR #7517][ShellyKa13] Add virtctl Memory Dump command</li>
  <li>[PR #7801][VirrageS] Empty (<code class="language-plaintext highlighter-rouge">nil</code> values) of <code class="language-plaintext highlighter-rouge">Address</code> and <code class="language-plaintext highlighter-rouge">Driver</code> fields in XML will be omitted.</li>
  <li>[PR #7475][raspbeep] Adds the reason of a live-migration failure to a recorded event in case EvictionStrategy is set but live-migration is blocked due to its limitations.</li>
  <li>[PR #7739][fossedihelm] Allow <code class="language-plaintext highlighter-rouge">virtualmachines/migrate</code> subresource to admin/edit users</li>
  <li>[PR #7618][lyarwood] The requirement to define a <code class="language-plaintext highlighter-rouge">Disk</code> or <code class="language-plaintext highlighter-rouge">Filesystem</code> for each <code class="language-plaintext highlighter-rouge">Volume</code> associated with a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> has been removed. Any <code class="language-plaintext highlighter-rouge">Volumes</code> without a <code class="language-plaintext highlighter-rouge">Disk</code> or <code class="language-plaintext highlighter-rouge">Filesystem</code> defined will have a <code class="language-plaintext highlighter-rouge">Disk</code> defined within the <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> at runtime.</li>
  <li>[PR #7529][xpivarc] NoReadyVirtController and NoReadyVirtOperator should be properly fired.</li>
  <li>[PR #7465][machadovilaca] Add metrics for migrations and respective phases</li>
  <li>[PR #7592][akalenyu] BugFix: virtctl guestfs incorrectly assumes image name</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.54.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.53.0</title><link href="https://kubevirt.io//2022/changelog-v0.53.0.html" rel="alternate" type="text/html" title="KubeVirt v0.53.0" /><published>2022-05-09T00:00:00+00:00</published><updated>2022-05-09T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.53.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.53.0.html"><![CDATA[<h2 id="v0530">v0.53.0</h2>

<p>Released on: Mon May 9 14:02:20 2022 +0000</p>

<ul>
  <li>[PR #7533][akalenyu] Add several VM snapshot metrics</li>
  <li>[PR #7574][rmohr] Pull in cdi dependencies with minimized transitive dependencies to ease API adoption</li>
  <li>[PR #7318][iholder-redhat] Snapshot restores now support restoring to a target VM different than the source</li>
  <li>[PR #7474][borod108] Added the following metrics for live migration: kubevirt_migrate_vmi_data_processed_bytes, kubevirt_migrate_vmi_data_remaining_bytes, kubevirt_migrate_vmi_dirty_memory_rate_bytes</li>
  <li>[PR #7441][rmohr] Add <code class="language-plaintext highlighter-rouge">virtctl scp</code> to ease copying files from and to VMs and VMIs</li>
  <li>[PR #7265][rthallisey] Support steady-state job types in the load-generator tool</li>
  <li>[PR #7544][fossedihelm] Upgraded go version to 1.17.8</li>
  <li>[PR #7582][acardace] Fix failed reported migrations when actually they were successful.</li>
  <li>[PR #7546][0xFelix] Update virtio-container-disk to virtio-win version 0.1.217-1</li>
  <li>[PR #7530][iholder-redhat] [External Kernel Boot]: Disallow kernel args without providing custom kernel</li>
  <li>[PR #7493][davidvossel] Adds new EvictionStrategy “External” for blocking eviction which is handled by an external controller</li>
  <li>[PR #7563][akalenyu] Switch VolumeSnapshot to v1</li>
  <li>[PR #7406][acardace] Reject <code class="language-plaintext highlighter-rouge">LiveMigrate</code> as a workload-update strategy if the <code class="language-plaintext highlighter-rouge">LiveMigration</code> feature gate is not enabled.</li>
  <li>[PR #7103][jean-edouard] Non-persistent vTPM now supported. Keep in mind that the state of the TPM is wiped after each shutdown. Do not enable Bitlocker!</li>
  <li>[PR #7277][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 8.0.0 and QEMU 6.2.0.</li>
  <li>[PR #7130][Barakmor1] Add field to kubevirtCR to set Prometheus ServiceMonitor object’s namespace</li>
  <li>[PR #7401][iholder-redhat] virt-api deployment is now scalable - replicas are determined by the number of nodes in the cluster</li>
  <li>[PR #7500][awels] BugFix: Fixed RBAC for admin/edit user to allow virtualmachine/addvolume and removevolume. This allows for persistent disks</li>
  <li>[PR #7328][apoorvajagtap] Don’t ignore –identity-file when setting –local-ssh=true on <code class="language-plaintext highlighter-rouge">virtctl ssh</code></li>
  <li>[PR #7469][xpivarc] Users can now enable the NonRoot feature gate instead of NonRootExperimental</li>
  <li>[PR #7451][fossedihelm] Reduce virt-launcher memory usage by splitting monitoring and launcher processes</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.53.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.52.0</title><link href="https://kubevirt.io//2022/changelog-v0.52.0.html" rel="alternate" type="text/html" title="KubeVirt v0.52.0" /><published>2022-04-08T00:00:00+00:00</published><updated>2022-04-08T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.52.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.52.0.html"><![CDATA[<h2 id="v0520">v0.52.0</h2>

<p>Released on: Fri Apr 8 16:17:56 2022 +0000</p>

<ul>
  <li>[PR #7024][fossedihelm] Add an warning message if the client and server virtctl versions are not aligned</li>
  <li>[PR #7486][rmohr] Move stable.txt location to a more appropriate path</li>
  <li>[PR #7372][saschagrunert] Fixed <code class="language-plaintext highlighter-rouge">KubeVirtComponentExceedsRequestedMemory</code> alert complaining about many-to-many matching not allowed.</li>
  <li>[PR #7426][iholder-redhat] Add warning for manually determining core-component replica count in Kubevirt CR</li>
  <li>[PR #7424][maiqueb] Provide interface binding types descriptions, which will be featured in the KubeVirt API.</li>
  <li>[PR #7422][orelmisan] Fixed setting custom guest pciAddress and bootOrder parameter(s) to a list of SR-IOV NICs.</li>
  <li>[PR #7421][rmohr] Fix knowhosts file corruption for virtctl ssh</li>
  <li>[PR #6854][rmohr] Make virtctl ssh work with ssh-rsa+ preauthentication</li>
  <li>[PR #7267][iholder-redhat] Applied migration configurations can now be found in VMI’s status</li>
  <li>[PR #7321][iholder-redhat] [Migration Policies]: precedence to VMI labels over Namespace labels</li>
  <li>[PR #7326][oshoval] The Ginkgo dependency has been upgraded to v2.1.3 (major version upgrade)</li>
  <li>[PR #7361][SeanKnight] Fixed a bug that prevents virtctl from working with clusters accessed via Rancher authentication proxy, or any other cluster where the server URL contains a path component. (#3760)</li>
  <li>[PR #7255][tyleraharrison] Users are now able to specify <code class="language-plaintext highlighter-rouge">--address [ip_address]</code> when using <code class="language-plaintext highlighter-rouge">virtctl vnc</code> rather than only using 127.0.0.1</li>
  <li>[PR #7275][enp0s3] Add observedGeneration to virt-operator to have a race-free way to detect KubeVirt config rollouts</li>
  <li>[PR #7233][xpivarc] Bug fix: Successfully aborted migrations should be reported now</li>
  <li>[PR #7158][AlonaKaplan] Add masquerade VMs support to single stack IPv6.</li>
  <li>[PR #7227][rmohr] Remove VMI informer from virt-api to improve scaling characteristics of virt-api</li>
  <li>[PR #7288][raspbeep] Users now don’t need to specify container for <code class="language-plaintext highlighter-rouge">kubectl logs &lt;vmi-pod&gt;</code> and <code class="language-plaintext highlighter-rouge">kubectl exec &lt;vmi-pod&gt;</code>.</li>
  <li>[PR #6709][xpivarc] Workloads will be migrated to nonroot implementation if NonRoot feature gate is set. (Except VirtioFS)</li>
  <li>[PR #7241][lyarwood] Fixed a bug that prevents only a unattend.xml configmap or secret being provided as contents for a sysprep disk. (#7240, @lyarwood)</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.52.0 changes]]></summary></entry><entry><title type="html">Load-balancer for virtual machines on bare metal Kubernetes clusters</title><link href="https://kubevirt.io//2022/Virtual-Machines-with-MetalLB.html" rel="alternate" type="text/html" title="Load-balancer for virtual machines on bare metal Kubernetes clusters" /><published>2022-04-03T00:00:00+00:00</published><updated>2022-04-03T00:00:00+00:00</updated><id>https://kubevirt.io//2022/Virtual-Machines-with-MetalLB</id><content type="html" xml:base="https://kubevirt.io//2022/Virtual-Machines-with-MetalLB.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Over the last year, Kubevirt and MetalLB have shown to be powerful duo in order to support fault-tolerant access to an application on virtual machines through an external IP address. 
As a Cluster administrator using an on-prem cluster without a network load-balancer, now it’s possible to use MetalLB operator to provide load-balancer capabilities (with Services of type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>) to virtual machines.</p>

<h2 id="metallb">MetalLB</h2>

<p><a href="https://metallb.universe.tf/">MetalLB</a> allows you to create Kubernetes services of type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>, and provides network load-balancer implementation in on-prem clusters that don’t run on a cloud provider.
MetalLB is responsible for assigning/unassigning an external IP Address to your service, using IPs from pre-configured pools. In order for the external IPs to be announced externally, MetalLB works in 2 modes, Layer 2 and BGP:</p>

<ul>
  <li>
    <p>Layer 2 mode (ARP/NDP):</p>

    <p>This mode - which actually does not implement real load-balancing behavior - provides a failover mechanism where a single node owns the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service, until it fails, triggering another node to be chosen as the service owner. This configuration mode makes the IPs reachable from the local network.<br />
In this method, the MetalLB speaker pod announces the IPs in ARP (for IPv4) and NDP (for IPv6) protocols over the host network. From a network perspective, the node owning the service appears to have multiple IP addresses assigned to a network interface. After traffic is routed to the node, the service proxy sends the traffic to the application pods.</p>
  </li>
  <li>
    <p>BGP mode:</p>

    <p>This mode provides real load-balancing behavior, by establishing BGP peering sessions with the network routers - which advertise the external IPs of the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service, distributing the load over the nodes.</p>
  </li>
</ul>

<p>To read more on MetalLB concepts, implementation and limitations, please read <a href="https://metallb.universe.tf/concepts/">its documentation</a>.</p>

<h2 id="demo-virtual-machine-with-external-ip-and-metallb-load-balancer">Demo: Virtual machine with external IP and MetalLB load-balancer</h2>

<p>With the following recipe we will end up with a nginx server running on a virtual machine, accessible outside the cluster using MetalLB load-balancer with Layer 2 mode.</p>

<h3 id="demo-environment-setup">Demo environment setup</h3>

<p>We are going to use <a href="https://kind.sigs.k8s.io">kind</a> provider as an ephemeral Kubernetes cluster.</p>

<p>Prerequirements:</p>
<ul>
  <li>First install kind on your machine following its <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">installation guide</a>.</li>
  <li>To use kind, you will also need to <a href="https://docs.docker.com/install/">install docker</a>.</li>
</ul>

<h4 id="external-ips-on-macos-and-windows">External IPs on macOS and Windows</h4>

<p>This demo runs Docker on Linux, which allows sending traffic directly to the load-balancer’s external IP if the IP space is within the docker IP space.
On macOS and Windows however, docker does not expose the docker network to the host, rendering the external IP unreachable from other kind nodes. In order to workaround this, one could expose pods and services using extra port mappings as shown in the extra port mappings section of kind’s <a href="https://kind.sigs.k8s.io/docs/user/configuration#extra-port-mappings">Configuration Guide</a>.</p>

<h3 id="deploying-cluster">Deploying cluster</h3>

<p>To start a kind cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kind create cluster
</code></pre></div></div>

<p>In order to interact with the specific cluster created:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl cluster-info <span class="nt">--context</span> kind-kind
</code></pre></div></div>

<h3 id="installing-components">Installing components</h3>

<h4 id="installing-metallb-on-the-cluster">Installing MetalLB on the cluster</h4>

<p>There are <a href="https://metallb.universe.tf/installation/">many ways</a> to install MetalLB. For the sake of this example, we will install MetalLB via manifests. To do this, follow this <a href="https://metallb.universe.tf/installation/#installation-by-manifest">guide</a>. 
Confirm successful installation by waiting for MetalLB pods to have a status of Running:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-n</span> metallb-system <span class="nt">--watch</span>
</code></pre></div></div>

<h4 id="installing-kubevirt-on-the-cluster">Installing Kubevirt on the cluster</h4>

<p>Following Kubevirt <a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a> to install released version v0.51.0</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span>v0.51.0
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="network-resources-configuration">Network resources configuration</h3>

<h4 id="setting-address-pool-to-be-used-by-the-loadbalancer">Setting Address Pool to be used by the LoadBalancer</h4>

<p>In order to complete the Layer 2 mode configuration, we need to set a range of IP addresses for the LoadBalancer to use.
On Linux we can use the docker kind network (macOS and Windows users see <a href="#external-ips-on-macos-and-windows">External IPs Prerequirement</a>), so by using this command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network inspect <span class="nt">-f</span> <span class="s1">''</span> kind
</code></pre></div></div>

<p>You should get the subclass you can set the IP range from. The output should contain a cidr such as 172.18.0.0/16.
Using this result we will create the following Layer 2 address pool with 172.18.1.1-172.18.1.16 range:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metallb-system</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">config</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">address-pools:</span>
    <span class="s">- name: addresspool-sample1</span>
      <span class="s">protocol: layer2</span>
      <span class="s">addresses:</span>
      <span class="s">- 172.18.1.1-172.18.1.16</span>
<span class="s">EOF</span>
</code></pre></div></div>

<h3 id="network-utilization">Network utilization</h3>

<h4 id="spin-up-a-virtual-machine-running-nginx">Spin up a Virtual Machine running Nginx</h4>

<p>Now it’s time to start-up a virtual machine running nginx using the following yaml.
The virtual machine has a <code class="language-plaintext highlighter-rouge">metallb-service=nginx</code> we created to use when creating the service.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora-nginx</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
          <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirt/fedora-cloud-container-disk-demo</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
        <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
              <span class="s">packages:</span>
                <span class="s">- nginx</span>
              <span class="s">runcmd:</span>
                <span class="s">- [ "systemctl", "enable", "--now", "nginx" ]</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="s">EOF</span>
</code></pre></div></div>

<h4 id="expose-the-virtual-machine-with-a-typed-loadbalancer-service">Expose the virtual machine with a typed <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service</h4>

<p>When creating the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> typed service, we need to remember annotating the address-pool we want to use 
<code class="language-plaintext highlighter-rouge">addresspool-sample1</code> and also add the selector <code class="language-plaintext highlighter-rouge">metallb-service: nginx</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">metallb-nginx-svc</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">metallb.universe.tf/address-pool</span><span class="pi">:</span> <span class="s">addresspool-sample1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">externalTrafficPolicy</span><span class="pi">:</span> <span class="s">Local</span>
  <span class="na">ipFamilies</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">IPv4</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tcp-5678</span>
      <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">5678</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>Notice that the service got assigned with an external IP from the range assigned by the address pool:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get service <span class="nt">-n</span> default metallb-nginx-svc
</code></pre></div></div>

<p>Example output:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
metallb-nginx-svc   LoadBalancer   10.96.254.136   172.18.1.1    5678:32438/TCP   53s
</code></pre></div></div>

<h4 id="access-the-virtual-machine-from-outside-the-cluster">Access the virtual machine from outside the cluster</h4>

<p>Finally, we can check that the nginx server is accessible from outside the cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-s</span> <span class="nt">-o</span> /dev/null 172.18.1.1:5678 <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"URL exists"</span>
</code></pre></div></div>

<p>Example output:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>URL exists
</code></pre></div></div>
<p>Note that it may take a short while for the URL to work after setting the service.</p>

<h2 id="doing-this-on-your-own-cluster">Doing this on your own cluster</h2>

<p>Moving outside the demo example, one who would like use MetalLB on their real life cluster, should also take other considerations in mind:</p>
<ul>
  <li>User privileges: you should have <code class="language-plaintext highlighter-rouge">cluster-admin</code> privileges on the cluster - in order to install MetalLB.</li>
  <li>IP Ranges for MetalLB: getting IP Address pools allocation for MetalLB depends on your cluster environment:
    <ul>
      <li>If you’re running a bare-metal cluster in a shared host environment, you need to first reserve this IP Address pool from your hosting provider.</li>
      <li>Alternatively, if you’re running on a private cluster, you can use one of the private IP Address spaces (a.k.a RFC1918 addresses). Such addresses are free, and work fine as long as you’re only providing cluster services to your LAN.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog post we used MetalLB to expose a service using an external IP assigned to a virtual machine. 
This illustrates how virtual machine traffic can be load-balanced via a service.</p>]]></content><author><name>Ram Lavi</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="load-balancer" /><category term="MetalLB" /><summary type="html"><![CDATA[This post illustrates setting up a virtual machine with MetalLB LoadBalancer service.]]></summary></entry><entry><title type="html">KubeVirt v0.51.0</title><link href="https://kubevirt.io//2022/changelog-v0.51.0.html" rel="alternate" type="text/html" title="KubeVirt v0.51.0" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.51.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.51.0.html"><![CDATA[<h2 id="v0510">v0.51.0</h2>

<p>Released on: Tue Mar 8 21:06:59 2022 +0000</p>

<ul>
  <li>[PR #7102][machadovilaca] Add Virtual Machine name label to virt-launcher pod</li>
  <li>[PR #7139][davidvossel] Fixes inconsistent VirtualMachinePool VM/VMI updates by using controller revisions</li>
  <li>[PR #6754][jean-edouard] New and resized disks are now always 1MiB-aligned</li>
  <li>[PR #7086][acardace] Add ‘EvictionStrategy’ as a cluster-wide setting in the KubeVirt CR</li>
  <li>[PR #7232][rmohr] Properly format the PDB scale event during migrations</li>
  <li>[PR #7223][Barakmor1] Add a name label to virt-operator pods</li>
  <li>[PR #7221][davidvossel] RunStrategy: Once - allows declaring a VM should run once to a finalized state</li>
  <li>[PR #7091][EdDev] SR-IOV interfaces are now reported in the VMI status even without an active guest-agent.</li>
  <li>[PR #7169][rmohr] Improve device plugin de-registration in virt-handler and some test stabilizations</li>
  <li>[PR #6604][alicefr] Add shareable option to identify if the disk is shared with other VMs</li>
  <li>[PR #7144][davidvossel] Garbage collect finalized migration objects only leaving the most recent 5 objects</li>
  <li>[PR #6110][xpivarc] [Nonroot] SRIOV is now available.</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.51.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.50.0</title><link href="https://kubevirt.io//2022/changelog-v0.50.0.html" rel="alternate" type="text/html" title="KubeVirt v0.50.0" /><published>2022-02-09T00:00:00+00:00</published><updated>2022-02-09T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.50.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.50.0.html"><![CDATA[<h2 id="v0500">v0.50.0</h2>

<p>Released on: Wed Feb 9 18:01:08 2022 +0000</p>

<ul>
  <li>[PR #7056][fossedihelm] Update k8s dependencies to 0.23.1</li>
  <li>[PR #7135][davidvossel] Switch from reflects.DeepEquals to equality.Semantic.DeepEquals() across the entire project</li>
  <li>[PR #7052][sradco] Updated recording rule “kubevirt_vm_container_free_memory_bytes”</li>
  <li>[PR #7000][iholder-redhat] Adds a possibility to override default libvirt log filters though VMI annotations</li>
  <li>[PR #7064][davidvossel] Fixes issue associated with blocked uninstalls when VMIs exist during removal</li>
  <li>[PR #7097][iholder-redhat] [Bug fix] VMI with kernel boot stuck on “Terminating” status if more disks are defined</li>
  <li>[PR #6700][VirrageS] Simplify replacing <code class="language-plaintext highlighter-rouge">time.Ticker</code> in agent poller and fix default values for <code class="language-plaintext highlighter-rouge">qemu-*-interval</code> flags</li>
  <li>[PR #6581][ormergi] SRIOV network interfaces are now hot-plugged when disconnected manually or due to aborted migrations.</li>
  <li>[PR #6924][EdDev] Support for legacy GPU definition is removed. Please see https://kubevirt.io/user-guide/virtual_machines/host-devices on how to define host-devices.</li>
  <li>[PR #6735][uril] The command <code class="language-plaintext highlighter-rouge">migrate_cancel</code> was added to virtctl. It cancels an active VM migration.</li>
  <li>[PR #6883][rthallisey] Add instance-type to cloud-init metadata</li>
  <li>[PR #6999][maya-r] When expanding disk images, take the minimum between the request and the capacity - avoid using the full underlying file system on storage like NFS, local.</li>
  <li>[PR #6946][vladikr] Numa information of an assigned device will be presented in the devices metadata</li>
  <li>[PR #6042][iholder-redhat] Fully support cgroups v2, include a new cohesive package and perform major refactoring.</li>
  <li>[PR #6968][vladikr] Added Writeback disk cache support</li>
  <li>[PR #6995][sradco] Alert OrphanedVirtualMachineImages name was changed to OrphanedVirtualMachineInstances.</li>
  <li>[PR #6923][rhrazdil] Fix issue with ssh being unreachable on VMIs with Istio proxy</li>
  <li>[PR #6821][jean-edouard] Migrating VMIs that contain dedicated CPUs will now have properly dedicated CPUs on target</li>
  <li>[PR #6793][oshoval] Add infoSource field to vmi.status.interfaces.</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.50.0 changes]]></summary></entry></feed>