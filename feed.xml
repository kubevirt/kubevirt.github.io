<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kubevirt.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2025-10-30T15:46:02+00:00</updated><id>https://kubevirt.io//feed.xml</id><title type="html">KubeVirt.io</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">Stretching a Layer 2 network over multiple KubeVirt clusters</title><link href="https://kubevirt.io//2025/Stretched-layer2-network-between-clusters.html" rel="alternate" type="text/html" title="Stretching a Layer 2 network over multiple KubeVirt clusters" /><published>2025-10-13T00:00:00+00:00</published><updated>2025-10-13T00:00:00+00:00</updated><id>https://kubevirt.io//2025/Stretched-layer2-network-between-clusters</id><content type="html" xml:base="https://kubevirt.io//2025/Stretched-layer2-network-between-clusters.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>KubeVirt enables you to run virtual machines (VMs) within Kubernetes clusters,
but networking VMs across multiple clusters presents significant challenges.
Current KubeVirt networking relies on cluster-local solutions, which cannot
extend Layer 2 broadcast domains beyond cluster boundaries. This limitation
forces applications that require L2 connectivity to either remain within a
single cluster or undergo complex network reconfiguration when distributed
across clusters.</p>

<p>Integrating with EVPN addresses this fundamental limitation in distributed
KubeVirt deployments: the inability to maintain L2 adjacency between VMs
running on different clusters. By leveraging EVPN’s BGP-based control plane and
advanced MAC/IP advertisement mechanisms, we can now stretch Layer 2 broadcast
domains across geographically distributed KubeVirt clusters, creating a unified
network fabric that treats multiple clusters as a single, cohesive
infrastructure.</p>

<h3 id="why-stretch-l2-networks-across-different-clusters-">Why Stretch L2 Networks across different clusters ?</h3>
<p>The ability to extend L2 domains between KubeVirt clusters unlocks several
critical capabilities that were previously difficult to achieve.
Traditional cluster networking creates isolation boundaries that, while
beneficial for security and resource management, can become barriers when
applications require tight coupling or when operational requirements demand
flexibility in workload placement.</p>

<p>All in all, stretching an L2 domain across cluster boundaries enables use cases
that are fundamental to infrastructure reliability and flexibility, which include:</p>
<ul>
  <li><strong>Cross Cluster Live Migration:</strong> VMs must migrate between clusters without
requiring IP address changes, DNS updates, or application reconfiguration. This
capability is essential for disaster recovery scenarios where VMs must failover
to geographically distant clusters while still maintaining their network
identity and established connections.</li>
  <li><strong>Legacy enterprise applications availability:</strong> many mission-critical
workloads were designed with assumptions about L2 adjacency, such as database
clusters requiring heartbeat mechanisms over broadcast domains, application
servers expecting multicast discovery, or network-attached storage systems
relying on L2 protocols.</li>
  <li><strong>Resource optimization and capacity planning:</strong> organizations can distribute
VM workloads based on compute availability, cost considerations, or compliance
requirements while maintaining the network simplicity that applications expect.
This flexibility becomes particularly valuable in hybrid cloud scenarios where
workloads may need to seamlessly span on-premises KubeVirt clusters and
cloud-hosted instances.</li>
</ul>

<p>This is where the power of EVPN comes into play: by integrating EVPN into the
KubeVirt ecosystem, we can create a sophisticated L2 overlay. Think of it as a
virtual network fabric that stretches across your data centers or cloud
regions, enabling the workloads running in KubeVirt clusters to attach to a
single, unified L2 domain.</p>

<p>In this post, we’ll dive into how this powerful combination works and how it
unlocks true application mobility for your virtualized workloads on Kubernetes.</p>

<h2 id="prerequisites">Prerequisites</h2>
<p>The list below is required to run this demo. This will enable you to run
multiple Kubernetes clusters in your own laptop, interconnected by EVPN using
<a href="https://openperouter.github.io/">openperouter</a>.</p>
<ul>
  <li>container runtime - docker - installed in your system</li>
  <li>git</li>
  <li>make</li>
</ul>

<h2 id="the-testbed">The testbed</h2>
<p>The testbed will be implemented using a physical network deployed in
leaf/spine topology, which is a common two-layer network architecture used in
data centers. It consists of leaf switches that connect to end devices, and
spine switches that interconnect all leaf switches. This way, workloads will
always be (at most) two hops away from one another.</p>

<p align="center">
  <img src="../assets/2025-10-13-evpn-integration/01-evpn-integration-testbed.png" alt="The testbed" width="100%" />
</p>

<p>The diagram highlights the autonomous system (AS) numbers each of the
components will use.</p>

<p>We can infer from the AS numbers provided above that the testbed will feature
eBGP configuration, thus providing routing between different autonomous
systems.</p>

<p>We will setup the testbed using <a href="https://containerlab.dev/">containerlab</a>, and
the Kubernetes clusters are deployed using <a href="https://kind.sigs.k8s.io/">KinD</a>.
The BGP speakers (routers) in each leaf are implemented using
<a href="https://frrouting.org/">FRR</a>.</p>

<h3 id="spawning-the-testbed-on-your-laptop">Spawning the testbed on your laptop</h3>
<p>To spawn the tested in your laptop, you should clone the openperouter repo.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/openperouter/openperouter.git
git checkout c9d591a
<span class="nb">cd </span>openperouter
</code></pre></div></div>

<p>Assuming you have all the <a href="#prerequisites">requirements</a> installed in your
laptop, all you need to do is build the router component, and execute the
<code class="language-plaintext highlighter-rouge">deploy-multi</code> make target. Then, you should be ready to go!</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sysctl <span class="nt">-w</span> fs.inotify.max_user_instances<span class="o">=</span>1024 <span class="c"># might need sudo</span>
make docker-build <span class="o">&amp;&amp;</span> make deploy-multi
</code></pre></div></div>

<p>After running this make target, you should have the testbed deployed as shown
in the testbed’s <a href="#the-testbed">diagram</a>. One thing is missing though: the
autonomous systems in the kind clusters are not configured yet! This will be
configured in the <a href="#configuring-the-kubevirt-clusters">next section</a>.</p>

<p>The kubeconfigs to connect to each cluster can be found in <code class="language-plaintext highlighter-rouge">openperouter</code>’s
<code class="language-plaintext highlighter-rouge">bin</code> directory:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-<span class="k">*</span>
/root/github/openperouter/bin/kubeconfig-pe-kind-a  /root/github/openperouter/bin/kubeconfig-pe-kind-b
</code></pre></div></div>

<p>Before moving to the configuration section, let’s install KubeVirt in both
clusters:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>kubeconfig <span class="k">in</span> <span class="si">$(</span><span class="nb">ls </span>bin/kubeconfig-<span class="k">*</span><span class="si">)</span><span class="p">;</span> <span class="k">do
    </span><span class="nb">echo</span> <span class="s2">"Installing KubeVirt in cluster using KUBECONFIG=</span><span class="nv">$kubeconfig</span><span class="s2">"</span>
    <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$kubeconfig</span> kubectl apply <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/v1.5.2/kubevirt-operator.yaml
    <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$kubeconfig</span> kubectl apply <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/v1.5.2/kubevirt-cr.yaml
    <span class="c"># Patch KubeVirt to allow scheduling on control-planes, so we can test live migration between two nodes</span>
    <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$kubeconfig</span> kubectl patch <span class="nt">-n</span> kubevirt kubevirt kubevirt <span class="nt">--type</span> merge <span class="nt">--patch</span> <span class="s1">'{"spec": {"workloads": {"nodePlacement": {"tolerations": [{"key": "node-role.kubernetes.io/control-plane", "operator": "Exists", "effect": "NoSchedule"}]}}}}'</span>
    <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$kubeconfig</span> kubectl <span class="nb">wait</span> <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Available kubevirt/kubevirt <span class="nt">-n</span> kubevirt <span class="nt">--timeout</span><span class="o">=</span>10m
    <span class="nb">echo</span> <span class="s2">"Finished installing KubeVirt in cluster using KUBECONFIG=</span><span class="nv">$kubeconfig</span><span class="s2">"</span>
<span class="k">done</span>
</code></pre></div></div>

<h2 id="configuring-the-kubevirt-clusters">Configuring the KubeVirt clusters</h2>
<p>As indicated in the <a href="#introduction">introduction</a> section, the end goal is to
stretch a layer 2 network across both Kubernetes clusters, using EVPN. Please
refer to the image below for a simple diagram.</p>

<p align="center">
  <img src="../assets/2025-10-13-evpn-integration/02-stretched-l2-evpn.png" alt="Layer 2 network stretched across both clusters" width="100%" />
</p>

<p>In order to stretch an L2 overlay across both cluster we need to:</p>
<ul>
  <li>configure the underlay network</li>
  <li>configure the EVPN VXLAN VNI</li>
</ul>

<p>We will rely on <a href="https://openperouter.github.io/">openperouter</a> for both of
these.</p>

<p>Let’s start with the underlay network, in which we will connect the Kubernetes
clusters to each cluster’s top of rack BGP/EVPN speaker.</p>

<h3 id="configuring-the-underlay-network">Configuring the underlay network</h3>
<p>The first thing we need to do is to finish setting up the testbed by
peering our two Kubernetes clusters with the BGP/EVPN speaker in each cluster’s
top of rack:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">kindleaf-a</code> for cluster-a</li>
  <li><code class="language-plaintext highlighter-rouge">kindleaf-b</code> for cluster-b</li>
</ul>

<p>This will require you to specify the expected AS numbers, to define the VXLAN
tunnel endpoint addresses, and also specify which node interface will be used
to connect to external routers.</p>

<p>For that,
you will need to provision the following CRs:</p>

<ul>
  <li>Cluster A.</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: Underlay
metadata:
  name: underlay
  namespace: openperouter-system
spec:
  asn: 64514
  evpn:
    vtepcidr:  100.65.0.0/24
  nics:
    - toswitch
  neighbors:
    - asn: 64512
      address: 192.168.11.2
</span><span class="no">EOF
</span></code></pre></div></div>

<ul>
  <li>Cluster B.</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: Underlay
metadata:
  name: underlay
  namespace: openperouter-system
spec:
  asn: 64518
  evpn:
    vtepcidr: 100.65.1.0/24
  routeridcidr: 10.0.1.0/24
  nics:
    - toswitch
  neighbors:
    - asn: 64516
      address: 192.168.12.2
</span><span class="no">EOF
</span></code></pre></div></div>

<h3 id="configuring-the-evpn-vni">Configuring the EVPN VNI</h3>
<p>Once we have configured both Kubernetes cluster’s peering with the external
routers in <code class="language-plaintext highlighter-rouge">kindleaf-a</code> and <code class="language-plaintext highlighter-rouge">kindleaf-b</code>, we can now focus on defining the
<code class="language-plaintext highlighter-rouge">layer2</code> EVPN. For that, we will use openperouter’s <code class="language-plaintext highlighter-rouge">L2VNI</code> CRD.</p>

<p>Execute the following commands to provision the <code class="language-plaintext highlighter-rouge">L2VNI</code> in both clusters:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># provision L2VNI in cluster: pe-kind-a</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L2VNI
metadata:
  name: layer2
  namespace: openperouter-system
spec:
  hostmaster:
    autocreate: true
    type: bridge
  l2gatewayip: 192.170.1.1/24
  vni: 110
  vrf: red
</span><span class="no">EOF

</span><span class="c"># provision L2VNI in cluster: pe-kind-b</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L2VNI
metadata:
  name: layer2
  namespace: openperouter-system
spec:
  hostmaster:
    autocreate: true
    type: bridge
  l2gatewayip: 192.170.1.1/24
  vni: 110
  vrf: red
</span><span class="no">EOF
</span></code></pre></div></div>

<p>After this step, we will have created an L2 overlay network on top of the
network fabric. We now need to enable it to be plumbed to the workloads.
Execute the commands below to provision a network attachment definition in both
clusters:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># provision NAD in cluster: pe-kind-a</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: evpn
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "evpn",
      "type": "bridge",
      "bridge": "br-hs-110",
      "macspoofchk": false,
      "disableContainerInterface": true
    }
</span><span class="no">EOF

</span><span class="c"># provision NAD in cluster: pe-kind-b</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: evpn
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "evpn",
      "type": "bridge",
      "bridge": "br-hs-110",
      "macspoofchk": false,
      "disableContainerInterface": true
    }
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Now that we have set up networking for the workloads, we can proceed with
actually instantiating the VMs which will attach to this network overlay.</p>

<h3 id="provisioning-and-running-the-vm-workloads">Provisioning and running the VM workloads</h3>

<p>You will have one VM running in cluster A (vm-1), and another VM running in
cluster B (vm-2).</p>

<p>The VMs will each have one network interface, attached to the layer2 overlay.
The VMs are using bridge binding, and they attach to the overlay using bridge-cni.
Both VMs have static IPs, configured over cloud-init. They are:</p>

<table>
  <thead>
    <tr>
      <th>VM name</th>
      <th>Cluster</th>
      <th>IP address</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>vm-1</td>
      <td>pe-kind-a</td>
      <td>192.170.1.3</td>
    </tr>
    <tr>
      <td>vm-2</td>
      <td>pe-kind-b</td>
      <td>192.170.1.30</td>
    </tr>
  </tbody>
</table>

<p>To provision these, follow these steps:</p>

<ol>
  <li>Provision <code class="language-plaintext highlighter-rouge">vm-1</code> in cluster <code class="language-plaintext highlighter-rouge">pe-kind-a</code>:</li>
</ol>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-1
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-1
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      domain:
        devices:
          interfaces:
          - bridge: {}
            name: evpn
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
        resources:
          requests:
            memory: 2048M
        machine:
          type: ""
      networks:
      - multus:
          networkName: evpn
        name: evpn
      terminationGracePeriodSeconds: 0
      volumes:
      - containerDisk:
          image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.5.2
        name: containerdisk
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth0:
                addresses:
                - 192.170.1.3/24
                gateway4: 192.170.1.1
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<ol>
  <li>Provision <code class="language-plaintext highlighter-rouge">vm-2</code> in cluster <code class="language-plaintext highlighter-rouge">pe-kind-b</code>:</li>
</ol>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-2
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-2
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      domain:
        devices:
          interfaces:
          - bridge: {}
            name: evpn
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
        resources:
          requests:
            memory: 2048M
        machine:
          type: ""
      networks:
      - multus:
          networkName: evpn
        name: evpn
      terminationGracePeriodSeconds: 0
      volumes:
      - containerDisk:
          image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.5.2
        name: containerdisk
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth0:
                addresses:
                - 192.170.1.30/24
                gateway4: 192.170.1.1
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>We will use <code class="language-plaintext highlighter-rouge">vm-2</code> (which runs in cluster <strong>B</strong>) as the “server”, and <code class="language-plaintext highlighter-rouge">vm-1</code>
(which runs in cluster <strong>A</strong>) as the “client”; however, we first need to wait
for the VMs to become <code class="language-plaintext highlighter-rouge">Ready</code>:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span>bin/kubeconfig-pe-kind-a kubectl <span class="nb">wait </span>vm vm-1 <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Ready <span class="nt">--timeout</span><span class="o">=</span>60s
<span class="nv">KUBECONFIG</span><span class="o">=</span>bin/kubeconfig-pe-kind-b kubectl <span class="nb">wait </span>vm vm-2 <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Ready <span class="nt">--timeout</span><span class="o">=</span>60s
</code></pre></div></div>

<p>Now that we know the VMs are <code class="language-plaintext highlighter-rouge">Ready</code>, let’s confirm the IP address for <code class="language-plaintext highlighter-rouge">vm-2</code>,
and reach into it from the <code class="language-plaintext highlighter-rouge">vm-1</code> VM, which is available in cluster A.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span>bin/kubeconfig-pe-kind-b kubectl get vmi vm-2 <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{.status.interfaces[0].ipAddress}"</span>
192.170.1.30
</code></pre></div></div>

<p>Let’s now serve some data. We will use a toy python webserver for that, which serves some files:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span><span class="nb">touch</span> <span class="si">$(</span><span class="nb">date</span><span class="si">)</span>
<span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-la</span>
total 12
drwx------. 1 fedora fedora 122 Oct 13 12:08 <span class="nb">.</span>
drwxr-xr-x. 1 root   root    12 Sep 13  2024 ..
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 12:08:15
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 13
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 2025
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora  18 Jul 21  2021 .bash_logout
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora 141 Jul 21  2021 .bash_profile
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora 492 Jul 21  2021 .bashrc
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 Mon
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 Oct
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 PM
drwx------. 1 fedora fedora  30 Sep 13  2024 .ssh
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 UTC
<span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span>python3 <span class="nt">-m</span> http.server 8090
Serving HTTP on 0.0.0.0 port 8090 <span class="o">(</span>http://0.0.0.0:8090/<span class="o">)</span> ...
</code></pre></div></div>

<p>And let’s try to access that from the VM which runs in the other cluster:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span>bin/kubeconfig-pe-kind-a virtctl console vm-1
<span class="c"># password to access the VM is fedora/fedora</span>
<span class="o">[</span>fedora@vm-1 ~]<span class="nv">$ </span>curl 192.170.1.30:8090
&lt;<span class="o">!</span>DOCTYPE HTML PUBLIC <span class="s2">"-//W3C//DTD HTML 4.01//EN"</span> <span class="s2">"http://www.w3.org/TR/html4/strict.dtd"</span><span class="o">&gt;</span>
&lt;html&gt;
&lt;<span class="nb">head</span><span class="o">&gt;</span>
&lt;meta http-equiv<span class="o">=</span><span class="s2">"Content-Type"</span> <span class="nv">content</span><span class="o">=</span><span class="s2">"text/html; charset=utf-8"</span><span class="o">&gt;</span>
&lt;title&gt;Directory listing <span class="k">for</span> /&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Directory listing <span class="k">for</span> /&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">".bash_logout"</span><span class="o">&gt;</span>.bash_logout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">".bash_profile"</span><span class="o">&gt;</span>.bash_profile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">".bashrc"</span><span class="o">&gt;</span>.bashrc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">".ssh/"</span><span class="o">&gt;</span>.ssh/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"12%3A08%3A15"</span><span class="o">&gt;</span>12:08:15&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"13"</span><span class="o">&gt;</span>13&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"2025"</span><span class="o">&gt;</span>2025&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"Mon"</span><span class="o">&gt;</span>Mon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"Oct"</span><span class="o">&gt;</span>Oct&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"PM"</span><span class="o">&gt;</span>PM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"UTC"</span><span class="o">&gt;</span>UTC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre></div></div>

<p>As you can see, the VM running in cluster A was able to successfully reach into
the VM running in cluster B.</p>

<h3 id="bonus-track-connecting-to-provider-networks-using-an-l3vni">Bonus track: connecting to provider networks using an L3VNI</h3>
<p>This extra (optional) step showcases how you can import provider network routes
into the Kubernetes clusters - essentially creating an L3 overlay - using
<code class="language-plaintext highlighter-rouge">openperouter</code>s L3VNI CRD.</p>

<p>We will use it to reach into the webserver hosted in <code class="language-plaintext highlighter-rouge">hostA</code> (attached to
<code class="language-plaintext highlighter-rouge">leafA</code> in the <a href="#the-testbed">diagram</a>) from the VMs running in both clusters.
Please refer to the image below to get a better understanding of the scenario.</p>

<p align="center">
  <img src="../assets/2025-10-13-evpn-integration/03-wrap-l3vni-over-stretched-l2.png" alt="Wrap an L3VNI over a stretched L2 EVPN" width="100%" />
</p>

<p>Since we already have configured the <code class="language-plaintext highlighter-rouge">underlay</code> in a
<a href="#configuring-the-underlay-network">previous step</a>, all we need to do is to
configure the <code class="language-plaintext highlighter-rouge">L3VNI</code>; for that, provision the following CR in <strong>both</strong>
clusters:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># provision L3VNI in cluster: pe-kind-a</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L3VNI
metadata:
  name: red
  namespace: openperouter-system
spec:
  vni: 100
  vrf: red
</span><span class="no">EOF

</span><span class="c"># provision L3VNI in cluster: pe-kind-b</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L3VNI
metadata:
  name: red
  namespace: openperouter-system
spec:
  vni: 100
  vrf: red
</span><span class="no">EOF
</span></code></pre></div></div>

<p>This will essentially wrap the existing <code class="language-plaintext highlighter-rouge">L2VNI</code> with an L3 domain - i.e. a
separate Virtual Routing Function (VRF), whose Virtual Network Identifier (VNI)
is 100. This will enable the Kubernetes clusters to reach into services located in
the <code class="language-plaintext highlighter-rouge">red</code> VRF (which have VNI = 100). Services on <code class="language-plaintext highlighter-rouge">hostA</code> and/or <code class="language-plaintext highlighter-rouge">hostB</code> with
VNI = 200 are not accessible, since we haven’t exposed them over EVPN (using an
<code class="language-plaintext highlighter-rouge">L3VNI</code>).</p>

<p>Once we’ve provisioned the aforementioned <code class="language-plaintext highlighter-rouge">L3VNI</code>, we can now check accessing
the webserver located in the host in <code class="language-plaintext highlighter-rouge">leafA</code> - <code class="language-plaintext highlighter-rouge">clab-kind-leafA</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec </span>clab-kind-hostA_red ip <span class="nt">-4</span> addr show dev eth1
228: eth1@if227: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9500 qdisc noqueue state UP group default  link-netnsid 1
    inet 192.168.20.2/24 scope global eth1
       valid_lft forever preferred_lft forever
</code></pre></div></div>

<p>Let’s also check the same thing for the <code class="language-plaintext highlighter-rouge">blue</code> VRF - for which we do <strong>not</strong>
have any VNI configuration.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec </span>clab-kind-hostA_blue ip <span class="nt">-4</span> addr show dev eth1
273: eth1@if272: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9500 qdisc noqueue state UP group default  link-netnsid 1
    inet 192.168.21.2/24 scope global eth1
       valid_lft forever preferred_lft forever
</code></pre></div></div>

<p>And let’s now access these services from the VMs we have in both clusters.</p>

<p>From <code class="language-plaintext highlighter-rouge">vm-1</code>, in cluster A:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># username/password =&gt; fedora/fedora</span>
virtctl console vm-1
Successfully connected to vm-1 console. The escape sequence is ^]

vm-1 login: fedora
Password:
<span class="o">[</span>fedora@vm-1 ~]<span class="nv">$ </span>curl 192.168.20.2:8090/clientip <span class="c"># we have access to the RED VRF</span>
192.170.1.3:35146
<span class="o">[</span>fedora@vm-1 ~]<span class="nv">$ </span>curl 192.168.21.2:8090/clientip <span class="c"># we do NOT have access to the BLUE VRF</span>
curl: <span class="o">(</span>28<span class="o">)</span> Failed to connect to 192.168.21.2 port 8090 after 128318 ms: Connection timed out
</code></pre></div></div>

<p>From <code class="language-plaintext highlighter-rouge">vm-2</code>, in cluster B:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># username/password =&gt; fedora/fedora</span>
virtctl console vm-2
Successfully connected to vm-2 console. The escape sequence is ^]

vm-2 login: fedora
Password:
<span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span>curl 192.168.20.2:8090/clientip <span class="c"># we have access to the RED VRF</span>
192.170.1.30:52924
<span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span>curl 192.168.21.2:8090/clientip <span class="c"># we do NOT have access to the BLUE VRF</span>
curl: <span class="o">(</span>28<span class="o">)</span> Failed to connect to 192.168.21.2 port 8090 after 130643 ms: Connection timed out
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>
<p>In this article we have explained EVPN and which virtualization use cases it
can provide.</p>

<p>We have also shown how the <a href="https://openperouter.github.io/">openperouter</a>
<code class="language-plaintext highlighter-rouge">L2VNI</code> CRD can be used to stretch a Layer 2 overlay across multiple Kubernetes
clusters.</p>

<p>Finally, we have also seen how <code class="language-plaintext highlighter-rouge">openperouter</code> <code class="language-plaintext highlighter-rouge">L3VNI</code> can be used to create
Layer 3 overlays, which allows the VMs running in the Kubernetes clusters to
access services in the exposed provider networks.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="evpn" /><category term="bgp" /><category term="openperouter" /><category term="network" /><category term="networking" /><summary type="html"><![CDATA[Explore EVPN and see how openperouter creates Layer 2 and Layer 3 overlays across Kubernetes clusters.]]></summary></entry><entry><title type="html">Building VM golden images with Packer</title><link href="https://kubevirt.io//2025/Building-VM-golden-image-with-Packer.html" rel="alternate" type="text/html" title="Building VM golden images with Packer" /><published>2025-09-15T00:00:00+00:00</published><updated>2025-09-15T00:00:00+00:00</updated><id>https://kubevirt.io//2025/Building-VM-golden-image-with-Packer</id><content type="html" xml:base="https://kubevirt.io//2025/Building-VM-golden-image-with-Packer.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Creating and maintaining VM golden images can be time-consuming, often
requiring local virtualization tools and manual setup. With <a href="https://kubevirt.io">KubeVirt</a>
running inside your <a href="https://kubernetes.io">Kubernetes</a> cluster, you can
manage virtual machines alongside your containers, but it lacks automation
for creating consistent, reusable VM images.</p>

<p>That’s where <a href="https://packer.io">Packer</a> and the new KubeVirt plugin come
in. The plugin lets you build VM images directly in Kubernetes, enabling you
to automate OS installation from ISO, customize the VM during build, and produce
a reusable bootable volume, all without leaving your cluster.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before you begin, make sure you have the following installed:</p>

<ul>
  <li><a href="https://developer.hashicorp.com/packer/install">Packer</a></li>
  <li><a href="https://kubernetes.io/docs/tasks/tools">Kubernetes</a></li>
  <li><a href="https://kubevirt.io/user-guide/cluster_admin/installation">KubeVirt</a></li>
  <li><a href="https://kubevirt.io/user-guide/storage/containerized_data_importer/#install-cdi">Containerized Data Importer (CDI)</a></li>
</ul>

<h2 id="plugin-features">Plugin Features</h2>

<p>The Packer plugin for KubeVirt offers a variety of features that simplify
the VM golden image creation process:</p>

<ul>
  <li><strong>HCL Template</strong>: Define infrastructure as code for easy versioning and reuse using <a href="https://developer.hashicorp.com/packer/docs/templates/hcl_templates">HCL templates</a>.</li>
  <li><strong>ISO Installation</strong>: Build VM golden images from an ISO file using the <code class="language-plaintext highlighter-rouge">kubevirt-iso</code> builder.</li>
  <li><strong>ISO Media Files</strong>: Include additional files (e.g., configs, scripts, and more) during the installation process.</li>
  <li><strong>Boot Command</strong>: Automate the VM boot process via a <a href="https://en.wikipedia.org/wiki/VNC">VNC</a> connection with a predefined set of commands.</li>
  <li><strong>Integrated SSH/WinRM Access</strong>: Provision and customize VMs via <a href="https://man7.org/linux/man-pages/man1/ssh.1.html">SSH</a> or <a href="https://learn.microsoft.com/en-us/windows/win32/winrm/portal">WinRM</a>.</li>
</ul>

<p><strong>Note</strong>: This plugin is currently in pre-release and actively under development by
<a href="https://www.redhat.com">Red Hat</a> and <a href="https://www.hashicorp.com">HashiCorp</a> together.</p>

<h2 id="plugin-components">Plugin Components</h2>

<p>The core component of this plugin is the <code class="language-plaintext highlighter-rouge">kubevirt-iso</code> builder. This builder
allows you to start from an ISO file and create a VM golden image directly
on your Kubernetes cluster.</p>

<h3 id="builder-design">Builder Design</h3>

<p align="center">
  <img src="../assets/2025-09-15-Packer-Plugin/kubevirt-iso-builder-design.png" alt="Design" width="1125" />
</p>

<p>This diagram shows the workflow for building a bootable volume in a
Kubernetes cluster using Packer with the KubeVirt plugin.</p>

<ol>
  <li>Creates a temporary VM from an ISO image.</li>
  <li>Runs provisioning using either the <a href="https://developer.hashicorp.com/packer/docs/provisioners/shell">Shell</a> or <a href="https://developer.hashicorp.com/packer/integrations/hashicorp/ansible/latest/components/provisioner/ansible">Ansible</a> provisioner.</li>
  <li>Clones the VM’s disk to create a reusable bootable volume (<a href="https://kubevirt.io/user-guide/storage/disks_and_volumes/#datavolume">DataVolume and DataSource</a>).</li>
</ol>

<p>This bootable volume can then be reused to instantiate new VMs without
repeating the installation.</p>

<h2 id="step-by-step-example-building-a-fedora-vm-image">Step-by-Step Example: Building a Fedora VM Image</h2>

<p>The following Packer template (Fedora 42) demonstrates key features:</p>

<ul>
  <li>ISO-based installation using the <code class="language-plaintext highlighter-rouge">kubevirt-iso</code> builder.</li>
  <li>Embedded configuration file to automate the installation.</li>
  <li>Sending boot commands to inject <code class="language-plaintext highlighter-rouge">ks.cfg</code> in GRUB.</li>
  <li>SSH provisioning with a <a href="https://developer.hashicorp.com/packer/docs/provisioners/shell">Shell</a> provisioner.</li>
  <li>Full integration with <a href="https://kubevirt.io/user-guide/user_workloads/instancetypes">InstanceTypes and Preferences</a>.</li>
</ul>

<p>Follow these steps to build a Fedora VM image inside your Kubernetes cluster.</p>

<h3 id="step-1-export-kubeconfig-variable">Step 1: Export KubeConfig Variable</h3>

<p>Export your <a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#the-kubeconfig-environment-variable">KubeConfig</a>
variable, which is also used by the Packer plugin:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>~/.kube/config
</code></pre></div></div>

<p>This is required to communicate with your Kubernetes cluster.</p>

<h3 id="step-2-deploy-iso-datavolume">Step 2: Deploy ISO DataVolume</h3>

<p>Create a <a href="https://kubevirt.io/user-guide/storage/disks_and_volumes/#datavolume">DataVolume</a> to import the Fedora ISO into your cluster’s storage:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: fedora-42-x86-64-iso
  annotations:
    #
    # This annotation triggers immediate binding of the PVC,
    # speeding up provisioning.
    #
    cdi.kubevirt.io/storage.bind.immediate.requested: "true"
spec:
  source:
    http:
      #
      # Please check if this URL link is valid, in case the import fails.
      # If so, please modify the URL here below.
      #
      url: "https://download.fedoraproject.org/pub/fedora/linux/releases/42/Server/x86_64/iso/Fedora-Server-dvd-x86_64-42-1.1.iso"
  pvc:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 3Gi
</span><span class="no">EOF
</span></code></pre></div></div>

<h4 id="alternative-upload-a-local-iso">Alternative: Upload a local ISO</h4>

<p>Instead of importing from a URL, you can upload a local ISO
using the <a href="https://kubevirt.io/user-guide/user_workloads/virtctl_client_tool">virtctl</a> client tool:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virtctl image-upload dv fedora-42-x86-64-iso <span class="se">\</span>
  <span class="nt">--size</span><span class="o">=</span>3Gi <span class="se">\</span>
  <span class="nt">--image-path</span><span class="o">=</span>./Fedora-Server-dvd-x86_64-42-1.1.iso <span class="se">\</span>
</code></pre></div></div>

<p>The <a href="https://fedoraproject.org/server/download">Fedora Server 42 ISO</a> is available on Fedora’s official website.</p>

<h3 id="step-3-create-kickstart-file">Step 3: Create Kickstart File</h3>

<p>This <a href="https://en.wikipedia.org/wiki/Kickstart_(Linux)">Kickstart</a> file automates
Fedora installation, enabling unattended VM setup.</p>

<p>Create a file named <code class="language-plaintext highlighter-rouge">ks.cfg</code> with the following configuration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> ks.cfg <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
cdrom
text
firstboot --disable
lang en_US.UTF-8
keyboard us
timezone Europe/Paris --utc
selinux --enforcing
rootpw root
firewall --enabled --ssh
network --bootproto dhcp
user --groups=wheel --name=user --password=root --uid=1000 --gecos="user" --gid=1000

bootloader --location=mbr --append="net.ifnames=0 biosdevname=0 crashkernel=no"

zerombr
clearpart --all --initlabel
autopart --type=lvm

poweroff

%packages --excludedocs
@core
qemu-guest-agent
openssh-server
%end

%post
systemctl enable --now sshd
systemctl enable --now qemu-guest-agent
%end
</span><span class="no">EOF
</span></code></pre></div></div>

<p>This configuration enables SSH to provision the temporary VM, and <a href="https://qemu-project.gitlab.io/qemu/interop/qemu-ga.html">QEMU Guest Agent</a>
to have a better integration with KubeVirt itself.</p>

<h3 id="step-4-create-packer-template">Step 4: Create Packer Template</h3>

<p>Create an example of the Packer template (<code class="language-plaintext highlighter-rouge">fedora.pkr.hcl</code>):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> fedora.pkr.hcl <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
packer {
  required_plugins {
    kubevirt = {
      source  = "github.com/hashicorp/kubevirt"
      version = "&gt;= 0.8.0"
    }
  }
}

variable "kube_config" {
  type    = string
  default = "</span><span class="k">${</span><span class="nv">env</span><span class="p">(</span><span class="s2">"KUBECONFIG"</span><span class="p">)</span><span class="k">}</span><span class="sh">"
}

variable "namespace" {
  type    = string
  default = "vm-images"
}

variable "name" {
  type    = string
  default = "fedora-42-rand-85"
}

source "kubevirt-iso" "fedora" {
  # Kubernetes configuration
  kube_config   = var.kube_config
  name          = var.name
  namespace     = var.namespace

  # ISO configuration
  iso_volume_name = "fedora-42-x86-64-iso"

  # VM type and preferences
  disk_size          = "10Gi"
  instance_type      = "o1.medium"
  preference         = "fedora"
  os_type            = "linux"

  # Default network configuration
  networks {
    name = "default"

    pod {}
  }

  # Files to include in the ISO installation
  media_files = [
    "./ks.cfg"
  ]

  # Boot process configuration
  # A set of commands to send over VNC connection
  boot_command = [
    "&lt;up&gt;e",                            # Modify GRUB entry
    "&lt;down&gt;&lt;down&gt;&lt;end&gt;",                # Navigate to kernel line
    " inst.ks=hd:LABEL=OEMDRV:/ks.cfg", # Set kickstart file location
    "&lt;leftCtrlOn&gt;x&lt;leftCtrlOff&gt;"        # Boot with modified command line
  ]
  boot_wait                 = "10s"     # Time to wait after boot starts
  installation_wait_timeout = "15m"     # Timeout for installation to complete

  # SSH configuration
  communicator      = "ssh"
  ssh_host          = "127.0.0.1"
  ssh_local_port    = 2020
  ssh_remote_port   = 22
  ssh_username      = "user"
  ssh_password      = "root"
  ssh_wait_timeout  = "20m"
}

build {
  sources = ["source.kubevirt-iso.fedora"]

  provisioner "shell" {
    inline = [
      "echo 'Install packages, configure services, or tweak system settings here.'",
    ]
  }
}
</span><span class="no">EOF
</span></code></pre></div></div>

<h3 id="step-5-export-vm-image-optional">Step 5: Export VM Image (Optional)</h3>

<p>Optionally, export the newly created disk image and package it
into a <a href="https://kubevirt.io/user-guide/storage/disks_and_volumes/#containerdisk">containerDisk</a>
so it can be shared across multiple Kubernetes clusters.</p>

<h4 id="required-dependencies">Required Dependencies</h4>

<p>Install these tools on the machine running Packer:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">virtctl</code>: Exports the VM image from the KubeVirt cluster.</li>
  <li><code class="language-plaintext highlighter-rouge">qemu-img</code>: Converts raw images to qcow2 format.</li>
  <li><code class="language-plaintext highlighter-rouge">gunzip</code>: Decompresses exported VM images.</li>
  <li><code class="language-plaintext highlighter-rouge">podman</code>: Builds and pushes container images.</li>
</ul>

<h4 id="example">Example</h4>

<p>Add a <code class="language-plaintext highlighter-rouge">shell-local</code> post-processor to the Packer build, which runs after the build is completed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>variable <span class="s2">"registry"</span> <span class="o">{</span>
  <span class="nb">type</span>    <span class="o">=</span> string
  default <span class="o">=</span> <span class="s2">"quay.io/containerdisks"</span>
<span class="o">}</span>

variable <span class="s2">"registry_username"</span> <span class="o">{</span>
  <span class="nb">type</span>      <span class="o">=</span> string
  sensitive <span class="o">=</span> <span class="nb">true</span>
<span class="o">}</span>

variable <span class="s2">"registry_password"</span> <span class="o">{</span>
  <span class="nb">type</span>      <span class="o">=</span> string
  sensitive <span class="o">=</span> <span class="nb">true</span>
<span class="o">}</span>

variable <span class="s2">"image_tag"</span> <span class="o">{</span>
  <span class="nb">type</span>    <span class="o">=</span> string
  default <span class="o">=</span> <span class="s2">"latest"</span>
<span class="o">}</span>

build <span class="o">{</span>
  ...

  post-processor <span class="s2">"shell-local"</span> <span class="o">{</span>
    inline <span class="o">=</span> <span class="o">[</span>
      <span class="c"># Export VM disk image from PVC</span>
      <span class="s2">"virtctl -n </span><span class="k">${</span><span class="nv">var</span><span class="p">.namespace</span><span class="k">}</span><span class="s2"> vmexport download </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">-export --pvc=</span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2"> --output=</span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.img.gz"</span>,

      <span class="c"># Decompress exported VM image</span>
      <span class="s2">"gunzip -k </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.img.gz"</span>,

      <span class="c"># Convert raw image to qcow2 (smaller and more efficient format)</span>
      <span class="s2">"qemu-img convert -c -O qcow2 </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.img </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.qcow2"</span>,

      <span class="c"># Generate Containerfile</span>
      <span class="s2">"echo 'FROM scratch' &gt; </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.Containerfile"</span>,
      <span class="s2">"echo 'COPY </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.qcow2 /disk/' &gt;&gt; </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.Containerfile"</span>,

      <span class="c"># Login to registry</span>
      <span class="s2">"podman login -u </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry_username</span><span class="k">}</span><span class="s2"> -p </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry_password</span><span class="k">}</span><span class="s2"> </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry</span><span class="k">}</span><span class="s2">"</span>,

      <span class="c"># Build and push image</span>
      <span class="s2">"podman build -t </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">var</span><span class="p">.image_tag</span><span class="k">}</span><span class="s2"> -f </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.Containerfile ."</span>,
      <span class="s2">"podman push </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">var</span><span class="p">.image_tag</span><span class="k">}</span><span class="s2">"</span>
    <span class="o">]</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Sensitive credentials such as registry usernames and passwords should never be hardcoded in templates.</p>

<h3 id="step-6-initialize-packer-plugin">Step 6: Initialize Packer Plugin</h3>

<p>Run the following command once to install the Packer plugin:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>packer init fedora.pkr.hcl
</code></pre></div></div>

<p>This downloads and sets up the KubeVirt plugin automatically.</p>

<h3 id="step-7-run-packer-build">Step 7: Run Packer Build</h3>

<p>Finally, run a build to create a new VM golden image with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>packer build fedora.pkr.hcl
</code></pre></div></div>

<p>Packer will create a new VM golden image in your Kubernetes cluster.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this walkthrough, you built a Fedora VM golden image inside Kubernetes
using Packer and the KubeVirt plugin. You defined an ISO source, automated
installation with Kickstart configuration and provisioned the VM over SSH
— all within your Kubernetes cluster.</p>

<p>From here, you can:</p>

<ul>
  <li>Reuse the bootable volume to launch new VMs instantly.</li>
  <li>Integrate Packer builds into your CI/CD pipelines.</li>
  <li>Adapt the same process to build images for other operating systems, such as <a href="https://github.com/hashicorp/packer-plugin-kubevirt/tree/main/examples/builder/kubevirt-iso/rhel">RHEL</a> and <a href="https://github.com/hashicorp/packer-plugin-kubevirt/tree/main/examples/builder/kubevirt-iso/windows">Windows</a>.</li>
</ul>

<p>The plugin is still in pre-release, but it already offers a streamlined way
to create consistent VM images inside Kubernetes.</p>

<p>Give it a try and share your feedback or contributions on <a href="https://github.com/hashicorp/packer-plugin-kubevirt">GitHub</a>!</p>]]></content><author><name>Ben Oukhanov</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="packer" /><category term="plugin" /><category term="images" /><summary type="html"><![CDATA[Packer plugin for KubeVirt that builds VM golden images inside Kubernetes.]]></summary></entry><entry><title type="html">KubeVirt v1.6.0</title><link href="https://kubevirt.io//2025/changelog-v1.6.0.html" rel="alternate" type="text/html" title="KubeVirt v1.6.0" /><published>2025-07-30T00:00:00+00:00</published><updated>2025-07-30T00:00:00+00:00</updated><id>https://kubevirt.io//2025/changelog-v1.6.0</id><content type="html" xml:base="https://kubevirt.io//2025/changelog-v1.6.0.html"><![CDATA[<h2 id="v160">v1.6.0</h2>

<p>Released on: Wed Jul 30 18:23:13 2025 +0000</p>

<ul>
  <li>[PR #15264][fossedihelm] Quarantined <code class="language-plaintext highlighter-rouge">should live migrate a container disk vm, with an additional PVC mounted, should stay mounted after migration</code> test</li>
  <li>[PR #15256][kubevirt-bot] Bumped the bundled common-instancetypes to v1.4.0 which add new preferences.</li>
  <li>[PR #15114][kubevirt-bot] Bugfix: Label upload PVCs to support CDI WebhookPvcRendering</li>
  <li>[PR #15214][dhiller] Quarantine flaky test <code class="language-plaintext highlighter-rouge">[sig-compute]VM state with persistent TPM VM option enabled should persist VM state of EFI across migration and restart</code></li>
  <li>[PR #15191][kubevirt-bot] Drop an arbitrary limitation on VM’s domain.firmaware.serial. Any string is passed verbatim to smbios. Illegal may be tweaked or ignored based on qemu/smbios version.</li>
  <li>[PR #15202][kubevirt-bot] BugFix: export fails when VMExport has dots in secret</li>
  <li>[PR #15201][xpivarc] Known issue: ParallelOutboundMigrationsPerNode might be ignored because of race condition</li>
  <li>[PR #15178][kubevirt-bot] Fix postcopy multifd compatibility during upgrade</li>
  <li>[PR #15171][kubevirt-bot] BugFix: export fails when VMExport has dots in name</li>
  <li>[PR #15102][dominikholler] Update dependecy golang.org/x/net to v0.38.0</li>
  <li>[PR #15101][dominikholler] Update dependecy golang.org/x/oauth2 to v0.27.0</li>
  <li>[PR #15080][kubevirt-bot] The synchronization controller migration network IP address is advertised by the KubeVirt CR</li>
  <li>[PR #15047][kubevirt-bot] Beta: NodeRestriction</li>
  <li>[PR #15039][alaypatel07] Add support for DRA devices such as GPUs and HostDevices.</li>
  <li>[PR #15020][kubevirt-bot] Possible to trust additional CAs for verifying kubevirt infra structure components</li>
  <li>[PR #15014][kubevirt-bot] Support seamless TCP migration with passt (alpha)</li>
  <li>[PR #14887][oshoval] Release passt CNI image, instead the CNI binary itself.</li>
  <li>[PR #13898][brandboat] Changed the time unit conversion in the kubevirt_vmi_vcpu_seconds_total metric from microseconds to nanoseconds.</li>
  <li>[PR #14935][alromeros] Add virtctl objectgraph command</li>
  <li>[PR #14744][tiraboschi] A few dynamic annotations are synced from VMs template to VMIs and to virt-launcher pods</li>
  <li>[PR #14907][mhenriks] Allow virtio bus for hotplugged disks</li>
  <li>[PR #14754][mhenriks] Allocate more PCI ports for hotplug</li>
  <li>[PR #13103][varunrsekar] Feature: Support for defining panic devices in VirtualMachineInstances to catch crash signals from the guest.</li>
  <li>[PR #14961][akalenyu] BugFix: Can’t LiveMigrate Windows VM after Storage Migration from HPP to OCS</li>
  <li>[PR #14956][RobertoMachorro] Added CRC to ADOPTERS document.</li>
  <li>[PR #14705][jean-edouard] The migration controller in virt-handler has been re-architected, migrations should be more stable</li>
  <li>[PR #13764][xpivarc] KubeVirt doesn’t use PDBs anymore</li>
  <li>[PR #14801][arsiesys] VirtualMachinePool now supports a <code class="language-plaintext highlighter-rouge">.ScaleInStrategy.Proactive.SelectionPolicy.BasePolicy</code> field to control scale-down behavior. The new <code class="language-plaintext highlighter-rouge">"DescendingOrder"</code> strategy deletes VMs by descending ordinal index, offering predictable downscale behavior. Defaults to <code class="language-plaintext highlighter-rouge">"random"</code> if not specified.</li>
  <li>[PR #14259][orelmisan] Integrate NIC hotplug with LiveUpdate rollout strategy</li>
  <li>[PR #14673][dasionov] Add Video Configuration Field for VMs to Enable Explicit Video Device Selection</li>
  <li>[PR #14681][victortoso] Windows offline activation with ACPI MSDM table</li>
  <li>[PR #14723][SkalaNetworks] Add VolumeRestorePolicies and VolumeRestoreOverrides to VMRestores</li>
  <li>[PR #14040][jschintag] Add support for Secure Execution VMs on IBM Z</li>
  <li>[PR #13847][mhenriks] Declarative Volume Hotplug with CD-ROM Inject/Eject</li>
  <li>[PR #14807][alromeros] Add Object Graph subresource</li>
  <li>[PR #14793][jean-edouard] Failed post-copy migrations now always end in VMI failure</li>
  <li>[PR #14632][iholder101] virt-handler: Reduce Get() calls for KSM handling</li>
  <li>[PR #14658][alromeros] Bugfix: Update backend-storage logic so it works with PVCs with non-standard naming convention</li>
  <li>[PR #14827][orelmisan] Fix network setup when emulation is enabled</li>
  <li>[PR #14538][iholder101] Move cgroup v1 to maintenance mode</li>
  <li>[PR #14823][xmulligan] Adding Isovalent to Adopters</li>
  <li>[PR #14805][machadovilaca] Replace metric labels’ none values with empty values</li>
  <li>[PR #14768][oshoval] Expose CONTAINER_NAME on hook sidecars.</li>
  <li>[PR #14183][aqilbeig] Add maxUnavailable support to VirtualMachinePool</li>
  <li>[PR #14695][alromeros] Bugfix: Fix online expansion by requeuing VMIs on PVC size change</li>
  <li>[PR #14738][oshoval] Clean absent interfaces and their relative networks from stopped VMs.</li>
  <li>[PR #14737][ShellyKa13] virt-Freeze: skip freeze if domain is not in running state</li>
  <li>[PR #14728][orelmisan] CPU hotplug with net multi-queue is now allowed</li>
  <li>[PR #14616][awels] VirtualMachineInstanceMigrations can now express that they are source or target migrations</li>
  <li>[PR #14619][cloud-j-luna] virtctl vnc command now supports user provided VNC clients.</li>
  <li>[PR #14130][dasionov] bug-fix: persist VM’s firmware UUID for existing VMs</li>
  <li>[PR #14640][xpivarc] ARM: CPU pinning doesn’t panic now</li>
  <li>[PR #14664][brianmcarey] Build KubeVirt with go v1.23.9</li>
  <li>[PR #14599][HarshithaMS005] Enabled watchdog validation on watchdog device models</li>
  <li>[PR #13806][iholder101] Dirty rate is reported as part of a new <code class="language-plaintext highlighter-rouge">GetDomainDirtyRateStats()</code> gRPC method and by a Prometheus metric: <code class="language-plaintext highlighter-rouge">kubevirt_vmi_dirty_rate_bytes_per_second</code>.</li>
  <li>[PR #14617][SkalaNetworks] Added support for custom JSON patches in VirtualMachineClones.</li>
  <li>[PR #14637][alromeros] Label backend PVC to support CDI WebhookPvcRendering</li>
  <li>[PR #14440][pstaniec-catalogicsoftware] add CloudCasa by Catalogic to integrations in the adopters.md</li>
  <li>[PR #14602][orelmisan] The “RestartRequired” condition is not set on VM objects for live-updatable network fields</li>
  <li>[PR #14267][Barakmor1] Implement container disk functionality using ImageVolume, protected by the ImageVolume feature gate.</li>
  <li>[PR #14539][nirdothan] Enable vhost-user mode for passt network binding plugin</li>
  <li>[PR #14520][dasionov] Enable node-labeller for ARM64 clusters, supporting machine-type labels.</li>
  <li>[PR #14203][machadovilaca] Trigger VMCannotBeEvicted only for running VMIs</li>
  <li>[PR #14449][0xFelix] The 64-Bit PCI hole can now be disabled by adding the kubevirt.io/disablePCIHole annotation to VirtualMachineInstances. This allows legacy OSes such as Windows XP or Server 2003 to boot on KubeVirt using the Q35 machine type.</li>
  <li>[PR #13297][mhenriks] hotplug volume: Boot from hotpluggable disk</li>
  <li>[PR #14509][phoracek] Network conformance tests are now marked using the <code class="language-plaintext highlighter-rouge">Conformance</code> decorator. Use <code class="language-plaintext highlighter-rouge">--ginkgo.label-filter '(sig-network &amp;&amp; conformance)</code> to select them.</li>
  <li>[PR #14338][dasionov] Bug fix: MaxSockets is limited so maximum of vcpus doesn’t go over 512.</li>
  <li>[PR #14327][machadovilaca] Handle lowercase instancetypes/preference keys in VM monitoring</li>
  <li>[PR #14437][jschintag] Ensure stricter check for valid machine type when validating VMI</li>
  <li>[PR #13911][avlitman] VirtHandlerRESTErrorsHigh, VirtOperatorRESTErrorsHigh, VirtAPIRESTErrorsHigh and VirtControllerRESTErrorsHigh alerts removed.</li>
  <li>[PR #14277][HarshithaMS005] Enable Watchdog device support on s390x using the Diag288 device model.</li>
  <li>[PR #13422][mhenriks] guest console log: make virt-tail a proper sidecar</li>
  <li>[PR #14426][avlitman] Added kubevirt_vmi_migrations_in_unset_phase, instead of including it in kubevirt_vmi_migration_failed.</li>
  <li>[PR #14428][jean-edouard] To use nfs-csi, the env variable KUBEVIRT_NFS_DIR has to be set to a location on the host for NFS data</li>
  <li>[PR #13951][alromeros] Bugfix: Truncate volume names in export pod</li>
  <li>[PR #14405][jpeimer] supplementalPool added to the description of the ioThreadsPolicy possible values</li>
  <li>[PR #14145][ayushpatil2122] handle nil pointer dereference in cellToCell</li>
  <li>[PR #14281][ShellyKa13] VMRestore: Keep VM RunStrategy as before the restore</li>
  <li>[PR #14374][kubevirt-bot] Updated common-instancetypes bundles to v1.3.1</li>
  <li>[PR #14219][lyarwood] A request to create a VirtualMachines that references a non-existent  instance type or preference are no longer rejected. The VirtualMachine will be created but will fail to start until the missing resources are created in the cluster.</li>
  <li>[PR #14288][qinqon] Don’t expose as VMI status the implicit qemu domain pause at the end of live migration</li>
  <li>[PR #14309][alicefr] Fixed persistent reservation support for multipathd by improving socket access and multipath files in pr-helper</li>
  <li>[PR #14325][vamsikrishna-siddu] fix: disks-images-provider is pointing to wrong alpine image for s390x.</li>
  <li>[PR #14048][lyarwood] The <code class="language-plaintext highlighter-rouge">v1alpha{1,2}</code> versions of the <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io</code> API group are no longer served or supported.</li>
  <li>[PR #14316][lyarwood] A new <code class="language-plaintext highlighter-rouge">Enabled</code> attribute has been added to the <code class="language-plaintext highlighter-rouge">TPM</code> device allowing users to explicitly disable the device regardless of any referenced preference.</li>
  <li>[PR #14328][akalenyu] Cleanup: Fix unit tests on a sane, non-host-cgroup-sharing development setup</li>
  <li>[PR #14108][machadovilaca] Add interface name label to kubevirt_vmi_status_addresses</li>
  <li>[PR #14050][lyarwood] The <code class="language-plaintext highlighter-rouge">InstancetypeReferencePolicy</code> feature has graduated to GA and no longer requires the associated feature gate to be enabled.</li>
  <li>[PR #14286][machadovilaca] Register k8s client-go latency metrics on init</li>
  <li>[PR #14304][jean-edouard] Update module github.com/containers/common to v0.60.4</li>
  <li>[PR #14065][jean-edouard] VM Persistent State GA</li>
  <li>[PR #14096][ShellyKa13] VMSnapshot: add QuiesceFailed indication to snapshot if freeze failed</li>
  <li>[PR #14215][dominikholler] Update module golang.org/x/oauth2 to v0.27.0</li>
  <li>[PR #14068][jean-edouard] Default VM Rollout Strategy is now LiveUpdate. Important: to preserve previous behavior, rolloutStrategy needs to be set to “Stage” in the KubeVirt CR.</li>
  <li>[PR #14222][dominikholler] Update module golang.org/x/net to v0.36.0</li>
  <li>[PR #14218][dominikholler] Update golang.org/x/crypto to v0.35.0</li>
  <li>[PR #14217][dominikholler] Update module github.com/opencontainers/runc to v1.1.14</li>
  <li>[PR #14141][jean-edouard] Large number of migrations should no longer lead to active migrations timing out</li>
  <li>[PR #13870][dasionov] Ensure launcher pods are finalized and deleted before removing the VMI finalizer when the VMI is marked for deletion.</li>
  <li>[PR #14101][qinqon] libvirt: 10.10.0-7, qemu: 9.1.0-15</li>
  <li>[PR #14071][alicefr] Add entrypoint to the pr-helper for creating the symlink to the multipath socket</li>
  <li>[PR #12725][tiraboschi] Support live migration to a named node</li>
  <li>[PR #13888][Sreeja1725] Add v1.5.0 perf and scale benchmarks data</li>
  <li>[PR #13939][0xFelix] The virtctl port-forward/ssh/scp syntax was changed to type/name[/namespace]. It now supports resources with dots in their name properly.</li>
  <li>[PR #13807][Barakmor1] virt-launcher now uses bash to retrieve disk info and verify container-disk files, requiring bash to be included in the launcher image</li>
  <li>[PR #13744][nirdothan] Network interfaces state can be set to <code class="language-plaintext highlighter-rouge">down</code> or <code class="language-plaintext highlighter-rouge">up</code> in order to set the link state accordingly when VM is running. Hot plugging of interface in these states is also supported.</li>
  <li>[PR #13536][jean-edouard] Interrupted migrations will now be reconciled on next VM start.</li>
  <li>[PR #13690][dasionov] bug-fix: add machine type to <code class="language-plaintext highlighter-rouge">NodeSelector</code> to prevent breaking changes on unsupported nodes</li>
  <li>[PR #13940][tiraboschi] The node-restriction Validating Admission Policy will return consistent reasons on failures</li>
  <li>[PR #13916][lyarwood] Instance type and preference runtime data is now stored under <code class="language-plaintext highlighter-rouge">Status.{Instancetype,Preference}Ref</code> and is no longer mutated into the core VirtualMachine<code class="language-plaintext highlighter-rouge"> </code>Spec`.</li>
  <li>[PR #13831][ShellyKa13] VMClone: Remove webhook that checks Snapshot Source</li>
  <li>[PR #13815][acardace] GA ClusterProfiler FG and add a config to enable it</li>
  <li>[PR #13928][kubevirt-bot] Updated common-instancetypes bundles to v1.3.0</li>
  <li>[PR #13805][machadovilaca] Fetch non-cluster instance type and preferences with namespace key</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.6.0 changes]]></summary></entry><entry><title type="html">KubeVirt v1.5.0</title><link href="https://kubevirt.io//2025/changelog-v1.5.0.html" rel="alternate" type="text/html" title="KubeVirt v1.5.0" /><published>2025-03-13T00:00:00+00:00</published><updated>2025-03-13T00:00:00+00:00</updated><id>https://kubevirt.io//2025/changelog-v1.5.0</id><content type="html" xml:base="https://kubevirt.io//2025/changelog-v1.5.0.html"><![CDATA[<h2 id="v150">v1.5.0</h2>

<p>Released on: Thu Mar 13 18:01:18 2025 +0000</p>

<ul>
  <li>[PR #14200][kubevirt-bot] Fetch non-cluster instance type and preferences with namespace key</li>
  <li>[PR #14125][kubevirt-bot] Add entrypoint to the pr-helper for creating the symlink to the multipath socket</li>
  <li>[PR #13942][kubevirt-bot] Instance type and preference runtime data is now stored under <code class="language-plaintext highlighter-rouge">Status.{Instancetype,Preference}Ref</code> and is no longer mutated into the core VirtualMachine<code class="language-plaintext highlighter-rouge"> </code>Spec`.</li>
  <li>[PR #13988][kubevirt-bot] Network interfaces state can be set to <code class="language-plaintext highlighter-rouge">down</code> or <code class="language-plaintext highlighter-rouge">up</code> in order to set the link state accordingly when VM is running. Hot plugging of interface in these states is also supported.</li>
  <li>[PR #13985][kubevirt-bot] Interrupted migrations will now be reconciled on next VM start.</li>
  <li>[PR #13936][kubevirt-bot] Updated common-instancetypes bundles to v1.3.0</li>
  <li>[PR #13871][0xFelix] By default the local SSH client on the machine running <code class="language-plaintext highlighter-rouge">virtctl ssh</code> is now used. The <code class="language-plaintext highlighter-rouge">--local-ssh</code> flag is now deprecated.</li>
  <li>[PR #11964][ShellyKa13] VMClone: Remove webhook that checks VM Source</li>
  <li>[PR #13918][0xFelix] <code class="language-plaintext highlighter-rouge">type</code> being optional in the syntax of virtctl port-forward/ssh/scp is now deprecated.</li>
  <li>[PR #13838][iholder101] Add the KeepValueUpdated() method to time-defined cache</li>
  <li>[PR #13857][ShellyKa13] VMSnapshot: allow creating snapshot when source doesnt exist yet</li>
  <li>[PR #13864][alromeros] Reject VM clone when source uses backend storage PVC</li>
  <li>[PR #13850][nirdothan] Network interfaces state can be set to <code class="language-plaintext highlighter-rouge">down</code> or <code class="language-plaintext highlighter-rouge">up</code> in order to set the link state accordingly.</li>
  <li>[PR #13803][ShellyKa13] BugFix: VMSnapshot: wait for volumes to be bound instead of skip</li>
  <li>[PR #13610][avlitman] Added kubevirt_vm_vnic_info and kubevirt_vmi_vnic_info metrics</li>
  <li>[PR #13642][0xFelix] VMs in a VMPool are able to receive individual configuration through individually indexed ConfigMaps and Secrets.</li>
  <li>[PR #12624][victortoso] Better handle unsupported volume type with Slic table</li>
  <li>[PR #13775][sbrivio-rh] This version of KubeVirt upgrades the passt package, providing user-mode networking, to match upstream version 2025_01_21.4f2c8e7.</li>
  <li>[PR #13717][alicefr] Refuse to volume migrate to legacy datavolumes using no-CSI storageclasses</li>
  <li>[PR #13208][davidvossel] Add VM reset functionality to virtctl and api</li>
  <li>[PR #13817][Barakmor1] The <code class="language-plaintext highlighter-rouge">AutoResourceLimits</code> feature gate is now deprecated with the feature state graduated to <code class="language-plaintext highlighter-rouge">GA</code> and thus enabled by default</li>
  <li>[PR #13756][germag] Live migration support for VMIs with (virtiofs) filesystem devices</li>
  <li>[PR #13497][tiraboschi] As an hardening measure (principle of least privilege), the right of creating, editing and deleting <code class="language-plaintext highlighter-rouge">VirtualMachineInstanceMigrations</code> are not anymore assigned by default to namespace admins.</li>
  <li>[PR #13777][0xFelix] virtctl: VMs/VMIs with dots in their name are now supported in virtctl portforward, ssh and scp.</li>
  <li>[PR #13713][akalenyu] Enhancement: Declare to libvirt upfront which filesystems are shared to allow migration on some NFS backed provisioners</li>
  <li>[PR #13535][machadovilaca] Collect resource requests and limits from VM instance type/preference</li>
  <li>[PR #13708][orelmisan] Network interfaces’ link state will be reported for interfaces present in VMI spec</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[PR #13428][machadovilaca] Add kubevirt_vmi_migration_(start</td>
          <td>end)_time_seconds metrics</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>[PR #11266][jean-edouard] KubeVirt will no longer deploy a custom SELinux policy on worker nodes</li>
  <li>[PR #13423][machadovilaca] Add kubevirt_vmi_migration_data_total_bytes metric</li>
  <li>[PR #13699][brianmcarey] Build KubeVirt with go v1.23.4</li>
  <li>[PR #13711][ShellyKa13] VMSnapshot: honor StorageProfile snapshotClass when choosing volumesnapshotclass</li>
  <li>[PR #13667][arnongilboa] Set VM status indication if storage exceeds quota</li>
  <li>[PR #13288][alicefr] Graduation of VolumeUpdateStrategy and VolumeMigration feature gates</li>
  <li>[PR #13520][iholder101] Graduate the clone API to v1beta1 and deprecate v1alpha1</li>
  <li>[PR #11997][jcanocan] Drop <code class="language-plaintext highlighter-rouge">ExperimentalVirtiofsSupport</code> feature gate in favor of <code class="language-plaintext highlighter-rouge">EnableVirtioFsConfigVolumes</code> for sharing ConfigMaps, Secrets, DownwardAPI and ServiceAccounts and <code class="language-plaintext highlighter-rouge">EnableVirtioFsPVC</code> for sharing PVCs.</li>
  <li>[PR #13641][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 10.10.0 and QEMU 9.1.0.</li>
  <li>[PR #13682][alromeros] Bugfix: Support online snapshot of VMs with backend storage</li>
  <li>[PR #13207][alromeros] Bugfix: Support offline snapshot of VMs with backend storage</li>
  <li>[PR #13587][sradco] Alert KubevirtVmHighMemoryUsage has been deprecated.</li>
  <li>[PR #13109][xpivarc] Test suite: 3 new labels are available to filter tests: HostDiskGate, requireHugepages1Gi, blockrwo</li>
  <li>[PR #13110][alicefr] Add the iothreads option to specify number of iothreads to be used</li>
  <li>[PR #13586][akalenyu] storage tests: assemble storage-oriented conformance test suite</li>
  <li>[PR #13606][dasionov] add support for virtio video device for amd64</li>
  <li>[PR #13603][akalenyu] Storage tests: eliminate runtime skips</li>
  <li>[PR #13546][akalenyu] BugFix: Volume hotplug broken with crun &gt;= 1.18</li>
  <li>[PR #13588][Yu-Jack] Ensure virt-tail and virt-monitor have the same timeout, preventing early termination of virt-tail while virt-monitor is still starting</li>
  <li>[PR #13545][alicefr] Upgrade of virt stack</li>
  <li>[PR #13152][akalenyu] VMExport: exported DV uses the storage API</li>
  <li>[PR #13562][kubevirt-bot] Updated common-instancetypes bundles to v1.2.1</li>
  <li>[PR #13496][0xFelix] virtctl expose now uses the unique <code class="language-plaintext highlighter-rouge">vm.kubevirt.io/name</code> label found on every virt-launcher Pod as a service selector.</li>
  <li>[PR #13547][0xFelix] virtctl create vm validates disk names and prevents disk names that will lead to rejection of a VM upon creation.</li>
  <li>[PR #13544][jean-edouard] Fixed bug where VMs may not get the persistent EFI they requested</li>
  <li>[PR #13431][avlitman] Add kubevirt_vm_create_date_timestamp_seconds metric</li>
  <li>[PR #13460][alromeros] Bugfix: Support exporting backend PVC</li>
  <li>[PR #13495][brianmcarey] Build KubeVirt with go v1.22.10</li>
  <li>[PR #13437][arnongilboa] Remove deprecated DataVolume garbage collection tests</li>
  <li>[PR #13386][machadovilaca] Ensure IP not empty in kubevirt_vmi_status_addresses metric</li>
  <li>[PR #13424][fossedihelm] Bugfix: fix possible virt-handler race condition and stuck situation during shutdown</li>
  <li>[PR #13458][orelmisan] Adjust managedTap binding to work with VMs with Address Conflict Detection enabled</li>
  <li>[PR #13250][Sreeja1725] Add virt-handler cpu and memory usage metrics</li>
  <li>[PR #13263][jean-edouard] /var/lib/kubelet on the nodes can now be a symlink</li>
  <li>[PR #12705][iholder101] Auto-configured parallel QEMU-level migration threads (a.k.a. multifd)</li>
  <li>[PR #13426][dasionov] bug-fix: prevent status update for old migrations</li>
  <li>[PR #13252][iholder101] Unconditionally disable libvirt’s VMPort feature which is relevant for VMWare only</li>
  <li>[PR #13305][ShellyKa13] VMRestore: remove VMSnapshot logic from vmrestore webhook</li>
  <li>[PR #13367][xpivarc] Bug-fix: Reduced probability of false “failed to detect socket for containerDisk disk0: … connection refused” warnings</li>
  <li>[PR #13243][orelmisan] Dynamic pod interface naming is declared GA</li>
  <li>[PR #13314][EdDev] Network Binding Plugin feature is declared GA</li>
  <li>[PR #13325][machadovilaca] Add node label to migration metrics</li>
  <li>[PR #13294][machadovilaca] Add Guest and Hugepages memory to kubevirt_vm_resource_requests</li>
  <li>[PR #13195][ShellyKa13] Vmrestore - add options to handle cases when target is not ready</li>
  <li>[PR #13138][mhenriks] Avoid NPE when getting filesystem overhead</li>
  <li>[PR #13270][ShellyKa13] VMSnapshot: propagate freeze error failure</li>
  <li>[PR #13148][avlitman] added a new label to kubevirt_vmi_info metric named vmi_pod and contain the current pod name that runs the VMI.</li>
  <li>[PR #12800][alicefr] Enable volume migration for hotplugged volumes</li>
  <li>[PR #12925][0xFelix] virtctl: Image uploads are retried up to 15 times</li>
  <li>[PR #13260][akalenyu] BugFix: VMSnapshot ‘InProgress’ and Failing for a VM with InstanceType and Preference</li>
  <li>[PR #13240][awels] Fix issue starting Virtual Machine Export when succeed/failed VMI exists for that VM</li>
  <li>[PR #12750][lyarwood] The inflexible <code class="language-plaintext highlighter-rouge">PreferredUseEFi</code> and <code class="language-plaintext highlighter-rouge">PreferredUseSecureBoot</code> preference fields have been deprecated ahead of removal in a future version of the <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io</code> API. Users should instead use <code class="language-plaintext highlighter-rouge">PreferredEfi</code> to provide a preferred <code class="language-plaintext highlighter-rouge">EFI</code> configuration for their <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</li>
  <li>[PR #13219][jean-edouard] backend-storage will now correctly use the default virtualization storage class</li>
  <li>[PR #13204][Sreeja1725] Add release v1.4.0 perf and scale benchmarks data</li>
  <li>[PR #13197][akalenyu] BugFix: VMSnapshots broken on OpenShift</li>
  <li>[PR #12765][avlitman] kubevirt_vm_disk_allocated_size_bytes metric added in order to monitor vm sizes</li>
  <li>[PR #12546][Sreeja1725] Update promql query of cpu and memory metrics for sig-performance tests</li>
  <li>[PR #12844][jschintag] Enable virt-exportproxy and virt-exportserver image for s390x</li>
  <li>[PR #12628][ShellyKa13] VMs admitter: remove validation of vm clone volume from the webhook</li>
  <li>[PR #13006][chomatdam] Added labels, annotations to VM Export resources and configurable pod readiness timeout</li>
  <li>[PR #13091][acardace] GA the <code class="language-plaintext highlighter-rouge">VMLiveUpdateFeatures</code> feature-gate.</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.5.0 changes]]></summary></entry><entry><title type="html">Announcing the release of KubeVirt v1.5</title><link href="https://kubevirt.io//2025/KubeVirt-v1-5_release.html" rel="alternate" type="text/html" title="Announcing the release of KubeVirt v1.5" /><published>2025-03-05T00:00:00+00:00</published><updated>2025-03-05T00:00:00+00:00</updated><id>https://kubevirt.io//2025/KubeVirt-v1-5_release</id><content type="html" xml:base="https://kubevirt.io//2025/KubeVirt-v1-5_release.html"><![CDATA[<p>The KubeVirt Community is pleased to announce the release of <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.5.0">KubeVirt v1.5</a>. This release aligns with <a href="https://kubernetes.io/blog/2024/12/11/kubernetes-v1-32-release/">Kubernetes v1.32</a> and is the seventh KubeVirt release to follow the Kubernetes release cadence.</p>

<p>This release sees the project adding some features that are aligned with more traditional virtualization platforms, such as enhanced volume and VM migration, increased CPU performance, and more precise network state control.</p>

<p>You can read the full <a href="https://kubevirt.io/user-guide/release_notes/#v150">release notes</a> in our user-guide, but we have included some highlights in this blog.</p>

<h3 id="breaking-change">Breaking change</h3>
<p>Please be aware that in v1.5 we have <a href="https://github.com/kubevirt/kubevirt/pull/13497">introduced a change</a> that affects permissions of namespace admins to trigger live migrations. As a hardening measure (principle of least privilege), the right of creating, editing and deleting <code class="language-plaintext highlighter-rouge">VirtualMachineInstanceMigrations</code> are no longer assigned by default to namespace admins.</p>

<p>For more information, see our post on the <a href="https://kubevirt.io/2025/Hardening-VMIM.html">KubeVirt blog</a>.</p>

<h3 id="feature-ga">Feature GA</h3>

<p>This release marks the graduation of a number of features to GA; deprecating the feature gate and now enabled by default:</p>

<ul>
  <li><a href="https://kubevirt.io/user-guide/storage/volume_migration/">Migration Update Strategy and Volume Migration</a>: Storage migration can be useful in the cases where the users need to change the underlying storage, for example, if the storage class has been deprecated, or there is a new more performant driver available.</li>
  <li><a href="https://kubevirt.io/user-guide/compute/resources_requests_and_limits/">Auto Resource Limits</a>: Automatically apply CPU limits to a VMI.</li>
  <li>VM Live Update Features: This feature underpins hotplugging of CPU, memory, and volume resources.</li>
  <li><a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network Binding Plugin</a>: A modular plugin which integrates with KubeVirt to implement a network binding.</li>
</ul>

<h3 id="compute">Compute</h3>

<p>You can now specify the number of <a href="https://kubevirt.io/user-guide/storage/disks_and_volumes/#iothreads">IOThreads to use</a> through virtqueue mapping to improve CPU performance. We also added <a href="https://github.com/kubevirt/kubevirt/pull/13606">virtio video support for amd64</a> as well as the <a href="https://github.com/kubevirt/kubevirt/pull/13208">ability to reset VMs</a>, which provides the means to restart the guest OS without requiring a new pod to be scheduled.</p>

<h3 id="networking">Networking</h3>

<p>You can now <a href="https://kubevirt.io/user-guide/network/interfaces_and_networks/#link-state-management">dynamically control the link state</a> (up/down) of a network interface.</p>

<h3 id="scale-and-performance">Scale and Performance</h3>

<p>A comprehensive list of performance and scale benchmarks for the release is <a href="https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md">available here</a>. A notable change added to the benchmarks was the <a href="https://github.com/kubevirt/kubevirt/pull/13250">virt-handler resource utilization metrics</a>. This metric gives the avg, max and min memory/cpu utilization per VMI that is scheduled on the node where virt-handler is running. Another notable shoutout from the benchmark document is changing how <a href="https://github.com/kubevirt/kubevirt/pull/12716">list calls are tracked</a>. KubeVirt clients were misreporting watch calls as list calls, which was fixed in this release.</p>

<h3 id="storage">Storage</h3>

<p>With this release you can now migrate <a href="https://kubevirt.io/user-guide/storage/volume_migration/">hotplugged volumes</a>. You can also migrate VMIs with a volume shared using virtiofs. And we <a href="https://github.com/kubevirt/kubevirt/pull/13713">addressed a recent change in libvirt</a> that was preventing some NFS shared volumes from migrating by providing shared filesystem paths upfront.</p>

<h3 id="thank-you-for-your-contribution">Thank you for your contribution!</h3>
<p>A lot of work from a <a href="https://kubevirt.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.4.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=All&amp;var-country_name=All&amp;var-companies=All">huge amount of people</a> goes into a release. A huge thank you to the 350+ people who contributed to this v1.5 release.</p>

<p>And if you’re interested in contributing to the project and being a part of the next release, please check out our <a href="https://kubevirt.io/user-guide/contributing/">contributing guide</a> and our <a href="https://github.com/kubevirt/community/blob/main/membership_policy.md">community membership guidelines</a>.</p>

<p>Contributing needn’t be designing a new feature or committing to a <a href="https://github.com/kubevirt/enhancements">Virtualization Enhancement Proposal</a>, there is always a need for reviews, help with our docs and website, or submitting good quality bugs. Every little bit counts.</p>]]></content><author><name>KubeVirt Maintainers</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.5" /><category term="release" /><category term="community" /><category term="cncf" /><category term="milestone" /><category term="party time" /><summary type="html"><![CDATA[With the release of KubeVirt v1.5 we see the community adding some features that align with more traditional virtualization platforms.]]></summary></entry><entry><title type="html">VirtualMachineInstanceMigrations RBAC hardening</title><link href="https://kubevirt.io//2025/Hardening-VMIM.html" rel="alternate" type="text/html" title="VirtualMachineInstanceMigrations RBAC hardening" /><published>2025-02-26T00:00:00+00:00</published><updated>2025-02-26T00:00:00+00:00</updated><id>https://kubevirt.io//2025/Hardening-VMIM</id><content type="html" xml:base="https://kubevirt.io//2025/Hardening-VMIM.html"><![CDATA[<h3 id="context">Context</h3>
<p>The request to live migrate a VM is represented by a <code class="language-plaintext highlighter-rouge">VirtualMachineInstanceMigration</code> instance.
A VirtualMachineInstanceMigration (VMIM) is a namespaced CRD, and its instances are expected to be in the namespace of the VM they refer to.</p>

<p>Up to KubeVirt v1.4, by default, a namespace admin (usually a namespace “owner” in a less formal definition) was able to create VMs and also VMIM objects to enqueue a live migration request for a VM within their namespace.
At the same time, live migrations can be triggered as part of critical infrastructure operations like node drains or upgrades, which are the domain of cluster admins. <br />
So, if namespace admins can continuously enqueue migration requests or delete scheduled VMIM objects needed for ongoing infrastructure-critical operations, they could delay or even prevent cluster-critical operations started by cluster admins, a role with greater privileges.</p>

<p>It was therefore possible that a malicious, lesser-privileged user could abuse this, causing a kind of DoS at the cluster level.
Even worse, Kubernetes RBAC permissions are purely additive (there are no “deny” rules), and KubeVirt roles are constantly reconciled by the virt-operator, so even a cluster admin who was aware of the issue was unable to deny these permissions as a precautionary measure.</p>

<p>For this reason, starting from KubeVirt v1.5, create/delete/update rights will no longer be granted by default to all namespace admins, in accordance with the principle of least privilege.
A new convenient ClusterRole named <code class="language-plaintext highlighter-rouge">kubevirt.io:migrate</code> has been introduced to allow cluster admins to easily grant this permission to selected users.</p>

<h3 id="side-effects-on-hotplug-operations">Side effects on hotplug operations</h3>
<p>Device hotplug operations, at least for CPU and memory, implicitly trigger a live migration executed by the virt-controller on behalf of the user. These operations will not be affected by this change. Under some circumstances or cluster configurations, live migrations are not automatically triggered when NIC devices are hotplugged. In such cases, the only option for namespace admins is to request VMIM permissions from a cluster admin to manually trigger the migration or concatenate two device hotplug operations (where the second one will implicitly complete the NIC hotplug).</p>

<h3 id="cluster-admin-tasks">Cluster-admin tasks</h3>
<p>A cluster admin can bind the new kubevirt.io:migrate ClusterRole to selected trusted users/groups at the namespace scope using:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-n</span> usernamespace rolebinding kvmigrate <span class="nt">--clusterrole</span><span class="o">=</span>kubevirt.io:migrate <span class="nt">--user</span><span class="o">=</span>user1 <span class="nt">--user</span><span class="o">=</span>user2 <span class="nt">--group</span><span class="o">=</span>group1
</code></pre></div></div>
<p>or at the cluster scope:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create clusterrolebinding kvmigrate <span class="nt">--clusterrole</span><span class="o">=</span>kubevirt.io:migrate <span class="nt">--user</span><span class="o">=</span>user1 <span class="nt">--user</span><span class="o">=</span>user2 <span class="nt">--group</span><span class="o">=</span>group1
</code></pre></div></div>

<p>A cluster admin can also restore the previous behavior (where all namespace admins are allowed to manage migrations) with:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl label <span class="nt">--overwrite</span> clusterrole kubevirt.io:migrate rbac.authorization.k8s.io/aggregate-to-admin<span class="o">=</span><span class="nb">true</span>
</code></pre></div></div>

<p>A highly cautious cluster admin who does not want any disruption due to the upgrade process could still create a temporary ClusterRole for migration before the upgrade, labeling it with <code class="language-plaintext highlighter-rouge">rbac.authorization.k8s.io/aggregate-to-admin=true</code>.
For example:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="s">rbac.authorization.k8s.io/aggregate-to-admin=true</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kubevirt.io:upgrademigrate</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">subresources.kubevirt.io</span>
  <span class="na">resources</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">virtualmachines/migrate</span>
  <span class="na">verbs</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">update</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">kubevirt.io</span>
  <span class="na">resources</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">virtualmachineinstancemigrations</span>
  <span class="na">verbs</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">get</span>
  <span class="pi">-</span> <span class="s">delete</span>
  <span class="pi">-</span> <span class="s">create</span>
  <span class="pi">-</span> <span class="s">update</span>
  <span class="pi">-</span> <span class="s">patch</span>
  <span class="pi">-</span> <span class="s">list</span>
  <span class="pi">-</span> <span class="s">watch</span>
  <span class="pi">-</span> <span class="s">deletecollection</span>
</code></pre></div></div>
<p>This ClusterRole will be aggregated into the <code class="language-plaintext highlighter-rouge">admin</code> role before the KubeVirt upgrade, and the upgrade process will not modify it, ensuring the previous behavior is maintained.
After the upgrade, the cluster admin will have sufficient time to bind the new <code class="language-plaintext highlighter-rouge">kubevirt.io:migrate</code> ClusterRole to selected users before removing the temporary ClusterRole.</p>]]></content><author><name>tiraboschi</name></author><category term="news" /><category term="VMIM" /><category term="migrate" /><category term="migrations" /><category term="RBAC" /><category term="hardening" /><category term="security" /><category term="v1.5" /><summary type="html"><![CDATA[Apply the principle of least privilege (PoLP) to VirtualMachineInstanceMigrations]]></summary></entry><entry><title type="html">KubeVirt v1.4.0</title><link href="https://kubevirt.io//2024/changelog-v1.4.0.html" rel="alternate" type="text/html" title="KubeVirt v1.4.0" /><published>2024-11-13T00:00:00+00:00</published><updated>2024-11-13T00:00:00+00:00</updated><id>https://kubevirt.io//2024/changelog-v1.4.0</id><content type="html" xml:base="https://kubevirt.io//2024/changelog-v1.4.0.html"><![CDATA[<h2 id="v140">v1.4.0</h2>

<p>Released on: Wed Nov 13 08:14:06 2024 +0000</p>

<ul>
  <li>[PR #13225][kubevirt-bot] backend-storage will now correctly use the default virtualization storage class</li>
  <li>[PR #13203][kubevirt-bot] BugFix: VMSnapshots broken on OpenShift</li>
  <li>[PR #13071][machadovilaca] Add kubevirt_vm_resource_limits metric</li>
  <li>[PR #13090][acardace] Allow live updating VMs’ tolerations</li>
  <li>[PR #12629][jean-edouard] backend-storage now supports RWO FS</li>
  <li>[PR #13086][lyarwood] A new <code class="language-plaintext highlighter-rouge">spec.configuration.instancetype.referencePolicy</code> configurable has been added to the <code class="language-plaintext highlighter-rouge">KubeVirt</code> CR with support for <code class="language-plaintext highlighter-rouge">reference</code> (default), <code class="language-plaintext highlighter-rouge">expand</code> and <code class="language-plaintext highlighter-rouge">expandAll</code> policies provided.</li>
  <li>[PR #13064][xpivarc] Fix cache corruption</li>
  <li>[PR #12967][xpivarc] BochsDisplayForEFIGuests is GAed, use  “kubevirt.io/vga-display-efi-x86” annotation on Kubevirt CR before upgrading in case you need retain compatibility.</li>
  <li>[PR #13078][qinqon] Add dynamic pod interface name feature gate</li>
  <li>[PR #13072][0xFelix] virtctl: virtctl create vm can now use the Access Credentials API to add credentials to a new VM</li>
  <li>[PR #13050][vamsikrishna-siddu] fix the cpu model issue for s390x.</li>
  <li>[PR #12802][machadovilaca] Add kubevirt_vmi_status_addresses metric</li>
  <li>[PR #13027][awels] BugFix: Stop creating tokenSecretRef when no volumes to export</li>
  <li>[PR #13001][awels] Relaxed check on modify VM spec during VM snapshot to only check disks/volumes</li>
  <li>[PR #13082][kubevirt-bot] Updated common-instancetypes bundles to v1.2.0</li>
  <li>[PR #12601][mhenriks] vmsnapshot: Enable status subresource for snapshot.kubevirt.io api group</li>
  <li>[PR #13018][orelmisan] Support Dynamic Primary Pod NIC Name</li>
  <li>[PR #13019][0xFelix] virtctl: The flags <code class="language-plaintext highlighter-rouge">--volume-clone-pvc</code>, <code class="language-plaintext highlighter-rouge">--volume-datasource</code> and <code class="language-plaintext highlighter-rouge">--volume-blank</code> are deprecated in favor of the <code class="language-plaintext highlighter-rouge">--volume-import</code> flag.</li>
  <li>[PR #13059][EdDev] Network hotplug feature is declared as GA.</li>
  <li>[PR #13024][EdDev] network binding plugin: Introduce a new <code class="language-plaintext highlighter-rouge">managedTap</code> <code class="language-plaintext highlighter-rouge">domainAttachmentType</code></li>
  <li>[PR #13060][EdDev] Network binding plugins feature is declared as Beta.</li>
  <li>[PR #13045][dasionov] Add ‘machine_type’ label for kubevirt_vm_info metric</li>
  <li>[PR #13030][alicefr] Removed the ManualRecoveryRequired field from the VolumeMigrationState and convert it to the VM condition ManualRecoveryRequired</li>
  <li>[PR #13053][0xFelix] virtctl: Users can specify a sysprep volume in VMs created with virtctl create vm</li>
  <li>[PR #12855][0xFelix] virtctl expose: Drop flag to set deprecated LoadBalancerIP option</li>
  <li>[PR #13008][0xFelix] virtctl: Allow creating a basic cloud-init config with virtctl create vm</li>
  <li>[PR #12829][0xFelix] fix: Proxies configured in kubeconfig are used in client-go for asynchronous subresources like VNC or Console</li>
  <li>[PR #12733][alromeros] Bugfix: Fix disk expansion logic by checking usable size instead of requested capacity</li>
  <li>[PR #13052][fossedihelm] Update code-generators to 1.31.1</li>
  <li>[PR #12882][brianmcarey] Build KubeVirt with go v1.22.8</li>
  <li>[PR #13040][awels] BugFix: Allow VMExport to work with VM columes that have dots in its name</li>
  <li>[PR #12729][fossedihelm] Update k8s dependencies to 0.31.0</li>
  <li>[PR #12867][jschintag] Fixed additional broken amd64 image in some image manifests</li>
  <li>[PR #12940][Barakmor1] Deprecate the DockerSELinuxMCS FeatureGate</li>
  <li>[PR #12943][Barakmor1] The <code class="language-plaintext highlighter-rouge">GPU</code> feature gate is now deprecated with the feature state graduated to <code class="language-plaintext highlighter-rouge">GA</code> and thus enabled by default</li>
  <li>[PR #12992][machadovilaca] Add a ‘outdated’ label to kubevirt_vmi_info metric</li>
  <li>[PR #12933][ShellyKa13] VM admitter: improve validation of vm spec datavolumetemplate</li>
  <li>[PR #12986][lyarwood] The <code class="language-plaintext highlighter-rouge">PreferredEfi</code> preference is now only applied when a user has not already enabled either <code class="language-plaintext highlighter-rouge">EFI</code> or <code class="language-plaintext highlighter-rouge">BIOS</code> within the underlying <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</li>
  <li>[PR #12117][Sreeja1725] Integrate kwok with sig-scale tests</li>
  <li>[PR #12716][Sreeja1725] Update kubevirt_rest_client_request_latency_seconds to count list calls if made using query params</li>
  <li>[PR #12578][dasionov] Mark Running field as deprecated</li>
  <li>[PR #12753][lyarwood] The <code class="language-plaintext highlighter-rouge">CommonInstancetypesDeploymentGate</code> feature gate and underlying feature are graduated to GA and now always enabled by default. A single new <code class="language-plaintext highlighter-rouge">KubeVirt</code> configurable is also introduced to allow cluster admins a way of explicitly disabling deployment when required.</li>
  <li>[PR #12645][avlitman] Add kubevirt_vmsnapshot_succeeded_timestamp_seconds metric</li>
  <li>[PR #11097][vamsikrishna-siddu] add s390x support for kubevirt builder</li>
  <li>[PR #12910][machadovilaca] Rename kubevirt_vm_resource_requests ‘vmi’ label to ‘name’</li>
  <li>[PR #12848][iholder101] Reduce default CompletionTimeoutPerGiB from 800s to 150s</li>
  <li>[PR #12861][ShellyKa13] bugfix: fix possible miss update of datavolumename on vmrestore restores</li>
  <li>[PR #12441][machadovilaca] Increase periodicity in domainstats migration metrics</li>
  <li>[PR #12718][machadovilaca] Add kubevirt_vm_info metric</li>
  <li>[PR #12599][xpivarc] MaxCpuSockets won’t block creation of VMs with more Sockets than MaxCpuSockets declare</li>
  <li>[PR #12857][akalenyu] BugFix: Fail to create VMExport via virtctl vmexport create</li>
  <li>[PR #12355][alicefr] Add the volume migration state in the VM status</li>
  <li>[PR #12726][awels] Concurrent addvolume/removevolume using virtctl no longer fail if concurrent modifications happen</li>
  <li>[PR #12835][ShellyKa13] bugfix: In case of err in vmrestore, leave VM without RestoreInProgress annotation allowing it to be started</li>
  <li>[PR #12809][dasionov] bug-fix: Ensure PDB associated with a VMI is deleted when it Reaches Succeeded or Failed phase</li>
  <li>[PR #12813][akalenyu] BugFix: can’t create export pod on OpenShift</li>
  <li>[PR #12786][0xFelix] virtctl: Created VMs can infer an instancetype or preference from PVC, Registry and Snapshot sources now.</li>
  <li>[PR #12764][ShellyKa13] bugfix: vmrestore create DVs before creation/update of restored VM</li>
  <li>[PR #10562][dhiller] Continue changes to Ginkgo V2 Serial runs</li>
  <li>[PR #12516][vamsikrishna-siddu] enable initial e2e tests for s390x.</li>
  <li>[PR #12739][lyarwood] A new <code class="language-plaintext highlighter-rouge">PreferredEfi</code> field has been added to preferences to express the preferred <code class="language-plaintext highlighter-rouge">EFI</code> configuration for a given <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</li>
  <li>[PR #12737][machadovilaca] Add evictable label to kubevirt_vmi_info</li>
  <li>[PR #12232][lyarwood] The <code class="language-plaintext highlighter-rouge">NUMA</code> feature gate is now deprecated with the feature state graduated to <code class="language-plaintext highlighter-rouge">GA</code> and thus enabled by default</li>
  <li>[PR #12582][mhenriks] vmsnapshot: when checking if a VM is running, ignore runStrategy</li>
  <li>[PR #12625][machadovilaca] Add kubevirt_vm_resource_requests for CPU resource</li>
  <li>[PR #12605][mhenriks] vmexport: enable status subresource for VirtualMachineExport</li>
  <li>[PR #12616][orenc1] replace <code class="language-plaintext highlighter-rouge">Update()</code> with <code class="language-plaintext highlighter-rouge">Patch()</code> for <code class="language-plaintext highlighter-rouge">test VirtualMachineInstancesPerNode</code></li>
  <li>[PR #12557][codingben] Optionally create data source using virtctl image upload.</li>
  <li>[PR #12547][mhenriks] virt-api: skip clone auth check when DataVolume already exists</li>
  <li>[PR #12613][orelmisan] Bridge binding: Static routes to subnets containing the pod’s NIC IP address are passed to the VM.</li>
  <li>[PR #12594][tiraboschi] [tests] introduce a decorator for Periodic_only tests</li>
  <li>[PR #12593][machadovilaca] Add kubevirt_vm_resource_requests metric for memory resource</li>
  <li>[PR #12617][Acedus] grpc from go.mod is now correctly shipped in release images</li>
  <li>[PR #12638][akalenyu] BugFix: “Cannot allocate memory” warnings for containerdisk VMs</li>
  <li>[PR #12395][alicefr] Add new condition for VMIStorageLiveMigratable</li>
  <li>[PR #12419][nunnatsa] Add timeout to validation webhooks</li>
  <li>[PR #12592][awels] Fixed issue emitting created secret events when not actually creating secrets during VMExport setup</li>
  <li>[PR #12584][brianmcarey] Build KubeVirt with go v1.22.6</li>
  <li>[PR #12575][Barakmor1] Advise users to use RunStrategy in virt-api messages</li>
  <li>[PR #12466][orenc1] tests/vm_tests.go: replace Update() with Patch()</li>
  <li>[PR #12548][kubevirt-bot] Updated common-instancetypes bundles to v1.1.0</li>
  <li>[PR #12476][jschintag] Enable live-migration and node labels on s390x</li>
  <li>[PR #12194][mhenriks] VM supports kubevirt.io/immediate-data-volume-creation: “false” which delays creating DataVolumeTemplates until VM is started</li>
  <li>[PR #11802][matthewei] Adding newMacAddresses validatewebhook for  VMCloneAPI</li>
  <li>[PR #11754][nickolaev] Adding support for the <code class="language-plaintext highlighter-rouge">igb</code> network interface model</li>
  <li>[PR #12254][jkinred] * Reduced the severity of log messages when a <code class="language-plaintext highlighter-rouge">VolumeSnapshotClass</code> is not found. When snapshots are not enabled for a volume, the reason will still be displayed in the <code class="language-plaintext highlighter-rouge">status.volumeSnapshotStatuses</code> field of a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> resource.</li>
  <li>[PR #12460][mhenriks] virt-api: unencode authorization extra headers</li>
  <li>[PR #12451][fossedihelm] Fix: eviction requests to completed virt-launcher pods cannot trigger a live migration</li>
  <li>[PR #11881][lyarwood] The <code class="language-plaintext highlighter-rouge">expand-spec</code> subresource API now applies defaults to the returned <code class="language-plaintext highlighter-rouge">VirtualMachine</code> to ensure the <code class="language-plaintext highlighter-rouge">VirtualMachineInstanceSpec</code> within is closer to the eventual version used when starting the original <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</li>
  <li>[PR #12452][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 10.5.0 and QEMU 9.0.0.</li>
  <li>[PR #12425][fudancoder] fix some comments</li>
  <li>[PR #12354][qinqon] Use optional interface at passt binding sidecar</li>
  <li>[PR #12268][fossedihelm] Drop <code class="language-plaintext highlighter-rouge">ForceRestart</code> and <code class="language-plaintext highlighter-rouge">ForceStop</code> methods from client-go</li>
  <li>[PR #12235][orelmisan] Network binding plugins: Enable the ability to specify compute memory overhead</li>
  <li>[PR #12209][orenc1] Fix wrong KubeVirtVMIExcessiveMigrations alert calculation in an upgrade scenario.</li>
  <li>[PR #12261][fossedihelm] Fix: persistent tpm can be used with vmis containing dots in their name</li>
  <li>[PR #12247][Sreeja1725] Add perf-scale benchmarks for release v1.3</li>
  <li>[PR #12181][akalenyu] BugFix: Grant namespace admin RBAC to passthrough a client USB to a VMI</li>
  <li>[PR #12096][machadovilaca] Fix missing performance metrics for VMI resources</li>
  <li>[PR #11856][Sreeja1725] Add unit tests to check for API backward compatibility</li>
  <li>[PR #12116][Sreeja1725] Add CPU/Memory utilization of components metrics to kubevirt benchmarks</li>
  <li>[PR #12195][awels] Virt export route has an edge termination of redirect</li>
  <li>[PR #12212][acardace] enable only for VMs with memory &gt;= 1Gi</li>
  <li>[PR #12053][vladikr] Only a single vgpu display option with ramfb will be configured per VMI.</li>
  <li>[PR #12193][acardace] fix RerunOnFailure stuck in Provisioning</li>
  <li>[PR #12186][kubevirt-bot] Updated common-instancetypes bundles to v1.0.1</li>
  <li>[PR #12180][0xFelix] VMs with a single socket and NetworkInterfaceMultiqueue enabled require a restart to hotplug additional CPU sockets.</li>
  <li>[PR #11927][lyarwood] All <code class="language-plaintext highlighter-rouge">preferredCPUTopology</code> constants prefixed with <code class="language-plaintext highlighter-rouge">Prefer</code> have been deprecated and will be removed in a future version of the <code class="language-plaintext highlighter-rouge">instancetype.kubevirt.io</code> API.</li>
  <li>[PR #12169][lyarwood] <code class="language-plaintext highlighter-rouge">PreferredDiskDedicatedIoThread</code> is now only applied to <code class="language-plaintext highlighter-rouge">virtio</code> disk devices</li>
  <li>[PR #12125][ksimon1] chore: bump virtio-win image version to 0.1.248</li>
  <li>[PR #12128][acardace] Memory Hotplug fixes and stabilization</li>
  <li>[PR #11911][alromeros] Bugfix: Implement retry mechanism in export server and vmexport</li>
  <li>[PR #11982][RamLavi] Introduce validatingAdmissionPolicy to restrict node patches on virt-handler</li>
  <li>[PR #12119][acardace] Fix VMPools when <code class="language-plaintext highlighter-rouge">LiveUpdate</code> as <code class="language-plaintext highlighter-rouge">vmRolloutStrategy</code> is used.</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.4.0 changes]]></summary></entry><entry><title type="html">You wanted more? It’s KubeVirt v1.4!</title><link href="https://kubevirt.io//2024/KubeVirt-v1-4.html" rel="alternate" type="text/html" title="You wanted more? It’s KubeVirt v1.4!" /><published>2024-11-12T00:00:00+00:00</published><updated>2024-11-12T00:00:00+00:00</updated><id>https://kubevirt.io//2024/KubeVirt-v1-4</id><content type="html" xml:base="https://kubevirt.io//2024/KubeVirt-v1-4.html"><![CDATA[<p>The KubeVirt Community is proud to announce the release of <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.4.0">v1.4</a>. This release aligns with <a href="https://kubernetes.io/blog/2024/08/13/kubernetes-v1-31-release/">Kubernetes v1.31</a> and is the sixth KubeVirt release to follow the Kubernetes release cadence.</p>

<p>What’s 1/3 of one thousand? Because that’s how many people have <a href="https://kubevirt.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.3.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=All&amp;var-country_name=All&amp;var-companies=All">contributed in some way</a> to this release, with 90 of those 333 people <a href="https://kubevirt.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.3.0%20-%20now&amp;var-metric=commits&amp;var-repogroup_name=All&amp;var-country_name=All&amp;var-companies=All">contributing commits to our repos</a>.</p>

<p>You can read the full <a href="https://kubevirt.io/user-guide/release_notes/#v140">release notes</a> in our user-guide, but we have included some highlights in this blog.</p>

<p>For those of you at KubeCon this week, be sure to check out our <a href="https://sched.co/1hoy6">maintainer talk</a> where our project maintainers will be going into these and other recent enhancements in KubeVirt.</p>

<h3 id="feature-ga">Feature GA</h3>
<p>This release marks the graduation of a number of features to GA; deprecating the feature gate and now enabled by default:</p>

<ul>
  <li><a href="https://kubevirt.io/user-guide/network/hotplug_interfaces/#hotplug-network-interfaces">Network hotplug</a>: Add network interfaces to, and remove them from, running virtual machines.</li>
  <li><a href="https://kubevirt.io/user-guide/user_workloads/instancetypes/">Common Instance types</a>: Simplify virtual machine creation with a predefined set of resource, performance, and runtime settings. We have also introduced a single configurable for cluster admins to explicitly disable this feature if required.</li>
  <li><a href="https://deploy-preview-840--kubevirt-user-guide.netlify.app/compute/numa/">NUMA</a>: Improving performance by mapping host NUMA topology to virtual machine topology.</li>
  <li><a href="https://deploy-preview-840--kubevirt-user-guide.netlify.app/compute/host-devices/#host-devices-assignment">GPU assignment</a>: An oldie but a goodie: Assign GPUs and vGPUs to virtual machines.</li>
</ul>

<p>This version of KubeVirt includes upgraded virtualization technology based on <a href="https://www.libvirt.org/news.html#v10-5-0-2024-07-01">libvirt 10.5.0</a> and <a href="https://www.qemu.org/2024/04/23/qemu-9-0-0/">QEMU 9.0.0</a>. Other KubeVirt-specific features of this release include the following:</p>

<h3 id="virtualization">Virtualization</h3>
<p>In the interest of security, we have restricted the <a href="https://github.com/kubevirt/kubevirt/pull/11982">ability of virt-handler</a> to patch nodes, and removed privileges for the cluster. You can also now <a href="https://github.com/kubevirt/kubevirt/pull/13090">live-update tolerations</a> to a running VM.</p>

<p>Our KubeVirt command line tool, virtctl, also received some love and <a href="https://kubevirt.io/user-guide/release_notes/ba#sig-compute">improved functionality</a> for VM creation, image upload, and source inference.</p>

<h3 id="networking">Networking</h3>
<p>The networking binding plugins have matured to Beta, and we have a new domain attachment type,<a href="https://github.com/kubevirt/kubevirt/pull/13024"><code class="language-plaintext highlighter-rouge">managedTap</code></a>, and the ability to <a href="https://github.com/kubevirt/kubevirt/pull/12235">reserve memory overhead</a> for binding plugins. <a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network binding plugins</a> enable vendors to provide their own VM-to-network plumbing alongside KubeVirt.</p>

<p>We also added support for the <code class="language-plaintext highlighter-rouge">igb</code> network interface model.</p>

<h3 id="storage">Storage</h3>
<p>If you’ve ever wanted to migrate your virtual machine volume from one storage type to another then you’ll be interested in our <a href="https://kubevirt.io/user-guide/storage/volume_migration/">volume migration</a> feature.</p>

<h3 id="scale-and-performance">Scale and Performance</h3>
<p>Our SIG scale and performance team have added performance benchmarks for resource utilization of virt-controller and virt-api components. Furthermore, the test-suite was enhanced by <a href="https://github.com/kubevirt/kubevirt/pull/12117">integrating KWOK with SIG-scale tests</a> to simulate nodes and VMIs to test KubeVirt performance while using minimum resources in test infrastructure. A comprehensive list of performance and scale benchmarks for the release is available <a href="https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md">here</a>.</p>

<h3 id="thanks">Thanks!</h3>
<p>A lot of work from a huge amount of people go into these releases. Some contributions are small, such as raising a bug or attending our community meeting, and others are massive, like working on a feature or reviewing PRs. Whatever your part: we thank you.</p>

<p>And if you’re interested in contributing to the project and being a part of the next release, please check out our <a href="https://kubevirt.io/user-guide/contributing/">contributing guide</a> and our <a href="https://github.com/kubevirt/community/blob/main/membership_policy.md">community membership guidelines</a>.</p>]]></content><author><name>KubeVirt Maintainers</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.4" /><category term="release" /><category term="community" /><category term="cncf" /><category term="milestone" /><category term="party time" /><summary type="html"><![CDATA[Introducing the KubeVirt v1.4 release]]></summary></entry><entry><title type="html">KubeVirt v1.3.0</title><link href="https://kubevirt.io//2024/changelog-v1.3.0.html" rel="alternate" type="text/html" title="KubeVirt v1.3.0" /><published>2024-07-17T00:00:00+00:00</published><updated>2024-07-17T00:00:00+00:00</updated><id>https://kubevirt.io//2024/changelog-v1.3.0</id><content type="html" xml:base="https://kubevirt.io//2024/changelog-v1.3.0.html"><![CDATA[<h2 id="v130">v1.3.0</h2>

<p>Released on: Wed Jul 17 15:09:44 2024 +0000</p>

<ul>
  <li>[PR #12319][Sreeja1725] Add v1.3.0 perf and scale benchmarks data</li>
  <li>[PR #12330][kubevirt-bot] Fix wrong KubeVirtVMIExcessiveMigrations alert calculation in an upgrade scenario.</li>
  <li>[PR #12328][acardace] enable only for VMs with memory &gt;= 1Gi</li>
  <li>[PR #12272][Sreeja1725] Add unit tests to check for API backward compatibility</li>
  <li>[PR #12296][orelmisan] Network binding plugins: Enable the ability to specify compute memory overhead</li>
  <li>[PR #12279][kubevirt-bot] Fix: persistent tpm can be used with vmis containing dots in their name</li>
  <li>[PR #12226][kubevirt-bot] Virt export route has an edge termination of redirect</li>
  <li>[PR #12240][kubevirt-bot] Updated common-instancetypes bundles to v1.0.1</li>
  <li>[PR #12249][kubevirt-bot] Fix missing performance metrics for VMI resources</li>
  <li>[PR #12237][vladikr] Only a single vgpu display option with ramfb will be configured per VMI.</li>
  <li>[PR #12122][kubevirt-bot] Fix VMPools when <code class="language-plaintext highlighter-rouge">LiveUpdate</code> as <code class="language-plaintext highlighter-rouge">vmRolloutStrategy</code> is used.</li>
  <li>[PR #12201][kubevirt-bot] fix RerunOnFailure stuck in Provisioning</li>
  <li>[PR #12151][kubevirt-bot] Bugfix: Implement retry mechanism in export server and vmexport</li>
  <li>[PR #12171][kubevirt-bot] <code class="language-plaintext highlighter-rouge">PreferredDiskDedicatedIoThread</code> is now only applied to <code class="language-plaintext highlighter-rouge">virtio</code> disk devices</li>
  <li>[PR #12146][kubevirt-bot] Memory Hotplug fixes and stabilization</li>
  <li>[PR #12185][kubevirt-bot] VMs with a single socket and NetworkInterfaceMultiqueue enabled require a restart to hotplug additional CPU sockets.</li>
  <li>[PR #12132][kubevirt-bot] Introduce validatingAdmissionPolicy to restrict node patches on virt-handler</li>
  <li>[PR #12109][acardace] Support Memory Hotplug with Hugepages</li>
  <li>[PR #12009][xpivarc] By enabling NodeRestriction feature gate, Kubevirt now authorize virt-handler’s requests to modify VMs.</li>
  <li>[PR #11681][lyarwood] The <code class="language-plaintext highlighter-rouge">CommonInstancetypesDeployment</code> feature and gate are retrospectively moved to Beta from the 1.2.0 release.</li>
  <li>[PR #12025][fossedihelm] Add descheduler compatibility</li>
  <li>[PR #12097][fossedihelm] Bump k8s deps to 0.30.0</li>
  <li>[PR #12089][jean-edouard] Less privileged virt-operator ClusterRole</li>
  <li>[PR #12064][akalenyu] BugFix: Graceful deletion skipped for any delete call to the VM (not VMI) resource</li>
  <li>[PR #10490][jschintag] Add support for building and running kubevirt on s390x.</li>
  <li>[PR #12079][EdDev] Network hotplug feature is declared as Beta.</li>
  <li>[PR #11455][lyarwood] <code class="language-plaintext highlighter-rouge">LiveUpdates</code>  of VMs using instance types are now supported with the same caveats as when making changes to a vanilla VM.</li>
  <li>[PR #12000][machadovilaca] Create kubevirt_vmi_launcher_memory_overhead_bytes metric</li>
  <li>[PR #11915][ormergi] The ‘passt’ core network binding is discontinued and removed.</li>
  <li>[PR #12016][acardace] fix starting VM with Manual RunStrategy</li>
  <li>[PR #11533][alicefr] Implement volume migration and introduce the migration updateVolumesStrategy field</li>
  <li>[PR #11934][assafad] Add kubevirt_vmi_last_connection_timestamp_seconds metric</li>
  <li>[PR #11956][mhenriks] Introduce export.kibevirt.io/v1beta1</li>
  <li>[PR #11996][ShellyKa13] BugFix: Fix restore panic in case of volumesnapshot missing</li>
  <li>[PR #11957][mhenriks] snapshot: Ignore unfreeze error if VMSnapshot deleting</li>
  <li>[PR #11906][machadovilaca] Create kubevirt_vmi_info metric</li>
  <li>[PR #11969][iholder101] Infra components control-plane nodes NoSchedule toleration</li>
  <li>[PR #11955][mhenriks] Introduce snapshot.kibevirt.io/v1beta1</li>
  <li>[PR #11883][orelmisan] Restart of a VM is required when the CPU socket count is reduced</li>
  <li>[PR #11835][talcoh2x] add Intel Gaudi to adopters.</li>
  <li>[PR #11344][aerosouund] Refactor device plugins to use a base plugin and define a common interface</li>
  <li>[PR #11973][fossedihelm] Bug fix: Correctly reflect RestartRequired condition</li>
  <li>[PR #11963][acardace] Fix RerunOnFailure RunStrategy</li>
  <li>[PR #11962][lyarwood] <code class="language-plaintext highlighter-rouge">VirtualMachines</code> referencing an instance type are now allowed when the <code class="language-plaintext highlighter-rouge">LiveUpdate</code> feature is enabled and will trigger the <code class="language-plaintext highlighter-rouge">RestartRequired</code> condition if the reference within the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> is changed.</li>
  <li>[PR #11942][ido106] Update virtctl to use v1beta1 endpoint for both regular and async image upload</li>
  <li>[PR #11648][kubevirt-bot] Updated common-instancetypes bundles to v1.0.0</li>
  <li>[PR #11659][iholder101] Require scheduling infra components onto control-plane nodes</li>
  <li>[PR #10545][lyarwood] <code class="language-plaintext highlighter-rouge">ControllerRevisions</code> containing instance types and preferences are now upgraded to their latest available version when the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> owning them is resync’d by <code class="language-plaintext highlighter-rouge">virt-controller</code>.</li>
  <li>[PR #11901][EdDev] The ‘macvtap’ core network binding is discontinued and removed.</li>
  <li>[PR #11922][alromeros] Bugfix: Fix VM manifest rendering in export controller</li>
  <li>[PR #11908][victortoso] sidecar-shim: allow stderr log from binary hooks</li>
  <li>[PR #11729][lyarwood] <code class="language-plaintext highlighter-rouge">spreadOptions</code> have been introduced to preferences in order to allow for finer grain control of the <code class="language-plaintext highlighter-rouge">preferSpread</code> <code class="language-plaintext highlighter-rouge">preferredCPUTopology</code>. This includes the ability to now spread vCPUs across guest visible sockets, cores and threads.</li>
  <li>[PR #11655][acardace] Allow to hotplug vcpus for VMs with CPU requests and/or limits set</li>
  <li>[PR #11701][EdDev] The SLIRP core binding is deprecated and removed.</li>
  <li>[PR #11773][jean-edouard] Persistent TPM/UEFI will use the default storage class if none is specified in the CR.</li>
  <li>[PR #11846][victortoso] SMBios sidecar can be built out-of-tree</li>
  <li>[PR #11788][ormergi] The network-info annotation is now used for mapping between SR-IOV network and the underlying device PCI address</li>
  <li>[PR #11700][alicefr] Add the updateVolumeStrategy field</li>
  <li>[PR #11256][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 10.0.0 and QEMU 8.2.0.</li>
  <li>[PR #11482][brianmcarey] Build KubeVirt with go v1.22.2</li>
  <li>[PR #11641][alicefr] Add kubevirt.io/testWorkloadUpdateMigrationAbortion annotation and a mechanism to abort workload updates</li>
  <li>[PR #11770][alicefr] Fix the live updates for volumes and disks</li>
  <li>[PR #11790][aburdenthehand] Re-adding Cloudflare to our ADOPTERS list</li>
  <li>[PR #11718][fossedihelm] Fix: SEV methods in client-go now satisfy the proxy server configuration, if provided</li>
  <li>[PR #11685][fossedihelm] Updated go version of the client-go to 1.21</li>
  <li>[PR #11618][AlonaKaplan] Extend network binding plugin to support device-info DownwardAPI.</li>
  <li>[PR #11283][assafad] Collect VMI OS info from the Guest agent as <code class="language-plaintext highlighter-rouge">kubevirt_vmi_phase_count</code> metric labels</li>
  <li>[PR #11676][machadovilaca] Rename rest client metrics to include kubevirt prefix</li>
  <li>[PR #11557][avlitman] New memory statistics added named kubevirt_memory_delta_from_requested_bytes</li>
  <li>[PR #11678][Vicente-Cheng] Improve the handling of ordinal pod interface name for upgrade</li>
  <li>[PR #11653][EdDev] Build the <code class="language-plaintext highlighter-rouge">passt</code>custom CNI binary statically, for the <code class="language-plaintext highlighter-rouge">passt</code> network binding plugin.</li>
  <li>[PR #11294][machadovilaca] Fix kubevirt_vm_created_total being broken down by virt-api pod</li>
  <li>[PR #11307][machadovilaca] Add e2e tests for metrics</li>
  <li>[PR #11479][vladikr] virtual machines instance will no longer be stuck in an irrecoverable state after an interrupted postcopy migration. Instead, these will fail and could be restarted again.</li>
  <li>[PR #11416][dhiller] emission of k8s logs when using programmatic focus with <code class="language-plaintext highlighter-rouge">FIt</code></li>
  <li>[PR #11272][dharmit] Make ‘image’ field in hook sidecar annotation optional.</li>
  <li>[PR #11500][iholder101] Support HyperV Passthrough: automatically use all available HyperV features</li>
  <li>[PR #11484][jcanocan] Reduce the downwardMetrics server maximum number of request per second to 1.</li>
  <li>[PR #11498][acardace] Allow to hotplug memory for VMs with memory limits set</li>
  <li>[PR #11470][brianmcarey] Build KubeVirt with Go version 1.21.8</li>
  <li>[PR #11312][alromeros] Improve handling of export resources in virtctl vmexport</li>
  <li>[PR #11367][alromeros] Bugfix: Allow vmexport download redirections by printing logs into stderr</li>
  <li>[PR #11219][alromeros] Bugfix: Improve handling of IOThreads with incompatible buses</li>
  <li>[PR #11149][0xFelix] virtctl: It is possible to import volumes from GCS when creating a VM now</li>
  <li>[PR #11404][avlitman] KubeVirtComponentExceedsRequestedCPU and KubeVirtComponentExceedsRequestedMemory alerts are deprecated; they do not indicate a genuine issue.</li>
  <li>[PR #11331][anjuls] add cloudraft to adopters.</li>
  <li>[PR #11387][alaypatel07] add perf-scale benchmarks for release v1.2</li>
  <li>[PR #11095][ShellyKa13] Expose volumesnapshot error in vmsnapshot object</li>
  <li>[PR #11372][xpivarc] Bug-fix: Fix nil panic if VM update fails</li>
  <li>[PR #11267][mhenriks] BugFix: Ensure DataVolumes created by virt-controller (DataVolumeTemplates) are recreated and owned by the VM in the case of DR and backup/restore.</li>
  <li>[PR #10900][KarstenB] BugFix: Fixed incorrect APIVersion of APIResourceList</li>
  <li>[PR #11306][fossedihelm] fix(ksm): set the <code class="language-plaintext highlighter-rouge">kubevirt.io/ksm-enabled</code> node label to true if the ksm is managed by KubeVirt, instead of reflect the actual ksm value.</li>
  <li>[PR #11330][jean-edouard] More information in the migration state of VMI / migration objects</li>
  <li>[PR #11264][machadovilaca] Fix perfscale buckets error</li>
  <li>[PR #11183][dhiller] Extend OWNERS for sig-buildsystem</li>
  <li>[PR #11058][fossedihelm] fix(vmclone): delete vmclone resource when the target vm is deleted</li>
  <li>[PR #11265][xpivarc] Bug fix: VM controller doesn’t corrupt its cache anymore</li>
  <li>[PR #11205][akalenyu] Fix migration breaking in case the VM has an rng device after hotplugging a block volume on cgroupsv2</li>
  <li>[PR #11051][alromeros] Bugfix: Improve error reporting when fsfreeze fails</li>
  <li>[PR #11156][nunnatsa] Move some verification from the VMI create validation webhook to the CRD</li>
  <li>[PR #11146][RamLavi] node-labeller: Remove obsolete functionalities</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v1.3.0 changes]]></summary></entry><entry><title type="html">KubeVirt Summit 2024 CfP is open!</title><link href="https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP.html" rel="alternate" type="text/html" title="KubeVirt Summit 2024 CfP is open!" /><published>2024-03-19T00:00:00+00:00</published><updated>2024-03-19T00:00:00+00:00</updated><id>https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP</id><content type="html" xml:base="https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP.html"><![CDATA[<p>We are very pleased to announce the details for this year’s KubeVirt Summit!!</p>

<h2 id="what-is-kubevirt-summit">What is KubeVirt Summit?</h2>

<p>KubeVirt Summit is our annual online conference, now in its fourth year, in which the entire broader community meets to showcase technical architecture, new features, proposed changes, and in-depth tutorials.
We have two tracks to cater for developer talks, and another for end users to share their deployment journey with KubeVirt and their use case(s) at scale. And there’s no reason why a talk can’t be both :)</p>

<h2 id="when-is-it">When is it?</h2>

<p>The event will take place online over two half-days:</p>

<ul>
  <li>Dates: <del>June 25 and 26</del> <strong>June 24 and 25</strong>, 2024</li>
  <li>Time: TBD 
(In the past we have aimed for 1200-1700 UTC but may modify these times slightly depending on our speaker timezones)</li>
</ul>

<h2 id="how-to-submit-a-proposal">How to submit a proposal?</h2>

<ul>
  <li>Please submit through this <a href="https://docs.google.com/forms/d/e/1FAIpQLSeELmfpD_20kZnrciXkdSdDS_MLFLN9xSaZDKptNPjg3JGLaA/viewform">Googleform</a></li>
  <li>CfP closes: <strong>May 20</strong>, 2024</li>
  <li>Schedule will announced at the end of May</li>
</ul>

<p>Do consider proposing a session, and help make our fourth Summit as valuable as possible. We welcome a range of session types, any of which can be simple and intended for beginners or face-meltingly technical. Check out <a href="https://www.youtube.com/playlist?list=PLnLpXX8KHIYwe_V5pCXfXVDs-lY5dX55Q">last year’s talks</a> for some ideas.</p>

<h2 id="have-questions">Have questions?</h2>

<p>Our <a href="/summit/">KubeVirt Summit 2024</a> page will continue to evolve with details as we get closer.</p>

<p>You can also reach out on our <a href="https://kubernetes.slack.com/messages/virtualization">virtualization</a> Slack channel (in the Kubernetes workspace).</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our mailing list, slack channels, weekly meetings, and more, all list in our <a href="https://github.com/kubevirt/community">community repo</a>.</p>

<p>Good luck!</p>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's fourth annual dedicated online event]]></summary></entry></feed>