<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed/uncategorized.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2024-11-13T15:44:00+00:00</updated><id>https://kubevirt.io//feed/uncategorized.xml</id><title type="html">KubeVirt.io | Uncategorized</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">Monitoring KubeVirt VMs from the inside</title><link href="https://kubevirt.io//2020/Monitoring-KubeVirt-VMs-from-the-inside.html" rel="alternate" type="text/html" title="Monitoring KubeVirt VMs from the inside" /><published>2020-12-10T00:00:00+00:00</published><updated>2020-12-10T00:00:00+00:00</updated><id>https://kubevirt.io//2020/Monitoring-KubeVirt-VMs-from-the-inside</id><content type="html" xml:base="https://kubevirt.io//2020/Monitoring-KubeVirt-VMs-from-the-inside.html"><![CDATA[<h2 id="monitoring-kubevirt-vms-from-the-inside">Monitoring KubeVirt VMs from the inside</h2>

<p>This blog post will guide you on how to monitor KubeVirt Linux based VirtualMachines with Prometheus <a href="https://github.com/prometheus/node_exporter">node-exporter</a>. Since node_exporter will run inside the VM and expose metrics at an HTTP endpoint, you can use this same guide to expose custom applications that expose metrics in the Prometheus format.</p>

<h2 id="environment">Environment</h2>

<p>This set of tools will be used on this guide:</p>

<ul>
  <li><a href="https://github.com/helm/helm">Helm v3</a> - To deploy the Prometheus-Operator.</li>
  <li><a href="https://github.com/kubernetes/minikube">minikube</a> - Will provide us a k8s cluster, you are free to choose any other k8s provider though.</li>
  <li><a href="https://github.com/kubernetes/kubectl">kubectl</a> - To deploy different k8s resources</li>
  <li>virtctl - to interact with KubeVirt VirtualMachines, can be downloaded from the <a href="https://github.com/kubevirt/kubevirt/releases">KubeVirt repo</a>.</li>
</ul>

<h2 id="deploy-prometheus-operator">Deploy Prometheus Operator</h2>

<p>Once you have your k8s cluster, with minikube or any other provider, the first step will be to deploy the Prometheus Operator. The reason is that the KubeVirt CR, when installed on the cluster, will detect if the ServiceMonitor CR already exists. If it does, then it will create ServiceMonitors configured to monitor all the KubeVirt components (virt-controller, virt-api, and virt-handler) out-of-the-box.</p>

<p>Although monitoring KubeVirt itself is not covered in this guide, it is a good practice to always deploy the Prometheus Operator before deploying KubeVirt.</p>

<p>To deploy the Prometheus Operator, you will need to create its namespace first, e.g. <code class="language-plaintext highlighter-rouge">monitoring</code>:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create ns monitoring
</code></pre></div></div>

<p>Then deploy the operator in the new namespace:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm fetch stable/prometheus-operator
<span class="nb">tar </span>xzf prometheus-operator<span class="k">*</span>.tgz
<span class="nb">cd </span>prometheus-operator/ <span class="o">&amp;&amp;</span> helm <span class="nb">install</span> <span class="nt">-n</span> monitoring <span class="nt">-f</span> values.yaml kubevirt-prometheus stable/prometheus-operator
</code></pre></div></div>

<p>After everything is deployed, you can delete everything that was downloaded by helm:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ..
<span class="nb">rm</span> <span class="nt">-rf</span> prometheus-operator<span class="k">*</span>
</code></pre></div></div>

<p>One thing to keep in mind is the release name we added here: <code class="language-plaintext highlighter-rouge">kubevirt-prometheus</code>. The release name will be used when declaring our <code class="language-plaintext highlighter-rouge">ServiceMonitor</code> later on..</p>

<h2 id="deploy-kubevirt-operators-and-kubevirt-customresources">Deploy KubeVirt Operators and KubeVirt CustomResources</h2>

<p>Alright, the next step will be deploying KubeVirt itself. We will start with its operator.</p>

<p>We will fetch the latest version, then use <code class="language-plaintext highlighter-rouge">kubectl create</code> to deploy the manifest directly from Github::</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">KUBEVIRT_VERSION</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> https://api.github.com/repos/kubevirt/kubevirt/releases | <span class="nb">grep </span>tag_name | <span class="nb">grep</span> <span class="nt">-v</span> <span class="nt">--</span> - | <span class="nb">sort</span> <span class="nt">-V</span> | <span class="nb">tail</span> <span class="nt">-1</span> | <span class="nb">awk</span> <span class="nt">-F</span><span class="s1">':'</span> <span class="s1">'{print $2}'</span> | <span class="nb">sed</span> <span class="s1">'s/,//'</span> | xargs<span class="si">)</span>
kubectl create <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/<span class="k">${</span><span class="nv">KUBEVIRT_VERSION</span><span class="k">}</span>/kubevirt-operator.yaml
</code></pre></div></div>

<p>Before deploying the KubeVirt CR, make sure that all kubevirt-operator replicas are ready, you can do that with:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl rollout status <span class="nt">-n</span> kubevirt deployment virt-operator
</code></pre></div></div>

<p>After that, we can deploy KubeVirt and wait for all it’s components to get ready in a similar manner:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/<span class="k">${</span><span class="nv">KUBEVIRT_VERSION</span><span class="k">}</span>/kubevirt-cr.yaml
kubectl rollout status <span class="nt">-n</span> kubevirt deployment virt-api
kubectl rollout status <span class="nt">-n</span> kubevirt deployment virt-controller
kubectl rollout status <span class="nt">-n</span> kubevirt daemonset virt-handler
</code></pre></div></div>

<p>If we want to monitor VMs that can restart, we want our node-exporter to be persisted and, thus, we need to set up persistent storage for them. <a href="https://github.com/kubevirt/containerized-data-importer">CDI</a> will be the component responsible for that, so we will deploy it’s operator and custom resource as well. As always, waiting for the right components to get ready before proceeding:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">CDI_VERSION</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> https://github.com/kubevirt/containerized-data-importer/releases/latest | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"v[0-9]</span><span class="se">\.</span><span class="s2">[0-9]*</span><span class="se">\.</span><span class="s2">[0-9]*"</span><span class="si">)</span>
kubectl create <span class="nt">-f</span> https://github.com/kubevirt/containerized-data-importer/releases/download/<span class="nv">$CDI_VERSION</span>/cdi-operator.yaml
kubectl rollout status <span class="nt">-n</span> cdi deployment cdi-operator

kubectl create <span class="nt">-f</span> https://github.com/kubevirt/containerized-data-importer/releases/download/<span class="nv">$CDI_VERSION</span>/cdi-cr.yaml
kubectl rollout status <span class="nt">-n</span> cdi deployment cdi-apiserver
kubectl rollout status <span class="nt">-n</span> cdi deployment cdi-uploadproxy
kubectl rollout status <span class="nt">-n</span> cdi deployment cdi-deployment
</code></pre></div></div>

<h2 id="deploying-a-virtualmachine-with-persistent-storage">Deploying a VirtualMachine with persistent storage</h2>

<p>Alright, cool. We have everything we need now. Let’s setup the VM.</p>

<p>We will start with the <code class="language-plaintext highlighter-rouge">PersistenVolume</code>’s required by <a href="https://github.com/kubevirt/containerized-data-importer/blob/main/doc/datavolumes.md">CDI’s DataVolume</a> resources. Since I’m using minikube with no dynamic storage provider, I’ll be creating 2 PVs with a reference to the PVCs that will claim them. Notice <code class="language-plaintext highlighter-rouge">claimRef</code> in each of the PVs.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-volume</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
  <span class="na">claimRef</span><span class="pi">:</span>
    <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">cirros-dv</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">2Gi</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/data/example-volume/</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-volume-scratch</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
  <span class="na">claimRef</span><span class="pi">:</span>
    <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">cirros-dv-scratch</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">2Gi</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/data/example-volume-scratch/</span>
</code></pre></div></div>

<p>With the persistent storage in place, we can create our VM with the following manifest:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">monitorable-vm</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">monitorable-vm</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-exporter"</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">my-data-volume</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">dataVolume</span><span class="pi">:</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">cirros-dv</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">my-data-volume</span>
  <span class="na">dataVolumeTemplates</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">cirros-dv"</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">source</span><span class="pi">:</span>
          <span class="na">http</span><span class="pi">:</span>
             <span class="na">url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">https://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img"</span>
      <span class="na">pvc</span><span class="pi">:</span>
        <span class="na">storageClassName</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">accessModes</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">storage</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2Gi"</span>
</code></pre></div></div>

<p>Notice that KubeVirt’s VirtualMachine resource has a VirtualMachine template and a dataVolumeTemplate. On the VirtualMachine template, it is important noticing that we named our VM <code class="language-plaintext highlighter-rouge">monitorable-vm</code>, and we will use this name to connect to its console with <code class="language-plaintext highlighter-rouge">virtctl</code> later on. The label we’ve added, <code class="language-plaintext highlighter-rouge">prometheus.kubevirt.io: "node-exporter"</code>, is also important, since we’ll use it when <a href="#configuring-prometheus-to-scrape-the-vms-node-exporter">configuring Prometheus to scrape the VM’s node-exporter</a></p>

<p>On dataVolumeTemplate, it is important noticing that we named the PVC <code class="language-plaintext highlighter-rouge">cirros-dv</code> and the DataVolume resource will create 2 PVCs with that, <code class="language-plaintext highlighter-rouge">cirros-dv</code> and <code class="language-plaintext highlighter-rouge">cirros-dv-scratch</code>. Notice that <code class="language-plaintext highlighter-rouge">cirros-dv</code> and <code class="language-plaintext highlighter-rouge">cirros-dv-scratch</code> are the names referenced on our PersistentVolume manifests. The names must match for this to work.</p>

<h2 id="installing-the-node-exporter-inside-the-vm">Installing the node-exporter inside the VM</h2>

<p>Once the VirtualMachineInstance is running, we can connect to its console using <code class="language-plaintext highlighter-rouge">virtctl console monitorable-vm</code>. If user and password are required, provide your credentials accordingly. If you are using the same disk image from this guide, the user and password are <code class="language-plaintext highlighter-rouge">cirros</code> and <code class="language-plaintext highlighter-rouge">gocubsgo</code> respectively.</p>

<p>The following script will install node-exporter and configure the VM to always start the exporter when booting:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-LO</span> <span class="nt">-k</span> https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz
<span class="nb">gunzip</span> <span class="nt">-c</span> node_exporter-1.0.1.linux-amd64.tar.gz | <span class="nb">tar </span>xopf -
./node_exporter-1.0.1.linux-amd64/node_exporter &amp;

<span class="nb">sudo</span> /bin/sh <span class="nt">-c</span> <span class="s1">'cat &gt; /etc/rc.local &lt;&lt;EOF
#!/bin/sh
echo "Starting up node_exporter at :9100!"

/home/cirros/node_exporter-1.0.1.linux-amd64/node_exporter 2&gt;&amp;1 &gt; /dev/null &amp;
EOF'</span>
<span class="nb">sudo chmod</span> +x /etc/rc.local
</code></pre></div></div>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p>P.S.: If you are using a different base image, please configure node-exporter to start at boot time accordingly</p>


</div></div>
<h2 id="configuring-prometheus-to-scrape-the-vms-node-exporter">Configuring Prometheus to scrape the VM’s node-exporter</h2>

<p>To configure Prometheus to scrape the node-exporter (or other applications) is really simple. All we need is to create a new <code class="language-plaintext highlighter-rouge">Service</code> and a <code class="language-plaintext highlighter-rouge">ServiceMonitor</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">monitorable-vm-node-exporter</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-exporter"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">metrics</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">9100</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">9100</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-exporter"</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">monitoring.coreos.com/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ServiceMonitor</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kubevirt-node-exporters-servicemonitor</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">monitoring</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-exporter"</span>
    <span class="na">release</span><span class="pi">:</span> <span class="s">monitoring</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">namespaceSelector</span><span class="pi">:</span>
    <span class="na">any</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-exporter"</span>
  <span class="na">endpoints</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="s">metrics</span>
    <span class="na">interval</span><span class="pi">:</span> <span class="s">15s</span>
</code></pre></div></div>

<p>Let’s break this down just to make sure we set up everything right. Starting with the <code class="language-plaintext highlighter-rouge">Service</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">metrics</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">9100</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">9100</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-exporter"</span>
</code></pre></div></div>

<p>On the specification, we are creating a new port named <code class="language-plaintext highlighter-rouge">metrics</code> that will be redirected to every pod labeled with <code class="language-plaintext highlighter-rouge">prometheus.kubevirt.io: "node-exporter"</code>, at port 9100, which is the default port number for the node-exporter.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">monitorable-vm-node-exporter</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-exporter"</span>
</code></pre></div></div>

<p>We are also labeling the Service itself with <code class="language-plaintext highlighter-rouge">prometheus.kubevirt.io: "node-exporter"</code>, that will be used by the <code class="language-plaintext highlighter-rouge">ServiceMonitor</code> object.</p>

<p>Now let’s take a look at our <code class="language-plaintext highlighter-rouge">ServiceMonitor</code> specification:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">namespaceSelector</span><span class="pi">:</span>
    <span class="na">any</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-exporter"</span>
  <span class="na">endpoints</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="s">metrics</span>
    <span class="na">interval</span><span class="pi">:</span> <span class="s">15s</span>
</code></pre></div></div>

<p>Since our ServiceMonitor will be deployed at the <code class="language-plaintext highlighter-rouge">monitoring</code> namespace, but our service is at the <code class="language-plaintext highlighter-rouge">default</code> namespace, we need <code class="language-plaintext highlighter-rouge">namespaceSelector.any=true</code>.</p>

<p>We are also telling our ServiceMonitor that Prometheus needs to scrape endpoints from services labeled with <code class="language-plaintext highlighter-rouge">prometheus.kubevirt.io: "node-exporter"</code> and which ports are named <code class="language-plaintext highlighter-rouge">metrics</code>. Luckily, that’s exactly what we did with our <code class="language-plaintext highlighter-rouge">Service</code>!</p>

<p>One last thing to keep an eye on. Prometheus configuration can be set up to watch multiple ServiceMonitors. We can see which ServiceMonitors our Prometheus is watching with the following command:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Look for Service Monitor Selector</span>
kubectl describe <span class="nt">-n</span> monitoring prometheuses.monitoring.coreos.com monitoring-prometheus-oper-prometheus
</code></pre></div></div>

<p>Make sure our ServiceMonitor has all labels required by Prometheus’ <code class="language-plaintext highlighter-rouge">Service Monitor Selector</code>. One common selector is the release name that we’ve set when deploying our Prometheus with helm!</p>

<h2 id="testing">Testing</h2>

<p>You can do a quick test by port-forwarding Prometheus web UI and executing some PromQL:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl port-forward <span class="nt">-n</span> monitoring prometheus-monitoring-prometheus-oper-prometheus-0 9090:9090
</code></pre></div></div>

<p>To make sure everything is working, access <code class="language-plaintext highlighter-rouge">localhost:9090/graph</code> and execute the PromQL <code class="language-plaintext highlighter-rouge">up{pod=~"virt-launcher.*"}</code>. Prometheus should return data that is being collected from <code class="language-plaintext highlighter-rouge">monitorable-vm</code>’s node-exporter.</p>

<p>You can play around with <code class="language-plaintext highlighter-rouge">virtctl</code>, stop and starting the VM to see how the metrics behave. You will notice that when stopping the VM with <code class="language-plaintext highlighter-rouge">virtctl stop monitorable-vm</code>, the VirtualMachineInstance is killed and, thus, so is it’s pod. This will result with our service not being able to find the pod’s endpoint and then it will be removed from Prometheus’ targets.</p>

<p>With this behavior, alerts like the one below won’t work since our target is literally gone, not down.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">alert</span><span class="pi">:</span> <span class="s">KubeVirtVMDown</span>
    <span class="s">expr</span><span class="err">:</span> <span class="s">up{pod=~"virt-launcher.*"} == </span><span class="m">0</span>
    <span class="na">for</span><span class="pi">:</span> <span class="s">1m</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">severity</span><span class="pi">:</span> <span class="s">warning</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="s">KubeVirt VM {{ $labels.pod }} is down.</span>
</code></pre></div></div>

<p><strong>BUT</strong>, if the VM is constantly crashing without being stopped, the pod won’t be killed and the target will still be monitored. Node-exporter will never start or will go down constantly alongside the VM, so an alert like this might work:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">alert</span><span class="pi">:</span> <span class="s">KubeVirtVMCrashing</span>
    <span class="s">expr</span><span class="err">:</span> <span class="s">up{pod=~"virt-launcher.*"} == </span><span class="m">0</span>
    <span class="na">for</span><span class="pi">:</span> <span class="s">5m</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">severity</span><span class="pi">:</span> <span class="s">critical</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="s">KubeVirt VM {{ $labels.pod }} is constantly crashing before node-exporter starts at boot.</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog post we used <a href="https://github.com/prometheus/node_exporter">node-exporter</a> to expose metrics out of a KubeVirt VM. We also configured <a href="https://github.com/prometheus-operator/prometheus-operator">Prometheus Operator</a> to collect these metrics. This illustrates how to bring Kubernetes monitoring best practices with applications running inside KubeVirt VMs.</p>]]></content><author><name>arthursens</name></author><category term="uncategorized" /><category term="kubevirt" /><category term="Kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="prometheus" /><category term="prometheus-operator" /><category term="node-exporter" /><category term="monitoring" /><summary type="html"><![CDATA[This blog post guides users on how to monitor linux based VMs from the inside with node-exporter and expose metrics to a Prometheus server]]></summary></entry><entry><title type="html">Proxy VM Conclusion</title><link href="https://kubevirt.io//2018/Proxy-vm-conclusion.html" rel="alternate" type="text/html" title="Proxy VM Conclusion" /><published>2018-06-13T00:00:00+00:00</published><updated>2018-06-13T00:00:00+00:00</updated><id>https://kubevirt.io//2018/Proxy-vm-conclusion</id><content type="html" xml:base="https://kubevirt.io//2018/Proxy-vm-conclusion.html"><![CDATA[<p>This blog post follow my previous research on how to allow vms inside a k8s cluster tp play nice with istio and other sidecars.</p>

<h1 id="research-conclusions-and-network-roadmap">Research conclusions and network roadmap</h1>

<p>After the deep research about different options/ways to connect VM to pods, we find that all the solution have different pros and cons.
All the represented solution need access to kernel modules and have the risk of conflicting with other networking tools.</p>

<p>We decided to implement a 100% Kubernetes compatible network approach on the KubeVirt project by using the slirp interface qemu provides.
This approach let the VM (from a networking perspective) behave like a process. Thus all traffic is going in and out of TCP or UDP sockets. The approach especially needs to avoid to rely on any specific Kernel configurations (like iptables, ebtables, tc, …) in order to not conflict with other Kubernetes networking tools like Istio or multus.</p>

<p>This is just an intermediate solution, because it’s shortcomings (unmaintained, unsafe, not performing well)</p>

<h3 id="slirp-interface">Slirp interface</h3>

<p>Pros:</p>

<ul>
  <li>vm ack like a process</li>
  <li>No external modules needed</li>
  <li>No external process needed</li>
  <li>Works with any sidecar solution</li>
  <li>no rely on any specific Kernel configurations</li>
  <li>pod can run without privilege</li>
</ul>

<p>Cons:</p>

<ul>
  <li>poor performance</li>
  <li>use userspace network stack</li>
</ul>

<h3 id="iptables-only">Iptables only</h3>

<p>Pros:</p>

<ul>
  <li>No external modules needed</li>
  <li>No external process needed</li>
  <li>All the traffic is handled by the kernel user space not involved</li>
</ul>

<p>Cons:</p>

<ul>
  <li><span style="color:red;">Istio dedicated solution!</span></li>
  <li>Not other process can change the iptables rules</li>
</ul>

<h3 id="iptables-with-a-nat-proxy">Iptables with a nat-proxy</h3>

<p>Pros:</p>

<ul>
  <li>No external modules needed</li>
  <li>Works with any sidecar solution</li>
</ul>

<p>Cons:</p>

<ul>
  <li>Not other process can change the iptables rules</li>
  <li>External process needed</li>
  <li>The traffic is passed to user space</li>
  <li>Only support ingress TCP connection</li>
</ul>

<h3 id="iptables-with-a-trasperent-proxy">Iptables with a trasperent-proxy</h3>

<p>Pros:</p>

<ul>
  <li>other process can change the nat table (this solution works on the mangle table)</li>
  <li>better preformance comparing to nat-proxy</li>
  <li>Works with any sidecar solution</li>
</ul>

<p>Cons:</p>

<ul>
  <li>Need NET_ADMIN capability for the docker</li>
  <li>External process needed</li>
  <li>The traffic is passed to user space</li>
  <li>Only support ingress TCP connection</li>
</ul>]]></content><author><name>SchSeba</name></author><category term="uncategorized" /><category term="istio" /><category term="multus" /><category term="roadmap" /><summary type="html"><![CDATA[This is a roadmap blog post for the network implementation in the kubevirt project]]></summary></entry><entry><title type="html">Non Dockerized Build</title><link href="https://kubevirt.io//2018/Non-Dockerized-Build.html" rel="alternate" type="text/html" title="Non Dockerized Build" /><published>2018-06-07T00:00:00+00:00</published><updated>2018-06-07T00:00:00+00:00</updated><id>https://kubevirt.io//2018/Non-Dockerized-Build</id><content type="html" xml:base="https://kubevirt.io//2018/Non-Dockerized-Build.html"><![CDATA[<p>In this post we will set up an alternative to the existing containerized build system used in KubeVirt.</p>

<p>A <a href="../assets/2018-06-07-Non-Dockerized-Build/Makefile.nocontainer">new makefile</a> will be presented here, which you can for experimenting (if you are brave enough…)</p>

<h1 id="why">Why?</h1>

<p>Current build system for KubeVirt is done inside docker. This ensures a robust and consistent build environment:</p>

<ul>
  <li>No need to install system dependencies</li>
  <li>Controlled versions of these dependencies</li>
  <li>Agnostic of local golang environment</li>
</ul>

<p>So, in general, <strong>you should just use the dockerized build system</strong>.</p>

<p>Still, there are some drawbacks there:</p>

<ul>
  <li>Tool integration:
    <ul>
      <li>Since your tools are not running in the dockerized environment, they may give different outcome than the ones running in the dockerized environment</li>
      <li>Invoking any of the dockerized scripts (under <code class="language-plaintext highlighter-rouge">hack</code> directory) may be inconsistent with the outside environment (e.g. file path is different than the one on your machine)</li>
    </ul>
  </li>
  <li>Build time: the dockerized build has some small overheads, and some improvements are still needed to make sure that caching work properly and build is optimized</li>
  <li>And last, but not least, <em>sometimes it is just hard to resist the tinkering…</em></li>
</ul>

<h2 id="how">How?</h2>

<p>Currently, the Makefile includes targets that address different things: building, dependencies, cluster management, testing etc. - here I tried to modify the minimum which is required for non-containerized build. Anything not related to it, should just be done using the existing Makefile.</p>

<blockquote>
  <p>note “Note
Cross compilation is not covered here (e.g. building <code class="language-plaintext highlighter-rouge">virtctl</code> for mac and windows)</p>
</blockquote>

<h3 id="prerequisites">Prerequisites</h3>

<p>Best place to look for that is in the docker file definition for the build environment: <a href="https://github.com/kubevirt/kubevirt/blob/main/hack/builder/Dockerfile">hack/docker-builder/Dockerfile</a></p>

<p>Note that not everything from there is needed for building, so the bare minimum on Fedora27 would be:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>dnf <span class="nb">install</span> <span class="nt">-y</span> git
<span class="nb">sudo </span>dnf <span class="nb">install</span> <span class="nt">-y</span> libvirt-devel
<span class="nb">sudo </span>dnf <span class="nb">install</span> <span class="nt">-y</span> golang
<span class="nb">sudo </span>dnf <span class="nb">install</span> <span class="nt">-y</span> docker
<span class="nb">sudo </span>dnf <span class="nb">install</span> <span class="nt">-y</span> qemu-img
</code></pre></div></div>

<p><em>Similarly to the containerized case</em>, docker is still needed (e.g. all the cluster stuff is done via docker), and therefore, any docker related preparations are needed as well. This would include running docker on startup and making sure that docker commands does not need root privileges. On Fedora27 this would mean:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>groupadd docker
<span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker <span class="nv">$USER</span>
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>docker
<span class="nb">sudo </span>systemctl start docker
</code></pre></div></div>

<p>Now, getting the actual code could be done either via <code class="language-plaintext highlighter-rouge">go get</code> (don’t forget to set the <code class="language-plaintext highlighter-rouge">GOPATH</code> environment variable):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>go get <span class="nt">-d</span> kubevirt.io/kubevirt/...

</code></pre></div></div>

<p>Or <code class="language-plaintext highlighter-rouge">git clone</code>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$GOPATH</span>/src/kubevirt.io/ <span class="o">&amp;&amp;</span> <span class="nb">cd</span> <span class="nv">$GOPATH</span>/src/kubevirt.io/
git clone https://github.com/kubevirt/kubevirt
</code></pre></div></div>

<h3 id="makefilenocontainer"><a href="../assets/2018-06-07-Non-Dockerized-Build/Makefile.nocontainer">Makefile.nocontainer</a></h3>

<div class="language-makefile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">all</span><span class="o">:</span> <span class="nf">build</span>

<span class="nl">bootstrap</span><span class="o">:</span>
    <span class="err">go</span> <span class="err">get</span> <span class="err">-u</span> <span class="err">github.com/onsi/ginkgo/ginkgo</span>
    <span class="err">go</span> <span class="err">get</span> <span class="err">-u</span> <span class="err">mvdan.cc/sh/cmd/shfmt</span>
    <span class="err">go</span> <span class="err">get</span> <span class="err">-u</span> <span class="err">-d</span> <span class="err">k8s.io/code-generator/cmd/deepcopy-gen</span>
    <span class="err">go</span> <span class="err">get</span> <span class="err">-u</span> <span class="err">-d</span> <span class="err">k8s.io/code-generator/cmd/defaulter-gen</span>
    <span class="err">go</span> <span class="err">get</span> <span class="err">-u</span> <span class="err">-d</span> <span class="err">k8s.io/code-generator/cmd/openapi-gen</span>
    <span class="err">cd</span> <span class="err">${GOPATH}/src/k8s.io/code-generator/cmd/deepcopy-gen</span> <span class="err">&amp;&amp;</span> <span class="err">git</span> <span class="err">checkout</span> <span class="err">release-1.9</span> <span class="err">&amp;&amp;</span> <span class="err">go</span> <span class="err">install</span>
    <span class="err">cd</span> <span class="err">${GOPATH}/src/k8s.io/code-generator/cmd/defaulter-gen</span> <span class="err">&amp;&amp;</span> <span class="err">git</span> <span class="err">checkout</span> <span class="err">release-1.9</span> <span class="err">&amp;&amp;</span> <span class="err">go</span> <span class="err">install</span>
    <span class="err">cd</span> <span class="err">${GOPATH}/src/k8s.io/code-generator/cmd/openapi-gen</span> <span class="err">&amp;&amp;</span> <span class="err">git</span> <span class="err">checkout</span> <span class="err">release-1.9</span> <span class="err">&amp;&amp;</span> <span class="err">go</span> <span class="err">install</span>

<span class="nl">generate</span><span class="o">:</span>
    <span class="err">./hack/generate.sh</span>

<span class="nl">apidocs</span><span class="o">:</span> <span class="nf">generate</span>
    <span class="err">./hack/gen-swagger-doc/gen-swagger-docs.sh</span> <span class="err">v1</span> <span class="err">html</span>

<span class="nl">build</span><span class="o">:</span> <span class="nf">check</span>
    <span class="err">go</span> <span class="err">install</span> <span class="err">-v</span> <span class="err">./cmd/...</span> <span class="err">./pkg/...</span>
    <span class="err">./hack/copy-cmd.sh</span>

<span class="nl">test</span><span class="o">:</span> <span class="nf">build</span>
    <span class="err">go</span> <span class="err">test</span> <span class="err">-v</span> <span class="err">-cover</span> <span class="err">./pkg/...</span>

<span class="nl">check</span><span class="o">:</span>
    <span class="err">./hack/check.sh</span>

<span class="nv">OUT_DIR</span><span class="o">=</span>./_out
<span class="nv">TESTS_OUT_DIR</span><span class="o">=</span><span class="nv">${OUT_DIR}</span>/tests

<span class="nl">functest</span><span class="o">:</span> <span class="nf">build</span>
    <span class="err">go</span> <span class="err">build</span> <span class="err">-v</span> <span class="err">./tests/...</span>
    <span class="err">ginkgo</span> <span class="err">build</span> <span class="err">./tests</span>
    <span class="err">mkdir</span> <span class="err">-p</span> <span class="err">${TESTS_OUT_DIR}/</span>
    <span class="err">mv</span> <span class="err">./tests/tests.test</span> <span class="err">${TESTS_OUT_DIR}/</span>
    <span class="err">./hack/functests.sh</span>

<span class="nl">cluster-sync</span><span class="o">:</span> <span class="nf">build</span>
    <span class="err">./hack/build-copy-artifacts.sh</span>
    <span class="err">./hack/build-manifests.sh</span>
    <span class="err">./hack/build-docker.sh</span> <span class="err">build</span>
    <span class="err">./cluster/clean.sh</span>
    <span class="err">./cluster/deploy.sh</span>

<span class="nl">.PHONY</span><span class="o">:</span> <span class="nf">bootstrap generate apidocs build test check functest cluster-sync</span>
</code></pre></div></div>

<h3 id="targets">Targets</h3>

<p>To execute any of the targets use:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make <span class="nt">-f</span> Makefile.nocontainer &lt;target&gt;
</code></pre></div></div>

<p>File has the following targets:</p>

<ul>
  <li><strong>bootstrap</strong>: this is actually part of the prerequisites, but added all golang tool dependencies here, since this is agnostic of the running platform Should be called once
    <ul>
      <li>Note that the k8s code generators use specific version</li>
      <li>Note that these are not code dependencies, as they are handled by using a <code class="language-plaintext highlighter-rouge">vendor</code> directory, as well as the distclean, deps-install and deps-update targets in the <a href="https://github.com/kubevirt/kubevirt/blob/main/Makefile">standard makefile</a></li>
    </ul>
  </li>
  <li><strong>generate</strong>: Calling <a href="https://github.com/kubevirt/kubevirt/blob/main/hack/generate.sh">hack/generate.sh</a> script similarly to the <a href="https://github.com/kubevirt/kubevirt/blob/main/Makefile">standard makefile</a>. It builds all generators (under the <code class="language-plaintext highlighter-rouge">tools</code> directory) and use them to generate: test mocks, KubeVirt resources and test yamls</li>
  <li><strong>apidocs</strong>: this is similar to apidocs target in the <a href="https://github.com/kubevirt/kubevirt/blob/main/Makefile">standard makefile</a></li>
  <li><strong>build</strong>: this is building all product binaries, and then using a script (<a href="../assets/2018-06-07-Non-Dockerized-Build/copy-cmd.sh">copy-cmd.sh</a>, should be placed under: <code class="language-plaintext highlighter-rouge">hack</code>) to copy the binaries from their standard location into the <code class="language-plaintext highlighter-rouge">_out</code> directory, where the cluster management scripts expect them</li>
  <li><strong>test</strong>: building and running unit tests
check: using similar code to the one used in the standard makefile: formatting files, fixing package imports and calling go vet</li>
  <li><strong>functest</strong>: building and running integration tests. After tests are built , they are moved to the <code class="language-plaintext highlighter-rouge">_out</code> directory so that the standard script for running integration tests would find them</li>
  <li><strong>cluster-sync</strong>: this is the only “cluster management” target that had to be modified from the standard makefile</li>
</ul>]]></content><author><name>yuvalif</name></author><category term="uncategorized" /><category term="docker" /><category term="container" /><category term="build" /><summary type="html"><![CDATA[This post tries to unveil some of the internals of our build system, by allowing you to build natively on your host]]></summary></entry><entry><title type="html">Research Run Vms With Istio Service Mesh</title><link href="https://kubevirt.io//2018/Research-run-VMs-with-istio-service-mesh.html" rel="alternate" type="text/html" title="Research Run Vms With Istio Service Mesh" /><published>2018-06-03T00:00:00+00:00</published><updated>2018-06-03T00:00:00+00:00</updated><id>https://kubevirt.io//2018/Research-run-VMs-with-istio-service-mesh</id><content type="html" xml:base="https://kubevirt.io//2018/Research-run-VMs-with-istio-service-mesh.html"><![CDATA[<p>In this blog post we are going to talk about istio and virtual machines on top of Kubernetes. Some of the components we are going to use are <a href="https://istio.io/docs/concepts/what-is-istio/overview/">istio</a>, <a href="https://libvirt.org/index.html">libvirt</a>, <a href="http://ebtables.netfilter.org/">ebtables</a>, <a href="https://en.wikipedia.org/wiki/Iptables">iptables</a>, and <a href="https://github.com/LiamHaworth/go-tproxy">tproxy</a>. Please review the links provided for an overview and deeper dive into each technology</p>

<h1 id="research-explanation">Research explanation</h1>

<p>Our research goal was to give virtual machines running inside pods (KubeVirt project) all the benefits Kubernetes have to offer, one of them is a service mesh like istio.</p>

<h2 id="iptables-only-with-dnat-and-source-nat-configuration">Iptables only with dnat and source nat configuration</h2>

<p><span style="color:red;">This configuration is istio only!</span></p>

<p>For this solution we created the following architecture</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/Iptables-diagram.png" alt="Iptables-Diagram" /></p>

<p>With the following yaml configuration</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">application-devel</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">9080</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>

<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">libvirtd-client-devel</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">16509</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">client-connection</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">5900</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">spice</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">22</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">ssh</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">strategy</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">annotations</span><span class="pi">:</span>
        <span class="na">sidecar.istio.io/status</span><span class="pi">:</span> <span class="s1">'</span><span class="s">{"version":"43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78","initContainers":["istio-init","enable-core-dump"],"containers":["istio-proxy"],"volumes":["istio-envoy","istio-certs"]}'</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/sebassch/mylibvirtd:devel</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">compute</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">9080</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">16509</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">5900</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">22</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">capabilities</span><span class="pi">:</span>
              <span class="na">add</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">ALL</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">runAsUser</span><span class="pi">:</span> <span class="m">0</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/lib/libvirt/images</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">test-volume</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/host-dev</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">host-dev</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/host-sys</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">host-sys</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">LIBVIRTD_DEFAULT_NETWORK_DEVICE</span>
              <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">eth0"</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">proxy</span>
            <span class="pi">-</span> <span class="s">sidecar</span>
            <span class="pi">-</span> <span class="s">--configPath</span>
            <span class="pi">-</span> <span class="s">/etc/istio/proxy</span>
            <span class="pi">-</span> <span class="s">--binaryPath</span>
            <span class="pi">-</span> <span class="s">/usr/local/bin/envoy</span>
            <span class="pi">-</span> <span class="s">--serviceCluster</span>
            <span class="pi">-</span> <span class="s">productpage</span>
            <span class="pi">-</span> <span class="s">--drainDuration</span>
            <span class="pi">-</span> <span class="s">45s</span>
            <span class="pi">-</span> <span class="s">--parentShutdownDuration</span>
            <span class="pi">-</span> <span class="s">1m0s</span>
            <span class="pi">-</span> <span class="s">--discoveryAddress</span>
            <span class="pi">-</span> <span class="s">istio-pilot.istio-system:15005</span>
            <span class="pi">-</span> <span class="s">--discoveryRefreshDelay</span>
            <span class="pi">-</span> <span class="s">1s</span>
            <span class="pi">-</span> <span class="s">--zipkinAddress</span>
            <span class="pi">-</span> <span class="s">zipkin.istio-system:9411</span>
            <span class="pi">-</span> <span class="s">--connectTimeout</span>
            <span class="pi">-</span> <span class="s">10s</span>
            <span class="pi">-</span> <span class="s">--statsdUdpAddress</span>
            <span class="pi">-</span> <span class="s">istio-mixer.istio-system:9125</span>
            <span class="pi">-</span> <span class="s">--proxyAdminPort</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">15000"</span>
            <span class="pi">-</span> <span class="s">--controlPlaneAuthPolicy</span>
            <span class="pi">-</span> <span class="s">MUTUAL_TLS</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POD_NAME</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">metadata.name</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POD_NAMESPACE</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">metadata.namespace</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">INSTANCE_IP</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">status.podIP</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/istio/proxy:0.7.1</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-proxy</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">false</span>
            <span class="na">readOnlyRootFilesystem</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">runAsUser</span><span class="pi">:</span> <span class="m">1337</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/etc/istio/proxy</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">istio-envoy</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/etc/certs/</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">istio-certs</span>
              <span class="na">readOnly</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">initContainers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">-p</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">15001"</span>
            <span class="pi">-</span> <span class="s">-u</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">1337"</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/istio/proxy_init:0.7.1</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-init</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">capabilities</span><span class="pi">:</span>
              <span class="na">add</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">NET_ADMIN</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">-c</span>
            <span class="pi">-</span> <span class="s">sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;&amp; ulimit -c</span>
              <span class="s">unlimited</span>
          <span class="na">command</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">/bin/sh</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">alpine</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">enable-core-dump</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">emptyDir</span><span class="pi">:</span>
            <span class="na">medium</span><span class="pi">:</span> <span class="s">Memory</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-envoy</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">istio-certs</span>
          <span class="na">secret</span><span class="pi">:</span>
            <span class="na">optional</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">secretName</span><span class="pi">:</span> <span class="s">istio.default</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">host-dev</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/dev</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">host-sys</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/sys</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">test-volume</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="c1"># directory location on host</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/bricks/brick1/volume/Images</span>
            <span class="c1"># this field is optional</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
<span class="na">status</span><span class="pi">:</span> <span class="pi">{}</span>

<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">gateway-devel</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">kubernetes.io/ingress.class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">istio"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/devel-myvm</span>
            <span class="na">backend</span><span class="pi">:</span>
              <span class="na">serviceName</span><span class="pi">:</span> <span class="s">application-devel</span>
              <span class="na">servicePort</span><span class="pi">:</span> <span class="m">9080</span>
</code></pre></div></div>

<p>When the my-libvirt container starts it runs an entry point script for iptables configuration.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. iptables <span class="nt">-t</span> nat <span class="nt">-D</span> PREROUTING 1
2. iptables <span class="nt">-t</span> nat <span class="nt">-A</span> PREROUTING <span class="nt">-p</span> tcp <span class="nt">-m</span> comment <span class="nt">--comment</span> <span class="s2">"KubeVirt Spice"</span>  <span class="nt">--dport</span> 5900 <span class="nt">-j</span> ACCEPT
3. iptables <span class="nt">-t</span> nat <span class="nt">-A</span> PREROUTING <span class="nt">-p</span> tcp <span class="nt">-m</span> comment <span class="nt">--comment</span> <span class="s2">"KubeVirt virt-manager"</span>  <span class="nt">--dport</span> 16509 <span class="nt">-j</span> ACCEPT
4. iptables <span class="nt">-t</span> nat  <span class="nt">-A</span> PREROUTING <span class="nt">-d</span> 10.96.0.0/12 <span class="nt">-m</span> comment <span class="nt">--comment</span> <span class="s2">"istio/redirect-ip-range-10.96.0.0/12-service cidr"</span> <span class="nt">-j</span> ISTIO_REDIRECT
5. iptables <span class="nt">-t</span> nat  <span class="nt">-A</span> PREROUTING <span class="nt">-d</span> 192.168.0.0/16 <span class="nt">-m</span> comment <span class="nt">--comment</span> <span class="s2">"istio/redirect-ip-range-192.168.0.0/16-Pod cidr"</span> <span class="nt">-j</span> ISTIO_REDIRECT
6. iptables <span class="nt">-t</span> nat  <span class="nt">-A</span> OUTPUT <span class="nt">-d</span> 127.0.0.1/32 <span class="nt">-p</span> tcp <span class="nt">-m</span> comment <span class="nt">--comment</span> <span class="s2">"KubeVirt mesh application port"</span> <span class="nt">--dport</span> 9080 <span class="nt">-j</span> DNAT <span class="nt">--to-destination</span> 10.0.0.2
7. iptables <span class="nt">-t</span> nat  <span class="nt">-A</span> POSTROUTING <span class="nt">-s</span> 127.0.0.1/32 <span class="nt">-d</span> 10.0.0.2/32 <span class="nt">-m</span> comment <span class="nt">--comment</span> <span class="s2">"KubeVirt VM Forward"</span> <span class="nt">-j</span> SNAT <span class="nt">--to-source</span> <span class="sb">`</span>ifconfig eth0 | <span class="nb">grep </span>inet | <span class="nb">awk</span> <span class="s1">'{print $2}'</span>
</code></pre></div></div>

<p>Now let’s explain every one of these lines:</p>

<ol>
  <li>Remove istio ingress connection rule that send all the ingress traffic directly to the envoy proxy (our vm traffic is ingress traffic for our pod)</li>
  <li>Allow ingress connection with spice port to get our libvirt process running in the pod</li>
  <li>Allow ingress connection with virt-manager port to get our libvirt process running in the pod</li>
  <li>Redirect all the traffic that came from the k8s clusters services to the envoy process</li>
  <li>Redirect all the traffic that came from the k8s clusters pods to the envoy process</li>
  <li>Send all the traffic that came from envoy process to our vm by changing the destination ip address to ur vm ip address</li>
  <li>Change the source ip address of the packet send by envoy from localhost to the pod ip address so the virtual machine can return the connection</li>
</ol>

<h3 id="iptables-configuration-conclusions">Iptables configuration conclusions</h3>

<p>With this configuration all the traffic that exit the virtual machine to a k8s service will pass the envoy process and will enter the istio service mash.
Also all the traffic that came into the pod will be pass to envoy and after that it will be send to our virtual machine</p>

<p>Egress data flow in this solution:</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/iptables-egress.png" alt="iptables-egress-traffic" /></p>

<p>Ingress data flow in this solution:</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/iptables-ingress.png" alt="iptables-ingress-traffic" /></p>

<p>Pros:</p>

<ul>
  <li>No external modules needed</li>
  <li>No external process needed</li>
  <li>All the traffic is handled by the kernel user space not involved</li>
</ul>

<p>Cons:</p>

<ul>
  <li><span style="color:red;">Istio dedicated solution!</span></li>
  <li>Not other process can change the iptables rules</li>
</ul>

<h2 id="iptables-with-a-nat-proxy-process">Iptables with a nat-proxy process</h2>

<p>For this solution a created the following architecture</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/nat-proxy.png" alt="nat-proxy-Diagram" /></p>

<p>With the following yaml configuration</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">application-nat-proxt</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-nat-proxt</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">9080</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-nat-proxt</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>

<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">libvirtd-client-nat-proxt</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-nat-proxt</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">16509</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">client-connection</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">5900</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">spice</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">22</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">ssh</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-nat-proxt</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">libvirtd-nat-proxt</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">strategy</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">annotations</span><span class="pi">:</span>
        <span class="na">sidecar.istio.io/status</span><span class="pi">:</span> <span class="s1">'</span><span class="s">{"version":"43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78","initContainers":["istio-init","enable-core-dump"],"containers":["istio-proxy"],"volumes":["istio-envoy","istio-certs"]}'</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-nat-proxt</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/sebassch/mylibvirtd:devel</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">compute</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">9080</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">16509</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">5900</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">22</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">capabilities</span><span class="pi">:</span>
              <span class="na">add</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">ALL</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">runAsUser</span><span class="pi">:</span> <span class="m">0</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/lib/libvirt/images</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">test-volume</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/host-dev</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">host-dev</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/host-sys</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">host-sys</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">LIBVIRTD_DEFAULT_NETWORK_DEVICE</span>
              <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">eth0"</span>
        <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/sebassch/mynatproxy:devel</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">proxy</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">capabilities</span><span class="pi">:</span>
              <span class="na">add</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">NET_ADMIN</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">proxy</span>
            <span class="pi">-</span> <span class="s">sidecar</span>
            <span class="pi">-</span> <span class="s">--configPath</span>
            <span class="pi">-</span> <span class="s">/etc/istio/proxy</span>
            <span class="pi">-</span> <span class="s">--binaryPath</span>
            <span class="pi">-</span> <span class="s">/usr/local/bin/envoy</span>
            <span class="pi">-</span> <span class="s">--serviceCluster</span>
            <span class="pi">-</span> <span class="s">productpage</span>
            <span class="pi">-</span> <span class="s">--drainDuration</span>
            <span class="pi">-</span> <span class="s">45s</span>
            <span class="pi">-</span> <span class="s">--parentShutdownDuration</span>
            <span class="pi">-</span> <span class="s">1m0s</span>
            <span class="pi">-</span> <span class="s">--discoveryAddress</span>
            <span class="pi">-</span> <span class="s">istio-pilot.istio-system:15005</span>
            <span class="pi">-</span> <span class="s">--discoveryRefreshDelay</span>
            <span class="pi">-</span> <span class="s">1s</span>
            <span class="pi">-</span> <span class="s">--zipkinAddress</span>
            <span class="pi">-</span> <span class="s">zipkin.istio-system:9411</span>
            <span class="pi">-</span> <span class="s">--connectTimeout</span>
            <span class="pi">-</span> <span class="s">10s</span>
            <span class="pi">-</span> <span class="s">--statsdUdpAddress</span>
            <span class="pi">-</span> <span class="s">istio-mixer.istio-system:9125</span>
            <span class="pi">-</span> <span class="s">--proxyAdminPort</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">15000"</span>
            <span class="pi">-</span> <span class="s">--controlPlaneAuthPolicy</span>
            <span class="pi">-</span> <span class="s">MUTUAL_TLS</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POD_NAME</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">metadata.name</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POD_NAMESPACE</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">metadata.namespace</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">INSTANCE_IP</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">status.podIP</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/istio/proxy:0.7.1</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-proxy</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">false</span>
            <span class="na">readOnlyRootFilesystem</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">runAsUser</span><span class="pi">:</span> <span class="m">1337</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/etc/istio/proxy</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">istio-envoy</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/etc/certs/</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">istio-certs</span>
              <span class="na">readOnly</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">initContainers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">-p</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">15001"</span>
            <span class="pi">-</span> <span class="s">-u</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">1337"</span>
            <span class="pi">-</span> <span class="s">-i</span>
            <span class="pi">-</span> <span class="s">10.96.0.0/12,192.168.0.0/16</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/istio/proxy_init:0.7.1</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-init</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">capabilities</span><span class="pi">:</span>
              <span class="na">add</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">NET_ADMIN</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">-c</span>
            <span class="pi">-</span> <span class="s">sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;&amp; ulimit -c</span>
              <span class="s">unlimited</span>
          <span class="na">command</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">/bin/sh</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">alpine</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">enable-core-dump</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">emptyDir</span><span class="pi">:</span>
            <span class="na">medium</span><span class="pi">:</span> <span class="s">Memory</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-envoy</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">istio-certs</span>
          <span class="na">secret</span><span class="pi">:</span>
            <span class="na">optional</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">secretName</span><span class="pi">:</span> <span class="s">istio.default</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">host-dev</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/dev</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">host-sys</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/sys</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">test-volume</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="c1"># directory location on host</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/bricks/brick1/volume/Images</span>
            <span class="c1"># this field is optional</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
<span class="na">status</span><span class="pi">:</span> <span class="pi">{}</span>

<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">gateway-nat-proxt</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">kubernetes.io/ingress.class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">istio"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/nat-proxt-myvm</span>
            <span class="na">backend</span><span class="pi">:</span>
              <span class="na">serviceName</span><span class="pi">:</span> <span class="s">application-nat-proxt</span>
              <span class="na">servicePort</span><span class="pi">:</span> <span class="m">9080</span>
</code></pre></div></div>

<p>When the mynatproxy container starts it runs an entry point script for iptables configuration.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. iptables -t nat -I PREROUTING 1 -p tcp -s 10.0.1.2 -m comment --comment "nat-proxy redirect" -j REDIRECT --to-ports 8080
2. iptables -t nat -I OUTPUT 1 -p tcp -s 10.0.1.2 -j ACCEPT
3. iptables -t nat -I POSTROUTING 1 -s 10.0.1.2 -p udp -m comment --comment "nat udp connections" -j MASQUERADE
</code></pre></div></div>

<p>Now let’s explain every one of these lines:</p>

<ol>
  <li>Redirect all the tcp traffic that came from the virtual machine to our proxy on port 8080</li>
  <li>Accept all the traffic that go from the pod to the virtual machine</li>
  <li>Nat all the udp traffic that came from the virtual machine</li>
</ol>

<p>This solution uses a container I created that has two processes inside, one for the egress traffic of the virtual machine and one for the ingress traffic.
For the egress traffic I used a program writen in golang, and for the ingress traffic I used haproxy.</p>

<p>The nat-proxy used a system call to get the original destination address and port that it’s being redirected to us from the iptables rules I created.</p>

<p>The extract function:</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="n">getOriginalDst</span><span class="p">(</span><span class="n">clientConn</span> <span class="o">*</span><span class="n">net</span><span class="o">.</span><span class="n">TCPConn</span><span class="p">)</span> <span class="p">(</span><span class="n">ipv4</span> <span class="kt">string</span><span class="p">,</span> <span class="n">port</span> <span class="kt">uint16</span><span class="p">,</span> <span class="n">newTCPConn</span> <span class="o">*</span><span class="n">net</span><span class="o">.</span><span class="n">TCPConn</span><span class="p">,</span> <span class="n">err</span> <span class="kt">error</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="n">clientConn</span> <span class="o">==</span> <span class="no">nil</span> <span class="p">{</span>
        <span class="n">log</span><span class="o">.</span><span class="n">Printf</span><span class="p">(</span><span class="s">"copy(): oops, dst is nil!"</span><span class="p">)</span>
        <span class="n">err</span> <span class="o">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">New</span><span class="p">(</span><span class="s">"ERR: clientConn is nil"</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>

    <span class="c">// test if the underlying fd is nil</span>
    <span class="n">remoteAddr</span> <span class="o">:=</span> <span class="n">clientConn</span><span class="o">.</span><span class="n">RemoteAddr</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">remoteAddr</span> <span class="o">==</span> <span class="no">nil</span> <span class="p">{</span>
        <span class="n">log</span><span class="o">.</span><span class="n">Printf</span><span class="p">(</span><span class="s">"getOriginalDst(): oops, clientConn.fd is nil!"</span><span class="p">)</span>
        <span class="n">err</span> <span class="o">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">New</span><span class="p">(</span><span class="s">"ERR: clientConn.fd is nil"</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>

    <span class="n">srcipport</span> <span class="o">:=</span> <span class="n">fmt</span><span class="o">.</span><span class="n">Sprintf</span><span class="p">(</span><span class="s">"%v"</span><span class="p">,</span> <span class="n">clientConn</span><span class="o">.</span><span class="n">RemoteAddr</span><span class="p">())</span>

    <span class="n">newTCPConn</span> <span class="o">=</span> <span class="no">nil</span>
    <span class="c">// net.TCPConn.File() will cause the receiver's (clientConn) socket to be placed in blocking mode.</span>
    <span class="c">// The workaround is to take the File returned by .File(), do getsockopt() to get the original</span>
    <span class="c">// destination, then create a new *net.TCPConn by calling net.Conn.FileConn().  The new TCPConn</span>
    <span class="c">// will be in non-blocking mode.  What a pain.</span>
    <span class="n">clientConnFile</span><span class="p">,</span> <span class="n">err</span> <span class="o">:=</span> <span class="n">clientConn</span><span class="o">.</span><span class="n">File</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">err</span> <span class="o">!=</span> <span class="no">nil</span> <span class="p">{</span>
        <span class="n">log</span><span class="o">.</span><span class="n">Printf</span><span class="p">(</span><span class="s">"GETORIGINALDST|%v-&gt;?-&gt;FAILEDTOBEDETERMINED|ERR: could not get a copy of the client connection's file object"</span><span class="p">,</span> <span class="n">srcipport</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">clientConn</span><span class="o">.</span><span class="n">Close</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="c">// Get original destination</span>
    <span class="c">// this is the only syscall in the Golang libs that I can find that returns 16 bytes</span>
    <span class="c">// Example result: &amp;{Multiaddr:[2 0 31 144 206 190 36 45 0 0 0 0 0 0 0 0] Interface:0}</span>
    <span class="c">// port starts at the 3rd byte and is 2 bytes long (31 144 = port 8080)</span>
    <span class="c">// IPv4 address starts at the 5th byte, 4 bytes long (206 190 36 45)</span>
    <span class="n">addr</span><span class="p">,</span> <span class="n">err</span> <span class="o">:=</span> <span class="n">syscall</span><span class="o">.</span><span class="n">GetsockoptIPv6Mreq</span><span class="p">(</span><span class="kt">int</span><span class="p">(</span><span class="n">clientConnFile</span><span class="o">.</span><span class="n">Fd</span><span class="p">()),</span> <span class="n">syscall</span><span class="o">.</span><span class="n">IPPROTO_IP</span><span class="p">,</span> <span class="n">SO_ORIGINAL_DST</span><span class="p">)</span>
    <span class="n">log</span><span class="o">.</span><span class="n">Printf</span><span class="p">(</span><span class="s">"getOriginalDst(): SO_ORIGINAL_DST=%+v</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">addr</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">err</span> <span class="o">!=</span> <span class="no">nil</span> <span class="p">{</span>
        <span class="n">log</span><span class="o">.</span><span class="n">Printf</span><span class="p">(</span><span class="s">"GETORIGINALDST|%v-&gt;?-&gt;FAILEDTOBEDETERMINED|ERR: getsocketopt(SO_ORIGINAL_DST) failed: %v"</span><span class="p">,</span> <span class="n">srcipport</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>
    <span class="n">newConn</span><span class="p">,</span> <span class="n">err</span> <span class="o">:=</span> <span class="n">net</span><span class="o">.</span><span class="n">FileConn</span><span class="p">(</span><span class="n">clientConnFile</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">err</span> <span class="o">!=</span> <span class="no">nil</span> <span class="p">{</span>
        <span class="n">log</span><span class="o">.</span><span class="n">Printf</span><span class="p">(</span><span class="s">"GETORIGINALDST|%v-&gt;?-&gt;%v|ERR: could not create a FileConn fron clientConnFile=%+v: %v"</span><span class="p">,</span> <span class="n">srcipport</span><span class="p">,</span> <span class="n">addr</span><span class="p">,</span> <span class="n">clientConnFile</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">_</span><span class="p">,</span> <span class="n">ok</span> <span class="o">:=</span> <span class="n">newConn</span><span class="o">.</span><span class="p">(</span><span class="o">*</span><span class="n">net</span><span class="o">.</span><span class="n">TCPConn</span><span class="p">);</span> <span class="n">ok</span> <span class="p">{</span>
        <span class="n">newTCPConn</span> <span class="o">=</span> <span class="n">newConn</span><span class="o">.</span><span class="p">(</span><span class="o">*</span><span class="n">net</span><span class="o">.</span><span class="n">TCPConn</span><span class="p">)</span>
        <span class="n">clientConnFile</span><span class="o">.</span><span class="n">Close</span><span class="p">()</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">errmsg</span> <span class="o">:=</span> <span class="n">fmt</span><span class="o">.</span><span class="n">Sprintf</span><span class="p">(</span><span class="s">"ERR: newConn is not a *net.TCPConn, instead it is: %T (%v)"</span><span class="p">,</span> <span class="n">newConn</span><span class="p">,</span> <span class="n">newConn</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">Printf</span><span class="p">(</span><span class="s">"GETORIGINALDST|%v-&gt;?-&gt;%v|%s"</span><span class="p">,</span> <span class="n">srcipport</span><span class="p">,</span> <span class="n">addr</span><span class="p">,</span> <span class="n">errmsg</span><span class="p">)</span>
        <span class="n">err</span> <span class="o">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">New</span><span class="p">(</span><span class="n">errmsg</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>

    <span class="n">ipv4</span> <span class="o">=</span> <span class="n">itod</span><span class="p">(</span><span class="kt">uint</span><span class="p">(</span><span class="n">addr</span><span class="o">.</span><span class="n">Multiaddr</span><span class="p">[</span><span class="m">4</span><span class="p">]))</span> <span class="o">+</span> <span class="s">"."</span> <span class="o">+</span>
        <span class="n">itod</span><span class="p">(</span><span class="kt">uint</span><span class="p">(</span><span class="n">addr</span><span class="o">.</span><span class="n">Multiaddr</span><span class="p">[</span><span class="m">5</span><span class="p">]))</span> <span class="o">+</span> <span class="s">"."</span> <span class="o">+</span>
        <span class="n">itod</span><span class="p">(</span><span class="kt">uint</span><span class="p">(</span><span class="n">addr</span><span class="o">.</span><span class="n">Multiaddr</span><span class="p">[</span><span class="m">6</span><span class="p">]))</span> <span class="o">+</span> <span class="s">"."</span> <span class="o">+</span>
        <span class="n">itod</span><span class="p">(</span><span class="kt">uint</span><span class="p">(</span><span class="n">addr</span><span class="o">.</span><span class="n">Multiaddr</span><span class="p">[</span><span class="m">7</span><span class="p">]))</span>
    <span class="n">port</span> <span class="o">=</span> <span class="kt">uint16</span><span class="p">(</span><span class="n">addr</span><span class="o">.</span><span class="n">Multiaddr</span><span class="p">[</span><span class="m">2</span><span class="p">])</span><span class="o">&lt;&lt;</span><span class="m">8</span> <span class="o">+</span> <span class="kt">uint16</span><span class="p">(</span><span class="n">addr</span><span class="o">.</span><span class="n">Multiaddr</span><span class="p">[</span><span class="m">3</span><span class="p">])</span>

    <span class="k">return</span>
<span class="p">}</span>
</code></pre></div></div>

<p>After we get the original destination address and port we start a connection to it and copy all the packets.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">var</span> <span class="n">streamWait</span> <span class="n">sync</span><span class="o">.</span><span class="n">WaitGroup</span>
<span class="n">streamWait</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span><span class="m">2</span><span class="p">)</span>

<span class="n">streamConn</span> <span class="o">:=</span> <span class="k">func</span><span class="p">(</span><span class="n">dst</span> <span class="n">io</span><span class="o">.</span><span class="n">Writer</span><span class="p">,</span> <span class="n">src</span> <span class="n">io</span><span class="o">.</span><span class="n">Reader</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">io</span><span class="o">.</span><span class="n">Copy</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="n">streamWait</span><span class="o">.</span><span class="n">Done</span><span class="p">()</span>
<span class="p">}</span>

<span class="k">go</span> <span class="n">streamConn</span><span class="p">(</span><span class="n">remoteConn</span><span class="p">,</span> <span class="n">VMconn</span><span class="p">)</span>
<span class="k">go</span> <span class="n">streamConn</span><span class="p">(</span><span class="n">VMconn</span><span class="p">,</span> <span class="n">remoteConn</span><span class="p">)</span>

<span class="n">streamWait</span><span class="o">.</span><span class="n">Wait</span><span class="p">()</span>
</code></pre></div></div>

<p>The Haproxy help us with the ingress traffic with the follow configuration</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>defaults
  mode tcp
frontend main
  bind *:9080
  default_backend guest
backend guest
  server guest 10.0.1.2:9080 maxconn 2048
</code></pre></div></div>

<p>It sends all the traffic to our virtual machine on the service port the machine is listening.</p>

<p><a href="https://github.com/SchSeba/NatProxy">Code repository</a></p>

<h3 id="nat-proxy-conclusions">nat proxy conclusions</h3>

<p>This solution is a general solution, not a dedicated solution to istio only. Its make the vm traffic look like a regular process inside the pod so it will work with any sidecars projects</p>

<p>Egress data flow in this solution:</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/nat-proxy-egress-traffic.png" alt="nat-proxy-egress-traffic" /></p>

<p>Ingress data flow in this solution:</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/nat-proxy-ingress.png" alt="nat-proxy-ingress-traffic" /></p>

<p>Pros:</p>

<ul>
  <li>No external modules needed</li>
  <li>Works with any sidecar solution</li>
</ul>

<p>Cons:</p>

<ul>
  <li>Not other process can change the iptables rules</li>
  <li>External process needed</li>
  <li>The traffic is passed to user space</li>
  <li>Only support ingress TCP connection</li>
</ul>

<h2 id="iptables-with-a-trasperent-proxy-process">Iptables with a trasperent-proxy process</h2>

<p>This is the last solution I used in my research, it use a kernel module named TPROXY The <a href="https://www.kernel.org/doc/Documentation/networking/tproxy.txt">official documentation</a> from the linux kernel documentation.</p>

<p>For this solution I created the following architecture</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/semi-tproxy-diagram.png" alt="semi-tproxy-Diagram" /></p>

<p>With the follow yaml configuration</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">application-devel</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">9080</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>

<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">libvirtd-client-devel</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">16509</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">client-connection</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">5900</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">spice</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">22</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">ssh</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">strategy</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">annotations</span><span class="pi">:</span>
        <span class="na">sidecar.istio.io/status</span><span class="pi">:</span> <span class="s1">'</span><span class="s">{"version":"43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78","initContainers":["istio-init","enable-core-dump"],"containers":["istio-proxy"],"volumes":["istio-envoy","istio-certs"]}'</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">libvirtd-devel</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/sebassch/mylibvirtd:devel</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">compute</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">9080</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">16509</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">5900</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">22</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">capabilities</span><span class="pi">:</span>
              <span class="na">add</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">ALL</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">runAsUser</span><span class="pi">:</span> <span class="m">0</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/lib/libvirt/images</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">test-volume</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/host-dev</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">host-dev</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/host-sys</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">host-sys</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">LIBVIRTD_DEFAULT_NETWORK_DEVICE</span>
              <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">eth0"</span>
        <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/sebassch/mytproxy:devel</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">proxy</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">capabilities</span><span class="pi">:</span>
              <span class="na">add</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">NET_ADMIN</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">proxy</span>
            <span class="pi">-</span> <span class="s">sidecar</span>
            <span class="pi">-</span> <span class="s">--configPath</span>
            <span class="pi">-</span> <span class="s">/etc/istio/proxy</span>
            <span class="pi">-</span> <span class="s">--binaryPath</span>
            <span class="pi">-</span> <span class="s">/usr/local/bin/envoy</span>
            <span class="pi">-</span> <span class="s">--serviceCluster</span>
            <span class="pi">-</span> <span class="s">productpage</span>
            <span class="pi">-</span> <span class="s">--drainDuration</span>
            <span class="pi">-</span> <span class="s">45s</span>
            <span class="pi">-</span> <span class="s">--parentShutdownDuration</span>
            <span class="pi">-</span> <span class="s">1m0s</span>
            <span class="pi">-</span> <span class="s">--discoveryAddress</span>
            <span class="pi">-</span> <span class="s">istio-pilot.istio-system:15005</span>
            <span class="pi">-</span> <span class="s">--discoveryRefreshDelay</span>
            <span class="pi">-</span> <span class="s">1s</span>
            <span class="pi">-</span> <span class="s">--zipkinAddress</span>
            <span class="pi">-</span> <span class="s">zipkin.istio-system:9411</span>
            <span class="pi">-</span> <span class="s">--connectTimeout</span>
            <span class="pi">-</span> <span class="s">10s</span>
            <span class="pi">-</span> <span class="s">--statsdUdpAddress</span>
            <span class="pi">-</span> <span class="s">istio-mixer.istio-system:9125</span>
            <span class="pi">-</span> <span class="s">--proxyAdminPort</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">15000"</span>
            <span class="pi">-</span> <span class="s">--controlPlaneAuthPolicy</span>
            <span class="pi">-</span> <span class="s">MUTUAL_TLS</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POD_NAME</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">metadata.name</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POD_NAMESPACE</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">metadata.namespace</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">INSTANCE_IP</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">fieldRef</span><span class="pi">:</span>
                  <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">status.podIP</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/istio/proxy:0.7.1</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-proxy</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">false</span>
            <span class="na">readOnlyRootFilesystem</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">runAsUser</span><span class="pi">:</span> <span class="m">1337</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/etc/istio/proxy</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">istio-envoy</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/etc/certs/</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">istio-certs</span>
              <span class="na">readOnly</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">initContainers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">-p</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">15001"</span>
            <span class="pi">-</span> <span class="s">-u</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">1337"</span>
            <span class="pi">-</span> <span class="s">-i</span>
            <span class="pi">-</span> <span class="s">10.96.0.0/12,192.168.0.0/16</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/istio/proxy_init:0.7.1</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-init</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">capabilities</span><span class="pi">:</span>
              <span class="na">add</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">NET_ADMIN</span>
        <span class="pi">-</span> <span class="na">args</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">-c</span>
            <span class="pi">-</span> <span class="s">sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;&amp; ulimit -c</span>
              <span class="s">unlimited</span>
          <span class="na">command</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">/bin/sh</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">alpine</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">enable-core-dump</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">emptyDir</span><span class="pi">:</span>
            <span class="na">medium</span><span class="pi">:</span> <span class="s">Memory</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">istio-envoy</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">istio-certs</span>
          <span class="na">secret</span><span class="pi">:</span>
            <span class="na">optional</span><span class="pi">:</span> <span class="no">true</span>
            <span class="na">secretName</span><span class="pi">:</span> <span class="s">istio.default</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">host-dev</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/dev</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">host-sys</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/sys</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">test-volume</span>
          <span class="na">hostPath</span><span class="pi">:</span>
            <span class="c1"># directory location on host</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/bricks/brick1/volume/Images</span>
            <span class="c1"># this field is optional</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">Directory</span>
<span class="na">status</span><span class="pi">:</span> <span class="pi">{}</span>

<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">gateway-devel</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">kubernetes.io/ingress.class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">istio"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/devel-myvm</span>
            <span class="na">backend</span><span class="pi">:</span>
              <span class="na">serviceName</span><span class="pi">:</span> <span class="s">application-devel</span>
              <span class="na">servicePort</span><span class="pi">:</span> <span class="m">9080</span>
</code></pre></div></div>

<p>When the tproxy container starts it runs an entry point script for iptables configuration but this time the proxy redirect came in the mangle table and not in the nat table that because TPROXY module avilable only in the mangle table.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TPROXY
This target is only valid in the mangle table, in the
PREROUTING chain and user-defined chains which are only
called from this chain.  It redirects the packet to a local
socket without changing the packet header in any way. It can
also change the mark value which can then be used in
advanced routing rules.
</code></pre></div></div>

<p>iptables rules:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>iptables <span class="nt">-t</span> mangle <span class="nt">-vL</span>
iptables <span class="nt">-t</span> mangle <span class="nt">-N</span> KUBEVIRT_DIVERT
iptables <span class="nt">-t</span> mangle <span class="nt">-A</span> KUBEVIRT_DIVERT <span class="nt">-j</span> MARK <span class="nt">--set-mark</span> 8
iptables <span class="nt">-t</span> mangle <span class="nt">-A</span> KUBEVIRT_DIVERT <span class="nt">-j</span> ACCEPT

<span class="nv">table</span><span class="o">=</span>mangle
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-N</span> KUBEVIRT_INBOUND
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-A</span> PREROUTING <span class="nt">-p</span> tcp <span class="nt">-m</span> comment <span class="nt">--comment</span> <span class="s2">"KubeVirt Spice"</span>  <span class="nt">--dport</span> 5900 <span class="nt">-j</span> RETURN
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-A</span> PREROUTING <span class="nt">-p</span> tcp <span class="nt">-m</span> comment <span class="nt">--comment</span> <span class="s2">"KubeVirt virt-manager"</span>  <span class="nt">--dport</span> 16509 <span class="nt">-j</span> RETURN
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-A</span> PREROUTING <span class="nt">-p</span> tcp <span class="nt">-i</span> vnet0 <span class="nt">-j</span> KUBEVIRT_INBOUND

iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-N</span> KUBEVIRT_TPROXY
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-A</span> KUBEVIRT_TPROXY <span class="o">!</span> <span class="nt">-d</span> 127.0.0.1/32 <span class="nt">-p</span> tcp <span class="nt">-j</span> TPROXY <span class="nt">--tproxy-mark</span> 8/0xffffffff <span class="nt">--on-port</span> 9401
<span class="c">#iptables -t mangle -A KUBEVIRT_TPROXY ! -d 127.0.0.1/32 -p udp -j TPROXY --tproxy-mark 8/0xffffffff --on-port 8080</span>

<span class="c"># If an inbound packet belongs to an established socket, route it to the</span>
<span class="c"># loopback interface.</span>
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-A</span> KUBEVIRT_INBOUND <span class="nt">-p</span> tcp <span class="nt">-m</span> socket <span class="nt">-j</span> KUBEVIRT_DIVERT
<span class="c">#iptables -t mangle -A KUBEVIRT_INBOUND -p udp -m socket -j KUBEVIRT_DIVERT</span>

<span class="c"># Otherwise, it's a new connection. Redirect it using TPROXY.</span>
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-A</span> KUBEVIRT_INBOUND <span class="nt">-p</span> tcp <span class="nt">-j</span> KUBEVIRT_TPROXY
<span class="c">#iptables -t mangle -A KUBEVIRT_INBOUND -p udp -j KUBEVIRT_TPROXY</span>
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-I</span> OUTPUT 1 <span class="nt">-d</span> 10.0.1.2 <span class="nt">-j</span> ACCEPT

<span class="nv">table</span><span class="o">=</span>nat
<span class="c"># Remove vm Connection from iptables rules</span>
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-I</span> PREROUTING 1 <span class="nt">-s</span> 10.0.1.2 <span class="nt">-j</span> ACCEPT
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-I</span> OUTPUT 1 <span class="nt">-d</span> 10.0.1.2 <span class="nt">-j</span> ACCEPT

<span class="c"># Allow guest -&gt; world -- using nat for UDP</span>
iptables <span class="nt">-t</span> <span class="k">${</span><span class="nv">table</span><span class="k">}</span> <span class="nt">-I</span> POSTROUTING 1 <span class="nt">-s</span> 10.0.1.2 <span class="nt">-p</span> udp <span class="nt">-j</span> MASQUERADE
</code></pre></div></div>

<p>For this solution we also need to load the bridge kernel module</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>modprobe bridge
</code></pre></div></div>

<p>And create some ebtables rules so egress and ingress traffict from the virtial machine will exit the l2 rules and pass to the l3 rules:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ebtables <span class="nt">-t</span> broute <span class="nt">-F</span> <span class="c"># Flush the table</span>
    <span class="c"># inbound traffic</span>
    ebtables <span class="nt">-t</span> broute <span class="nt">-A</span> BROUTING <span class="nt">-p</span> IPv4 <span class="nt">--ip-dst</span> 10.0.1.2 <span class="se">\</span>
    <span class="nt">-j</span> redirect <span class="nt">--redirect-target</span> DROP
    <span class="c"># returning outbound traffic</span>
    ebtables <span class="nt">-t</span> broute <span class="nt">-A</span> BROUTING <span class="nt">-p</span> IPv4 <span class="nt">--ip-src</span> 10.0.1.2 <span class="se">\</span>
    <span class="nt">-j</span> redirect <span class="nt">--redirect-target</span> DROP
</code></pre></div></div>

<p>We also need to disable rp_filter on the virtual machine interface and the libvirt bridge interface</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo </span>0 <span class="o">&gt;</span> /proc/sys/net/ipv4/conf/virbr0/rp_filter
<span class="nb">echo </span>0 <span class="o">&gt;</span> /proc/sys/net/ipv4/conf/virbr0-nic/rp_filter
<span class="nb">echo </span>0 <span class="o">&gt;</span> /proc/sys/net/ipv4/conf/vnet0/rp_filter
</code></pre></div></div>

<p>After this configuration the container start the semi-tproxy process for egress traffic and the haproxy process for the ingress traffic.</p>

<p>The semi-tproxy program is a golag program,binding a listener socket with the IP_TRANSPARENT socket option
Preparing a socket to receive connections with TProxy is really no different than what is normally done when setting up a socket to listen for connections. The only difference in the process is before the socket is bound, the IP_TRANSPARENT socket option.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">syscall</span><span class="o">.</span><span class="n">SetsockoptInt</span><span class="p">(</span><span class="n">fileDescriptor</span><span class="p">,</span> <span class="n">syscall</span><span class="o">.</span><span class="n">SOL_IP</span><span class="p">,</span> <span class="n">syscall</span><span class="o">.</span><span class="n">IP_TRANSPARENT</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
</code></pre></div></div>

<p>About IP_TRANSPARENT</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>IP_TRANSPARENT (since Linux 2.6.24)
Setting this boolean option enables transparent proxying on
this socket.  This socket option allows the calling applica‐
tion to bind to a nonlocal IP address and operate both as a
client and a server with the foreign address as the local
end‐point.  NOTE: this requires that routing be set up in
a way that packets going to the foreign address are routed
through the TProxy box (i.e., the system hosting the
application that employs the IP_TRANSPARENT socket option).
Enabling this socket option requires superuser privileges
(the CAP_NET_ADMIN capability).

TProxy redirection with the iptables TPROXY target also
requires that this option be set on the redirected socket.
</code></pre></div></div>

<p>Then we set the IP_TRANSPARENT socket option on outbound connections
Same goes for making connections to a remote host pretending to be the client, the IP_TRANSPARENT socket option is set and the Linux kernel will allow the bind so along as a connection was intercepted with those details being used for the bind.</p>

<p>When the process get a new connection we start a connection to the real destination address and copy the traffic between both sockets</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">var</span> <span class="n">streamWait</span> <span class="n">sync</span><span class="o">.</span><span class="n">WaitGroup</span>
<span class="n">streamWait</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span><span class="m">2</span><span class="p">)</span>

<span class="n">streamConn</span> <span class="o">:=</span> <span class="k">func</span><span class="p">(</span><span class="n">dst</span> <span class="n">io</span><span class="o">.</span><span class="n">Writer</span><span class="p">,</span> <span class="n">src</span> <span class="n">io</span><span class="o">.</span><span class="n">Reader</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">io</span><span class="o">.</span><span class="n">Copy</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="n">streamWait</span><span class="o">.</span><span class="n">Done</span><span class="p">()</span>
<span class="p">}</span>

<span class="k">go</span> <span class="n">streamConn</span><span class="p">(</span><span class="n">remoteConn</span><span class="p">,</span> <span class="n">VMconn</span><span class="p">)</span>
<span class="k">go</span> <span class="n">streamConn</span><span class="p">(</span><span class="n">VMconn</span><span class="p">,</span> <span class="n">remoteConn</span><span class="p">)</span>

<span class="n">streamWait</span><span class="o">.</span><span class="n">Wait</span><span class="p">()</span>
</code></pre></div></div>

<p>The Haproxy helps us with the ingress traffic with the follow configuration</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>defaults
  mode tcp
frontend main
  bind *:9080
  default_backend guest
backend guest
  server guest 10.0.1.2:9080 maxconn 2048
</code></pre></div></div>

<p>It sends all the traffic to our virtual machine on the service port the machine is listening.</p>

<p><a href="https://github.com/SchSeba/SemiTrasperentProxy">Code repository</a></p>

<h3 id="tproxy-conclusions">tproxy conclusions</h3>

<p>This solution is a general solution, not a dedicated solution to istio only. Its make the vm traffic look like a regular process inside the pod so it will work with any sidecars projects</p>

<p>Egress data flow in this solution:</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/semi-tproxy-egress.png" alt="tproxy-egress-traffic" /></p>

<p>Ingress data flow in this solution:</p>

<p><img src="../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/nat-proxy-ingress.png" alt="tproxy-ingress-traffic" /></p>

<p>Pros:</p>

<ul>
  <li>other process can change the nat table (this solution works on the mangle table)</li>
  <li>better preformance comparing to nat-proxy</li>
  <li>Works with any sidecar solution</li>
</ul>

<p>Cons:</p>

<ul>
  <li>Need NET_ADMIN capability for the docker</li>
  <li>External process needed</li>
  <li>The traffic is passed to user space</li>
  <li>Only support ingress TCP connection</li>
</ul>

<h1 id="research-conclustion">Research Conclustion</h1>

<p>KubeVirt shows it is possible to run virtual machines inside a kubernetes cluster, and this post shows that the virtual machine can also get the benefit of it.</p>]]></content><author><name>SchSeba</name></author><category term="uncategorized" /><category term="istio" /><category term="iptables" /><category term="libvirt" /><category term="tproxy" /><category term="service mesh" /><category term="ebtables" /><summary type="html"><![CDATA[In this post we will deploy a vm on top of kubernetes with istio service mesh]]></summary></entry><entry><title type="html">Use Vs Code For Kube Virt Development</title><link href="https://kubevirt.io//2018/Use-VS-Code-for-Kube-Virt-Development.html" rel="alternate" type="text/html" title="Use Vs Code For Kube Virt Development" /><published>2018-05-22T00:00:00+00:00</published><updated>2018-05-22T00:00:00+00:00</updated><id>https://kubevirt.io//2018/Use-VS-Code-for-Kube-Virt-Development</id><content type="html" xml:base="https://kubevirt.io//2018/Use-VS-Code-for-Kube-Virt-Development.html"><![CDATA[<p>In this post we will install and configure Visual Studio Code (vscode) for KubeVirt development and debug.</p>

<p>Visual Studio Code is a source code editor developed by Microsoft for Windows, Linux and macOS.</p>

<p>It includes support for debugging, embedded Git control, syntax highlighting, intelligent code completion, snippets, and code refactoring.</p>

<h1 id="golang-installation">Golang Installation</h1>

<p>GO installation is required, We can find the binaries in <a href="https://golang.org/dl/">golang page</a>.</p>

<h2 id="golang-linux-installation">Golang Linux Installation</h2>

<p>After downloading the binaries extract them with the following command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gz
</code></pre></div></div>

<p>Now let’s Add /usr/local/go/bin to the PATH environment variable.</p>

<p>You can do this by adding this line to your /etc/profile (for a system-wide installation) or $HOME/.profile:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export PATH=$PATH:/usr/local/go/bin
</code></pre></div></div>

<h2 id="golang-windows-installation">Golang Windows Installation</h2>

<p>Open the MSI file and follow the prompts to install the Go tools.</p>

<p>By default, the installer puts the Go distribution in C:\Go.</p>

<p>The installer should put the C:\Go\bin directory in your PATH environment variable.</p>

<p>You may need to restart any open command prompts for the change to take effect.</p>

<h1 id="vscode-installation">VSCODE Installation</h1>

<p>Now we will install Visual Studio Code in our system.</p>

<h2 id="for-linux-machines">For linux machines</h2>

<p>We need to choose our linux distribution.</p>

<h3 id="for-rhelcentosfedora">For RHEL/Centos/Fedora</h3>

<p>The following script will install the key and repository:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc
sudo sh -c 'echo -e "[code]\nname=Visual Studio Code\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\nenabled=1\ngpgcheck=1\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc" &gt; /etc/yum.repos.d/vscode.repo'
</code></pre></div></div>

<p>Then update the package cache and install the package using dnf (Fedora 22 and above):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dnf check-update
sudo dnf install code
</code></pre></div></div>

<p>Or on older versions using yum:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yum check-update
sudo yum install code
</code></pre></div></div>

<h3 id="for-debianubuntu">For Debian/Ubuntu</h3>

<p>We need to download the .deb package from the <a href="https://code.visualstudio.com/Download">vscode download page</a>,
and from the command line run the package management.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo dpkg -i &lt;file&gt;.deb
sudo apt-get install -f # Install dependencies
</code></pre></div></div>

<h1 id="for-windows-machines">For Windows machines</h1>

<p>Download the <a href="https://go.microsoft.com/fwlink/?LinkID=534107">Visual Studio Code installer</a>, and then run the installer (VSCodeSetup-version.exe)</p>

<h1 id="go-project-struct">Go Project struct</h1>

<p>Let’s create the following structure for our kubevirt project development environment:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── &lt;Go-projects-folder&gt; # Your Golang projects root folder
│   ├── bin
│   ├── pkg
│   ├── src
│   │   ├── kubevirt.io
</code></pre></div></div>

<p>Now navigate to kubevirt.io folder and run:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone &lt;kubevirt-fork&gt;
</code></pre></div></div>

<h1 id="install-vscode-extensions">Install VSCODE Extensions</h1>

<p>Now we are going to install some extensions for a better development experience with the IDE.</p>

<p>Open vscode and select your go project root folder you created in the last step.</p>

<p>On the extensions tab (Ctrl+Shift+X), search for golang and install it.</p>

<p>Now open the command palette (Ctrl+Shift+P) view-&gt;Command Palette and type “Go: install/update tools”, this will install all the requirements for example: delve debugger, etc…</p>

<p>(optional) We can install docker extension for syntax highlighting, commands, etc..</p>

<p><img src="../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/extension-install.png" alt="Extension-Install" /></p>

<h1 id="gopath-and-goroot-configuration">GOPATH and GOROOT configuration</h1>

<p>Open the vscode configuration file (<code class="language-plaintext highlighter-rouge">ctrl+,</code>) file-&gt;preferences-&gt;settings.</p>

<p>Now on the right file we need to add this configuration:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"go.gopath": "&lt;Go-projects-folder&gt;",
"go.goroot": "/usr/local/go",
</code></pre></div></div>

<p><img src="../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/settings.png" alt="settings" /></p>

<h1 id="create-debug-configuration">Create debug configuration</h1>

<p>For the last part we are going to configure the debugger file, open it by Debug-&gt;Open Configurations and add to the configuration list the following structure</p>

<p>** Change the <Go-projects-folder> parameter to your golang projects root directory</Go-projects-folder></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">{</span>
  <span class="s2">"</span><span class="s">name"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Kubevirt"</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">type"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">go"</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">request"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">launch"</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">mode"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">debug"</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">remotePath"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">port"</span><span class="pi">:</span> <span class="nv">2345</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">host"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">127.0.0.1"</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">program"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">${fileDirname}"</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">env"</span><span class="pi">:</span> <span class="pi">{},</span>
  <span class="s2">"</span><span class="s">args"</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">--kubeconfig"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">cluster/k8s-1.9.3/.kubeconfig"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">--port"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">1234"</span><span class="pi">],</span>
  <span class="s2">"</span><span class="s">showLog"</span><span class="pi">:</span> <span class="nv">true</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">cwd"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">${workspaceFolder}/src/kubevirt.io/kubevirt"</span><span class="pi">,</span>
  <span class="s2">"</span><span class="s">output"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">&lt;Go-projects-folder&gt;/bin/${fileBasenameNoExtension}"</span><span class="pi">,</span>
<span class="pi">}</span>
</code></pre></div></div>

<p><img src="../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/debug-config.png" alt="debug-config" /></p>

<h1 id="debug-process">Debug Process</h1>

<p>For debug we need to open the main package we want to debug.</p>

<p>For example if we want to debug the virt-api component, open the main package:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubevirt.io/cmd/virt-api/virt-api.go
</code></pre></div></div>

<p><img src="../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/debug-file.png" alt="debug-file" /></p>

<p>Now change to debug view (<code class="language-plaintext highlighter-rouge">ctrl+shift+D</code>), check that we are using the kubevirt configuration and hit the play button</p>

<p><img src="../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/debug-view.png" alt="debug-view" /></p>

<h2 id="more-information">More Information</h2>

<p>For more information, keyboard shortcuts and advance vscode usage please refer the following link</p>

<p><a href="https://code.visualstudio.com/docs/editor/codebasics">editor code basics</a></p>]]></content><author><name>SchSeba</name></author><category term="uncategorized" /><category term="vscode" /><category term="development" /><category term="debug" /><summary type="html"><![CDATA[In this post we will install and configure vscode for KubeVirt development]]></summary></entry><entry><title type="html">Kubevirt Api Access Control</title><link href="https://kubevirt.io//2018/KubeVirt-API-Access-Control.html" rel="alternate" type="text/html" title="Kubevirt Api Access Control" /><published>2018-05-16T00:00:00+00:00</published><updated>2018-05-16T00:00:00+00:00</updated><id>https://kubevirt.io//2018/KubeVirt-API-Access-Control</id><content type="html" xml:base="https://kubevirt.io//2018/KubeVirt-API-Access-Control.html"><![CDATA[<p>Access to KubeVirt resources are controlled entirely by Kubernete’s Resource
Based Access Control (RBAC) system. This system allows KubeVirt to tie directly
into the existing authentication and authorization mechanisms Kubernetes
already provides to its core api objects.</p>

<h2 id="kubevirt-rbac-role-basics">KubeVirt RBAC Role Basics</h2>

<p>Typically, when people think of Kubernetes RBAC system, they’re thinking about
granting users access to create/delete kubernetes objects (like Pods,
deployments, etc), however those same RBAC mechanisms work naturally with
KubeVirt objects as well.</p>

<p>When we look at KubeVirt’s objects, we can see they are structured just like
the objects that come predefined in the Kubernetes core.</p>

<p>For example, look here’s an example of a VirtualMachine spec.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: kubevirt.io/v1alpha1
kind: VirtualMachine
metadata:
  name: vm-ephemeral
spec:
  domain:
    devices:
      disks:
      - disk:
          bus: virtio
        name: registrydisk
        volumeName: registryvolume
    resources:
      requests:
        memory: 64M
  volumes:
  - name: registryvolume
    registryDisk:
      image: kubevirt/cirros-container-disk-demo:devel
</code></pre></div></div>

<p>In the spec above, we see the KubeVirt VirtualMachine object has an <em>apiVersion</em>
field and a <em>kind</em> field just like a Pod spec does. The <strong>kubevirt.io</strong> portion
of the apiVersion field represents KubeVirt apiGroup the resource is a part of.
The <strong>kind</strong> field reflects the resource type.</p>

<p>Using that information, we can create an RBAC role that gives a user permission
to create, delete, and view all VirtualMachine objects.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: vm-access
  labels:
    kubevirt.io: ""
rules:
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
</code></pre></div></div>

<p>This same logic can be applied when creating RBAC roles for other KubeVirt
objects as well. If we wanted to extend this RBAC role to grant similar
permissions for VirtualMachinePreset objects, we’d just have to add a second
resource kubevirt.io resource list. The result would look like this.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: vm-access
  labels:
    kubevirt.io: ""
rules:
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
      - virtualmachinepresets
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
</code></pre></div></div>

<h2 id="kubevirt-subresource-rbac-roles">KubeVirt Subresource RBAC Roles</h2>

<p>Access to a VirtualMachines’s VNC and console stream using KubeVirt’s
<strong>virtctl</strong> tool is managed by the Kubernetes RBAC system as well. Permissions
for these resources work slightly different than the other KubeVirt objects
though.</p>

<p>Console and VNC access is performed using the KubeVirt Stream API, which has
its own api group called <strong>subresources.kubevirt.io</strong>. Below is an example of
how to create a role that grants access to the VNC and console streams APIs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: vm-vnc-access
  labels:
    kubevirt.io: ""
rules:
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/console
      - virtualmachines/vnc
    verbs:
      - get
</code></pre></div></div>

<h2 id="limiting-rbac-to-a-single-namespace">Limiting RBAC To a Single Namespace.</h2>

<p>A ClusterRole can be bound to a user in two different ways.</p>

<p>When a ClusterRoleBinding is used, a user is permitted access to all resources
defined in the ClusterRole across all namespaces in the cluster.</p>

<p>When a RoleBinding is used, a user is limited to accessing only the resources
defined in the ClusterRole within the namespace RoleBinding exists in.</p>

<h2 id="limiting-rbac-to-a-single-resource">Limiting RBAC To a Single Resource.</h2>

<p>A user can also be limit to accessing only a single resource within a resource
type. Below is an example that only grants VNC access to the VirtualMachine
named ‘bobs-vm’</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: vm-vnc-access
  labels:
    kubevirt.io: ""
rules:
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/console
      - virtualmachines/vnc
    resourceName:
      - bobs-vm
    verbs:
      - get
</code></pre></div></div>

<h2 id="default-kubevirt-rbac-roles">Default KubeVirt RBAC Roles</h2>

<p>The next release of KubeVirt is coming with three default ClusterRoles that
admins can use to grant users access to KubeVirt resources. In most cases,
these roles will prevent admins from ever having to create their own custom
KubeVirt RBAC roles.</p>

<p>More information about these default roles can be found in the KubeVirt
user guide <a href="https://kubevirt.io/user-guide/operations/authorization/">here</a></p>]]></content><author><name>davidvossel</name></author><category term="uncategorized" /><category term="api" /><category term="rbac" /><category term="roles" /><summary type="html"><![CDATA[How User Access Control works in KubeVirt]]></summary></entry><entry><title type="html">Use Glusterfs Cloning With Kubevirt</title><link href="https://kubevirt.io//2018/Use-GlusterFS-Cloning-with-KubeVirt.html" rel="alternate" type="text/html" title="Use Glusterfs Cloning With Kubevirt" /><published>2018-05-16T00:00:00+00:00</published><updated>2018-05-16T00:00:00+00:00</updated><id>https://kubevirt.io//2018/Use-GlusterFS-Cloning-with-KubeVirt</id><content type="html" xml:base="https://kubevirt.io//2018/Use-GlusterFS-Cloning-with-KubeVirt.html"><![CDATA[<p>Gluster seems like a good fit for storage in kubernetes and in particular in kubevirt. Still, as for other storage backends, we will likely need to use a golden set of images and deploy vms from them.</p>

<p>That’s where cloning feature of gluster comes at rescue!</p>

<h2 id="contents">Contents</h2>

<ul>
  <li>Prerequisites</li>
  <li>Installing Gluster provisioner</li>
  <li>Using The cloning feature</li>
  <li>Conclusion</li>
</ul>

<h2 id="prerequisites">Prerequisites</h2>

<p>I assume you already have a running instance of openshift and kubevirt along with gluster and an already existing pvc where you copied a base operating system ( you can get those from <a href="https://docs.openstack.org/image-guide/obtain-images.html">here</a>)</p>

<p>For reference, I used the following components and versions:</p>

<ul>
  <li>3 baremetal servers with Rhel 7.4 as base OS</li>
  <li>OpenShift and CNS 3.9</li>
  <li>KubeVirt latest</li>
</ul>

<h2 id="installing-gluster-provisioner">Installing Gluster provisioner</h2>

<h3 id="initial-deployment">initial deployment</h3>

<p>We will deploy the custom provisioner using <a href="../assets/2018-05-16-use-glustercloning-with-kubevirt/glusterfile-provisioner-template.yml">this template</a>, along with cluster rules located in <a href="../assets/2018-05-16-use-glustercloning-with-kubevirt/openshift-clusterrole.yaml">this file</a></p>

<p>Note that we also patch the image to use an existing one from gluster org located at docker.io instead of quay.io, as the corresponding repository is private by the time of this writing, and the heketi one, to make sure it has the code required to handle cloning</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAMESPACE="app-storage"
oc create -f openshift-clusterrole.yaml
oc process -f glusterfile-provisioner-template.yml | oc apply -f - -n $NAMESPACE
oc adm policy add-cluster-role-to-user cluster-admin -z glusterfile-provisioner -n $NAMESPACE
oc adm policy add-scc-to-user privileged -z glusterfile-provisioner
oc set image dc/heketi-storage heketi=gluster/heketiclone:latest  -n $NAMESPACE
oc set image dc/glusterfile-provisioner glusterfile-provisioner=gluster/glusterfileclone:latest  -n $NAMESPACE
</code></pre></div></div>

<p>And you will see something similar to this in your storage namespace</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master01 ~]# NAMESPACE="app-storage"
[root@master01 ~]# kubectl get pods -n $NAMESPACE
NAME                              READY     STATUS    RESTARTS   AGE
glusterfile-provisioner-3-vhkx6   1/1       Running   0          1d
glusterfs-storage-b82x4           1/1       Running   1          23d
glusterfs-storage-czthc           1/1       Running   0          23d
glusterfs-storage-z68hm           1/1       Running   0          23d
heketi-storage-2-qdrks            1/1       Running   0          6h
</code></pre></div></div>

<h3 id="additional-configuration">additional configuration</h3>

<p>for the custom provisioner to work, we need two additional things:</p>

<ul>
  <li>a storage class pointing to it, but also containing the details of the current heketi installation</li>
  <li>a secret similar to the one used by the current heketi installation, but using a different <em>type</em></li>
</ul>

<p>You can use the following</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAMESPACE="app-storage"
oc get sc glusterfs-storage -o yaml
oc get secret heketi-storage-admin-secret -n $NAMESPACE-o yaml
</code></pre></div></div>

<p>then, create the following objects:</p>

<ul>
  <li>glustercloning-heketi-secret secret in your storage namespace</li>
  <li>glustercloning storage class</li>
</ul>

<p>for reference, here are samples of those files.</p>

<p>Note how we change the type for the secret and add extra options for our storage class (in particular, enabling smartclone).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: v1
data:
  key: eEt0NUJ4cklPSmpJb2RZcFpqVExSSjUveFV5WHI4L0NxcEtMME1WVlVjQT0=
kind: Secret
metadata:
  name: glustercloning-heketi-secret
  namespace: app-storage
type: gluster.org/glusterfile
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: glustercloning
parameters:
  restsecretname: glustercloning-heketi-secret
  restsecretnamespace: app-storage
  resturl: http://heketi-storage.192.168.122.10.xip.io
  restuser: admin
  smartclone: "true"
  snapfactor: "10"
  volumeoptions: group virt
provisioner: gluster.org/glusterfile
reclaimPolicy: Delete
</code></pre></div></div>

<p>The full set of supported parameters can be found <a href="https://github.com/kubernetes-incubator/external-storage/blob/master/gluster/file/README.md">here</a></p>

<h2 id="using-the-cloning-feature">Using the cloning feature</h2>

<p>Once deployed, you can now provision pvcs from a base origin</p>

<h3 id="cloning-single-pvcs">Cloning single pvcs</h3>

<p>For instance, provided you have an existing pvc named <em>cirros</em> containing this base operating system, and that this PVC contains an annotion of the following</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(...)
metadata:
 annotations:
  gluster.org/heketi-volume-id: f0cbbb29ef4202c5226f87708da57e5c
(...)
</code></pre></div></div>

<p>A cloned pvc can be created with the following yaml ( note that we simply indicate a clone request in the annotations)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: testclone1
  namespace: default
  annotations:
    k8s.io/CloneRequest: cirros
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: glustercloning
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 1Gi
</code></pre></div></div>

<p>Once provisioned, the pvc will contain this additional annotation created by the provisioner</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(...)
metadata:
 annotations:
      k8s.io/CloneOf: cirros

(...)
</code></pre></div></div>

<h3 id="leveraging-the-feature-in-openshift-templates">Leveraging the feature in openshift templates</h3>

<p>We can make direct use of the feature in <a href="../assets/2018-05-16-use-glustercloning-with-kubevirt/template.yml">this openshift template</a> which would create the following objects:</p>

<ul>
  <li>a persistent volume claim as a clone of an existing pvc (cirros by default)</li>
  <li>an offline virtual machine object</li>
  <li>additional services for ssh and http access</li>
</ul>

<p>you can use it with something like</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>oc process -f template.yml -p Name=myvm | oc process -f - -n default
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>Cloning features in the storage backend allow us to simply use a given set of pvcs as base OS for the deployment of our vms. This feature is growing in gluster, worth giving it a try!</p>]]></content><author><name>karmab</name></author><category term="uncategorized" /><category term="glusterfs" /><category term="storage" /><summary type="html"><![CDATA[A first look at using gluster cloning with kubevirt]]></summary></entry><entry><title type="html">Ovn Multi Network Plugin For Kubernetes Kubetron</title><link href="https://kubevirt.io//2018/ovn-multi-network-plugin-for-kubernetes-kubetron.html" rel="alternate" type="text/html" title="Ovn Multi Network Plugin For Kubernetes Kubetron" /><published>2018-05-16T00:00:00+00:00</published><updated>2018-05-16T00:00:00+00:00</updated><id>https://kubevirt.io//2018/ovn-multi-network-plugin-for-kubernetes-kubetron</id><content type="html" xml:base="https://kubevirt.io//2018/ovn-multi-network-plugin-for-kubernetes-kubetron.html"><![CDATA[<p>Kubernetes networking model is suited for containerized applications, based mostly around L4 and L7 services, where all pods are connected to one big network. This is perfectly ok for most use cases. However, sometimes there is a need for fine-grained network configuration with better control. Use-cases such as L2 networks, static IP addresses, interfaces dedicated for storage traffic etc. For such needs there is ongoing effort in Kubernetes sig-network to support multiple networks (see <a href="https://docs.google.com/document/d/1Ny03h6IDVy_e_vmElOqR7UdTPAG_RNydhVE1Kx54kFQ">Kubernetes Network CRD De-Facto Standard</a>. There exist many prototypes of plugins providing such functionality. You are reading about one of them.</p>

<p>Kubetron (working name, <code class="language-plaintext highlighter-rouge">kubernetes + neutron</code>, quite misleading since we want to support setup without Neutron involved too), allows users to connect their pods to multiple networks configured on OVN. Important part here is, that such networks are configured by an external tool, be it OVN Northbound Database client or higher level tool such as Neutron or oVirt Provider OVN. This allows administrators to configure complicated networks, Kubernetes then only knows enough about the known networks to be able to connect to them - but not all the complexity involved to manage them. Kubetron does not affect default Kubernetes networking at all, default networks will be left intact.</p>

<p>In order to enable the use-cases outlined above, Kubetron can be used to provide multiple interfaces to a pod, further more KubeVirt will then use those interfaces to pass them to its virtual machines via the in progress <a href="https://docs.google.com/document/d/10rXr91aqn8MvVcLgHw33WX8BaQwHPZERp25PHxoZGgw/edit?usp=sharing">VirtualMachine networking API</a>.</p>

<p>You can find source code in <a href="https://github.com/phoracek/kubetron">Kubetron GitHub repository</a>.</p>

<h2 id="contents">Contents</h2>

<ul>
  <li>Desired Model and Usage</li>
  <li>Proof of Concept</li>
  <li>Demo</li>
  <li>Try it Yourself</li>
  <li>Looking for Help</li>
  <li>Disclaimer</li>
</ul>

<h2 id="desired-model-and-usage">Desired Model and Usage</h2>

<p>Let’s talk about how Kubetron looks from administrator’s and user’s point of view. Please note that following examples are still for the desired state and some of them might not be implemented in PoC yet. If you want to learn more about deployment and architecture, check <a href="https://docs.google.com/presentation/d/1KiHQyZngdaL8gtreL9Tmy7S1XiY5Sbnn0YuNCqhggF8/edit?usp=sharing">Kubetron slide deck</a>.</p>

<h3 id="configure-ovn-networks">Configure OVN Networks</h3>

<p>First of all, administrator must create and configure networks in OVN. That could be done either directly on OVN northbound database (e.g. using <code class="language-plaintext highlighter-rouge">ovn-nbctl</code>) or via OVN manager (e.g. Neutron or oVirt Provider OVN, using ansible).</p>

<h3 id="expose-available-networks">Expose Available Networks</h3>

<p>Once the networks are configured, there are two options how to expose available networks to a user. First one is providing some form of access to OVN or Neutron API, this one is completely out of Kubernetes’ and Kubetron’s
scope. Second option is to enable Network object support (as described in Kubernetes Network CRD De-Facto standard). With this option, administrator must create a Network object per each OVN network is allowed to be used by a user. This object allows administrator to expose only limited subset of networks or to limit access per Namespace. This process could be automated, e.g. via a service that monitors available logical switches and exposes them as Networks.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List networks (Logical Switches) directly from OVN Northbound database</span>
ovn-nbctl ls-list

<span class="c"># List networks available on Neutron</span>
neutron net-list

<span class="c"># List networks as Network objects created in Kubernetes</span>
kubectl get networks
</code></pre></div></div>

<h3 id="attach-pod-to-a-network">Attach pod to a Network</h3>

<p>Once user selects a desired network based on options described in previous section, he or she can request them for a pod using an annotation. This annotation is compatible with earlier mentioned Kubernetes Network CRD De-Facto Standard.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">network-consumer</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">kubernetes.v1.cni.cncf.io/networks</span><span class="pi">:</span> <span class="s">red</span> <span class="c1"># requested networks</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">busybox</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
</code></pre></div></div>

<h3 id="access-the-network-from-the-pod">Access the Network from the pod</h3>

<p>Once the pod is created, a user can list its interfaces and their assigned IP addresses:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> network-consumer <span class="nt">--</span> ip address
...
10: red-bcxoeffrsw: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1442 qdisc noqueue state UNKNOWN qlen 1000
    <span class="nb">link</span>/ether 4e:71:3b:ee:a5:f4 brd ff:ff:ff:ff:ff:ff
    inet 10.1.0.3/24 brd 10.1.0.255 scope global dynamic red-bcxoeffrsw
       valid_lft 86371sec preferred_lft 86371sec
    inet6 fe80::4c71:3bff:feee:a5f4/64 scope <span class="nb">link
       </span>valid_lft forever preferred_lft forever
...
</code></pre></div></div>

<p>In order to make it easier to obtain the network’s interface name inside pod’s containers, environment variables with network-interface mapping are created:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NETWORK_INTERFACE_RED</span>
red-bcxoeffrsw
</code></pre></div></div>

<h2 id="proof-of-concept">Proof of Concept</h2>

<p>As for now, current implementation does not completely implement the desired model yet:</p>

<ul>
  <li>Only Neutron mode is implemented, Kubetron can not be used with OVN alone</li>
  <li>Network object handling is not implemented, Kubetron obtains networks directly from Neutron</li>
  <li>Interface names are not exposed as environment variables</li>
</ul>

<p>It might be unstable and there are some missing parts. However, basic scenario works, at least in development environment.</p>

<h2 id="demo">Demo</h2>

<p>In the following recording we create two networks <code class="language-plaintext highlighter-rouge">red</code> and <code class="language-plaintext highlighter-rouge">blue</code> using Neutron API via Ansible. Then we create two pods and connect them to both mentioned networks. And then we <code class="language-plaintext highlighter-rouge">ping</code>.</p>

<p><a href="https://asciinema.org/a/7nB3vgIJcz05TxRNiaD2vLLdE"><img src="https://asciinema.org/a/7nB3vgIJcz05TxRNiaD2vLLdE.png" alt="asciicast" /></a></p>

<h2 id="try-it-yourself">Try it Yourself</h2>

<p>I encourage you to try Kubetron yourself. It has not yet been tested on regular Kubernetes deployment (and it likely won’t work without some tuning). Fortunately, Kubetron repository contains Vagrant file and set of scripts that will help you deploy multi-node Kubernetes
with OVN and Kubetron installed. On top of that it describes how to create networks and connect pods to them. Check out <a href="https://github.com/phoracek/kubetron/blob/master/README.md">Kubetron README.md</a> and give it a try!</p>

<h2 id="looking-for-help">Looking for Help</h2>

<p>If you are interested in contributing to Kubetron, please follow its GitHub repository. There are many missing features and possible improvements, I will open issues to track them soon. Stay tuned!</p>

<h2 id="disclaimer">Disclaimer</h2>

<p>Kubetron is in early development stage, both it’s architecture and tools to use it will change.</p>]]></content><author><name>phoracek</name></author><category term="uncategorized" /><category term="ovn" /><category term="kubetron" /><category term="network" /><category term="neutron" /><summary type="html"><![CDATA[OVN Multi-Network Plugin for Kubernetes, Kubetron]]></summary></entry><entry><title type="html">Kubevirt Objects</title><link href="https://kubevirt.io//2018/KubeVirt-objects.html" rel="alternate" type="text/html" title="Kubevirt Objects" /><published>2018-05-08T00:00:00+00:00</published><updated>2018-05-08T00:00:00+00:00</updated><id>https://kubevirt.io//2018/KubeVirt-objects</id><content type="html" xml:base="https://kubevirt.io//2018/KubeVirt-objects.html"><![CDATA[<p>The <a href="https://github.com/kubevirt/kubevirt/">KubeVirt</a> project provides extensions to Kubernetes via <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">custom resources</a>.</p>

<p>These resources are a collection a API objects that defines a virtual machine within Kubernetes.</p>

<p>I think it’s important to point out the two great resources that I used to
compile information for this post:</p>

<ul>
  <li><a href="http://kubevirt.io/user-guide/">user-guide</a></li>
  <li><a href="http://kubevirt.io/api-reference/">api-reference</a></li>
</ul>

<p>With that let’s take a look at the objects that are available.</p>

<h1 id="kubevirt-top-level-objects">KubeVirt top-level objects</h1>

<p>Below is a list of the top level API objects and descriptions that KubeVirt provides.</p>

<ul>
  <li>
    <p>VirtualMachine (vm[s]) - represents a virtual machine in the runtime environment of Kubernetes.</p>
  </li>
  <li>
    <p>OfflineVirtualMachine (ovm[s]) - handles the virtual machines that are not running or are in a stopped state.</p>
  </li>
  <li>
    <p>VirtualMachinePreset (vmpreset[s]) - is an extension to general VirtualMachine configuration behaving much like PodPresets from Kubernetes. When a VirtualMachine is created, any applicable VirtualMachinePresets will be applied to the existing spec for the VirtualMachine. This allows for re-use of common settings that should apply to multiple VirtualMachines.</p>
  </li>
  <li>
    <p>VirtualMachineReplicaSet (vmrs[s]) - tries to ensures that a specified number of VirtualMachine replicas are running at any time.</p>
  </li>
</ul>

<p><a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_domainspec">DomainSpec</a> is listed as a top-level object but is only used within all of the objects above. Currently the <code class="language-plaintext highlighter-rouge">DomainSpec</code> is a subset of what is configurable via <a href="https://libvirt.org/formatdomain.html">libvirt domain XML</a>.</p>

<h2 id="virtualmachine">VirtualMachine</h2>

<p>VirtualMachine is mortal object just like a
<a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pod</a> within Kubernetes.
It only runs once and cannot be resurrected. This might seem problematic especially
to an administrator coming from a traditional virtualization background. Fortunately
later we will discuss OfflineVirtualMachines which will address this.</p>

<p>First let’s use <code class="language-plaintext highlighter-rouge">kubectl</code> to retrieve a list of <code class="language-plaintext highlighter-rouge">VirtualMachine</code> objects.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get vms -n nodejs-ex
NAME      AGE
mongodb   5d
nodejs    5d
</code></pre></div></div>

<p>We can also use <code class="language-plaintext highlighter-rouge">kubectl describe</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl describe vms -n test
Name:         testvm
Namespace:    test
Labels:       guest=testvm
              kubevirt.io/nodeName=kn2.virtomation.com
              kubevirt.io/size=small
...output...
Events:
  Type    Reason              Age                From                               Message
  ----    ------              ----               ----                               -------
  Normal  SuccessfulCreate    59m                virtualmachine-controller          Created virtual machine pod virt-launcher-testvm-8h927
  Normal  SuccessfulHandOver  59m                virtualmachine-controller          Pod owner ship transfered to the node virt-launcher-testvm-8h927
  Normal  Created             59m (x2 over 59m)  virt-handler, kn2.virtomation.com  VM defined.
  Normal  Started             59m                virt-handler, kn2.virtomation.com  VM started.

</code></pre></div></div>

<p>And just in case if you want to return the yaml definition of a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> object here is an example.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl -o yaml get vms mongodb -n nodejs-ex
apiVersion: kubevirt.io/v1alpha1
kind: VirtualMachine
...output...
</code></pre></div></div>

<p>The first object we will annotate is <code class="language-plaintext highlighter-rouge">VirtualMachine</code>. The important sections <code class="language-plaintext highlighter-rouge">.spec</code> for <code class="language-plaintext highlighter-rouge">VirtualMachineSpec</code> and <code class="language-plaintext highlighter-rouge">.spec.domain</code> for <code class="language-plaintext highlighter-rouge">DomainSpec</code> will be annotated only in this section then referred to in the other object sections.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">labels</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">string</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">string</span>
<span class="na">spec</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<h3 id="node-placement">Node Placement</h3>

<p>Kubernetes has the ability to schedule a pod to specific nodes based on <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature">affinity and anti-affinity</a> rules.</p>

<p><a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_nodeaffinity">Node affinity</a> is also possible with KubeVirt. To <a href="hhttps://kubevirt.io/user-guide/operations/node_assignment/#affinity-and-anti-affinity">constrain a virtual machine</a> to run on a node define a matching expressions using node labels.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">affinity</span><span class="pi">:</span>
  <span class="na">nodeAffinity</span><span class="pi">:</span>
    <span class="na">preferredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">preference</span><span class="pi">:</span>
          <span class="na">matchExpressions</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">string</span>
              <span class="na">operator</span><span class="pi">:</span> <span class="s">string</span>
              <span class="na">values</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">string</span>
        <span class="na">weight</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
      <span class="na">nodeSelectorTerms</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">matchExpressions</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">string</span>
              <span class="na">operator</span><span class="pi">:</span> <span class="s">string</span>
              <span class="na">values</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">string</span>
</code></pre></div></div>

<p>A virtual machine can also more easily be constrained by using <a href="https://kubevirt.io/user-guide/operations/node_assignment/#nodeselector">nodeSelector</a> which is defined by node’s label and value. Here is an example</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">nodeSelector</span><span class="pi">:</span>
  <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">kn1.virtomation.com</span>
</code></pre></div></div>

<h3 id="clocks-and-timers">Clocks and Timers</h3>

<p>Configures the <a href="https://kubevirt.io/user-guide/virtual_machines/virtual_hardware/#clock">virtualize hardware</a> clock provided by <a href="https://www.qemu.org/docs/master/system/invocation.html#hxtool-9">QEMU</a>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">domain</span><span class="pi">:</span>
  <span class="na">clock</span><span class="pi">:</span>
    <span class="na">timezone</span><span class="pi">:</span> <span class="s">string</span>
    <span class="na">utc</span><span class="pi">:</span>
      <span class="na">offsetSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div></div>

<p>The <a href="https://kubevirt.io/user-guide/virtual_machines/virtual_hardware/#timers">timer</a> defines the <a href="https://libvirt.org/formatdomain.html#elementsTime">type and policy attribute</a> that determines what action is take when QEMU misses a deadline for injecting a tick to the guest.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">domain</span><span class="pi">:</span>
  <span class="na">clock</span><span class="pi">:</span>
    <span class="na">timer</span><span class="pi">:</span>
      <span class="na">hpet</span><span class="pi">:</span>
        <span class="na">present</span><span class="pi">:</span> <span class="no">true</span>
        <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">hyperv</span><span class="pi">:</span>
        <span class="na">present</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">kvm</span><span class="pi">:</span>
        <span class="na">present</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">pit</span><span class="pi">:</span>
        <span class="na">present</span><span class="pi">:</span> <span class="no">true</span>
        <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">rtc</span><span class="pi">:</span>
        <span class="na">present</span><span class="pi">:</span> <span class="no">true</span>
        <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">string</span>
        <span class="na">track</span><span class="pi">:</span> <span class="s">string</span>
</code></pre></div></div>

<h3 id="cpu-and-memory">CPU and Memory</h3>

<p>The number of <a href="https://kubevirt.io/user-guide/virtual_machines/virtual_hardware/#cpu">CPU cores</a> a virtual machine will be assigned. <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_cpu">.spec.domain.cpu.cores</a> will not be used for scheduling use <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_resourcerequirements">.spec.domain.resources.requests.cpu</a> instead.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">cpu</span><span class="pi">:</span>
  <span class="na">cores</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div>

<p>There are two supported <a href="https://kubevirt.io/user-guide/virtual_machines/virtual_hardware/#resources-requests-and-limits">resource limits and requests</a>: <code class="language-plaintext highlighter-rouge">cpu</code> and <code class="language-plaintext highlighter-rouge">memory</code>. A <code class="language-plaintext highlighter-rouge">.spec.domain.resources.requests.memory</code> should be defined to determine the allocation of memory provided to the virtual machine. These values will be used to in scheduling decisions.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">resources</span><span class="pi">:</span>
  <span class="na">limits</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">requests</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<h3 id="watchdog-devices">Watchdog Devices</h3>

<p><a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_watchdog">.spec.domain.watchdog</a> automatically triggers an action via <a href="https://libvirt.org/formatdomain.html#elementsWatchdog">Libvirt</a> and <a href="https://www.qemu.org/docs/master/system/invocation.html#hxtool-9">QEMU</a> when the virtual machine operating system hangs or crashes.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">watchdog</span><span class="pi">:</span>
  <span class="na">i6300esb</span><span class="pi">:</span>
    <span class="na">action</span><span class="pi">:</span> <span class="s">string</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">string</span>
</code></pre></div></div>

<h3 id="features">Features</h3>

<p><a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_features">.spec.domain.features</a>
are hypervisor cpu or machine features that can be enabled.
After reviewing both Linux and Microsoft QEMU virtual machines managed by
<a href="https://libvirt.org/formatdomain.html#elementsFeatures">Libvirt</a>
both acpi and
<a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_featureapic">apic</a>
should be enabled.
The <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_featurehyperv">hyperv</a> features should be enabled only for Windows-based virtual machines. For additional information regarding features please visit the <a href="https://kubevirt.io/user-guide/virtual_machines/virtual_hardware/#features">virtual hardware configuration</a> in the kubevirt user guide.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">features</span><span class="pi">:</span>
  <span class="na">acpi</span><span class="pi">:</span>
    <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">apic</span><span class="pi">:</span>
    <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">endOfInterrupt</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">hyperv</span><span class="pi">:</span>
    <span class="na">relaxed</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">reset</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">runtime</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">spinlocks</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">spinlocks</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">synic</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">synictimer</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">vapic</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">vendorid</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">vendorid</span><span class="pi">:</span> <span class="s">string</span>
    <span class="na">vpindex</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<h3 id="qemu-machine-type">QEMU Machine Type</h3>

<p><a href="https://kubevirt.io/user-guide/virtual_machines/virtual_hardware/#machine-type">.spec.domain.machine.type</a> is the emulated machine architecture provided by <a href="https://www.qemu.org/docs/master/system/invocation.html#hxtool-0">QEMU</a>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">machine</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">string</span>
</code></pre></div></div>

<p>Here is an example how to retrieve the supported QEMU machine types.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nv">$ </span>qemu-system-x86_64 <span class="nt">--machine</span> <span class="nb">help
    </span>Supported machines are:
    ...output...
    pc                   Standard PC <span class="o">(</span>i440FX + PIIX, 1996<span class="o">)</span> <span class="o">(</span><span class="nb">alias </span>of pc-i440fx-2.10<span class="o">)</span>
    pc-i440fx-2.10       Standard PC <span class="o">(</span>i440FX + PIIX, 1996<span class="o">)</span> <span class="o">(</span>default<span class="o">)</span>
    ...output...
    q35                  Standard PC <span class="o">(</span>Q35 + ICH9, 2009<span class="o">)</span> <span class="o">(</span><span class="nb">alias </span>of pc-q35-2.10<span class="o">)</span>
    pc-q35-2.10          Standard PC <span class="o">(</span>Q35 + ICH9, 2009<span class="o">)</span>
</code></pre></div></div>

<h3 id="disks-and-volumes">Disks and Volumes</h3>

<p><a href="https://kubevirt.io/api-reference/master/definitions.html#_v1_disk">.spec.domain.devices.disks</a> configures a <a href="https://www.qemu.org/docs/master/system/invocation.html#hxtool-1">QEMU</a> type of <a href="https://libvirt.org/formatdomain.html#elementsDisks">disk</a> to the virtual machine and assigns a specific <a href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">volume and its type to that disk</a> via the <code class="language-plaintext highlighter-rouge">volumeName</code>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">devices</span><span class="pi">:</span>
  <span class="na">disks</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
        <span class="na">bus</span><span class="pi">:</span> <span class="s">string</span>
        <span class="na">readonly</span><span class="pi">:</span> <span class="no">true</span>
        <span class="na">tray</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">disk</span><span class="pi">:</span>
        <span class="na">bus</span><span class="pi">:</span> <span class="s">string</span>
        <span class="na">readonly</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">floppy</span><span class="pi">:</span>
        <span class="na">readonly</span><span class="pi">:</span> <span class="no">true</span>
        <span class="na">tray</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">lun</span><span class="pi">:</span>
        <span class="na">bus</span><span class="pi">:</span> <span class="s">string</span>
        <span class="na">readonly</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">volumeName</span><span class="pi">:</span> <span class="s">string</span>
</code></pre></div></div>

<p><a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_cloudinitnocloudsource">cloudInitNoCloud</a>
injects scripts and configuration into a virtual machine operating system.
There are three different parameters that can be used to provide the
cloud-init coniguration: <code class="language-plaintext highlighter-rouge">secretRef</code>, <code class="language-plaintext highlighter-rouge">userData</code> or <code class="language-plaintext highlighter-rouge">userDataBase64</code>.</p>

<p>See the user-guide for examples of how to use <a href="https://kubevirt.io/user-guide/virtual_machines/startup_scripts/#cloud-init-examples">.spec.volumes.cloudInitNoCloud</a>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
      <span class="na">secretRef</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">userData</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">userDataBase64</span><span class="pi">:</span> <span class="s">string</span>
</code></pre></div></div>

<p>An <a href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#emptydisk">emptyDisk volume</a> creates an extra qcow2 disk that is created with the virtual machine. It will be removed if the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> object is deleted.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">emptyDisk</span><span class="pi">:</span>
  <span class="na">capacity</span><span class="pi">:</span> <span class="s">string</span>
</code></pre></div></div>

<p><a href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#ephemeral">Ephemeral volume</a> creates a temporary local copy on write image storage that will be discarded when the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> is removed.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">ephemeral</span><span class="pi">:</span>
  <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
    <span class="na">claimName</span><span class="pi">:</span> <span class="s">string</span>
    <span class="na">readOnly</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">string</span>
</code></pre></div></div>

<p><a href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#persistentvolumeclaim">persistentVolumeClaim volume</a> persists after the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> is deleted.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">persistentVolumeClaim</span><span class="pi">:</span>
  <span class="na">claimName</span><span class="pi">:</span> <span class="s">string</span>
  <span class="na">readOnly</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<p><a href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">registryDisk volume</a> type uses a virtual machine disk that is stored in a container image registry.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">registryDisk</span><span class="pi">:</span>
  <span class="na">image</span><span class="pi">:</span> <span class="s">string</span>
  <span class="na">imagePullSecret</span><span class="pi">:</span> <span class="s">string</span>
</code></pre></div></div>

<h3 id="virtual-machine-status">Virtual Machine Status</h3>

<p>Once the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> object has been created the <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_virtualmachinestatus">VirtualMachineStatus</a> will be available. <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_virtualmachinestatus">VirtualMachineStatus</a> can be used in automation tools such as Ansible to confirm running state, determine where a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> is running via <code class="language-plaintext highlighter-rouge">nodeName</code> or the <code class="language-plaintext highlighter-rouge">ipAddress</code> of the virtual machine operating system.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl -o yaml get vm mongodb -n nodejs-ex

# ...output...
status:
  interfaces:
  - ipAddress: 10.244.2.7
  nodeName: kn2.virtomation.com
  phase: Running
</code></pre></div></div>

<p>Example using <code class="language-plaintext highlighter-rouge">--template</code> to retrieve the <code class="language-plaintext highlighter-rouge">.status.phase</code> of the <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get vm mongodb --template {{.status.phase}} -n nodejs-ex
Running
</code></pre></div></div>

<h3 id="examples">Examples</h3>

<ul>
  <li><a href="https://kubevirt.io/user-guide/virtual_machines/virtual_machine_instances/#virtualmachineinstance-api">https://kubevirt.io/user-guide/virtual_machines/virtual_machine_instances/#virtualmachineinstance-api</a></li>
</ul>

<h2 id="offlinevirtualmachine">OfflineVirtualMachine</h2>

<p>An OfflineVirtualMachine is an immortal object within KubeVirt. The VirtualMachine
described within the spec will be recreated with a start power operation, host issue
or simply a accidental deletion of the VirtualMachine object.
For a traditional virtual administrator this object might be appropriate for
most use-cases.</p>

<p>Just like <code class="language-plaintext highlighter-rouge">VirtualMachine</code> we can retrieve the <code class="language-plaintext highlighter-rouge">OfflineVirtualMachine</code> objects.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get ovms -n nodejs-ex
NAME      AGE
mongodb   5d
nodejs    5d
</code></pre></div></div>

<p>And display the object in yaml.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl -o yaml get ovms mongodb -n nodejs-ex
apiVersion: kubevirt.io/v1alpha1
kind: OfflineVirtualMachine
metadata:
...output...
</code></pre></div></div>

<p>We continue by annotating <code class="language-plaintext highlighter-rouge">OfflineVirtualMachine</code> object.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">OfflineVirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">labels</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">string</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">string</span>
<span class="na">spec</span><span class="pi">:</span>
</code></pre></div></div>

<h3 id="what-is-running-in-offlinevirtualmachine">What is Running in OfflineVirtualMachine?</h3>

<p><a href="https://kubevirt.io/api-reference/master/definitions.html">.spec.runStrategy</a> controls whether and when the associated VirtualMachineInstance object is created. In other words this controls the <a href="https://kubevirt.io/user-guide/virtual_machines/lifecycle/#stopping-a-virtual-machine">power status</a> of the virtual machine.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  runStrategy: Always
</code></pre></div></div>

<p>This will create a <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> object which will instantiate and power on a virtual machine.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl patch offlinevirtualmachine mongodb --type merge -p '{"spec":{"runStrategy": "Always"}}' -n nodejs-ex
</code></pre></div></div>

<p>This will delete the <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> object which will power off the virtual machine.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl patch offlinevirtualmachine mongodb --type merge -p '{"spec":{"runStrategy": "Halted"}}' -n nodejs-ex
</code></pre></div></div>

<p>And if you would rather not have to remember the <code class="language-plaintext highlighter-rouge">kubectl patch</code> command above
the KubeVirt team has provided a cli tool <code class="language-plaintext highlighter-rouge">virtctl</code> that can start and stop
a guest.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./virtctl start mongodb <span class="nt">-n</span> nodejs-ex
./virtctl stop mongodb <span class="nt">-n</span> nodejs-ex
</code></pre></div></div>

<h3 id="offline-virtual-machine-status">Offline Virtual Machine Status</h3>

<p>Once the <code class="language-plaintext highlighter-rouge">OfflineVirtualMachine</code> object has been created the <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_offlinevirtualmachinestatus">OfflineVirtualMachineStatus</a> will be available. Like <code class="language-plaintext highlighter-rouge">VirtualMachineStatus</code> <code class="language-plaintext highlighter-rouge">OfflineVirtualMachineStatus</code> can be used for automation tools such as Ansible.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl -o yaml get ovms mongodb -n nodejs-ex

# ...output...
status:
  created: true
  ready: true
</code></pre></div></div>

<p>Example using <code class="language-plaintext highlighter-rouge">--template</code> to retrieve the <code class="language-plaintext highlighter-rouge">.status.conditions[0].type</code> of <code class="language-plaintext highlighter-rouge">OfflineVirtualMachine</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get ovm mongodb --template "{{.status.ready}}" -n nodejs-ex
true
</code></pre></div></div>

<h2 id="virtualmachinereplicaset">VirtualMachineReplicaSet</h2>

<p><a href="https://kubevirt.io/user-guide/virtual_machines/replicaset/">VirtualMachineReplicaSet</a> is great when you want to run multiple identical virtual machines.</p>

<p>Just like the other top-level objects we can retrieve <code class="language-plaintext highlighter-rouge">VirtualMachineReplicaSet</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get vmrs -n nodejs-ex
NAME      AGE
replica   1m
</code></pre></div></div>

<p>With the <code class="language-plaintext highlighter-rouge">replicas</code> parameter set to <code class="language-plaintext highlighter-rouge">2</code> the command below displays the two <code class="language-plaintext highlighter-rouge">VirtualMachine</code> objects that were created.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get vms -n nodejs-ex
NAME           AGE
replicanmgjl   7m
replicarjhdz   7m
</code></pre></div></div>

<h3 id="pause-rollout">Pause rollout</h3>

<p>The <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_vmreplicasetspec">.spec.paused</a> parameter if true pauses the deployment of the <code class="language-plaintext highlighter-rouge">VirtualMachineReplicaSet</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  paused: true
</code></pre></div></div>

<h3 id="replica-quantity">Replica quantity</h3>

<p>The <a href="https://kubevirt.io/user-guide/virtual_machines/replicaset/#using-virtualmachineinstancereplicaset">.spec.replicas</a> number of <code class="language-plaintext highlighter-rouge">VirtualMachine</code> objects that should be created.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  replicas: 0
</code></pre></div></div>

<p>The <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_labelselector">selector</a> must be defined and match labels defined in the template. It is used by the controller to keep track of managed virtual machines.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">selector</span><span class="pi">:</span>
  <span class="na">matchExpressions</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">operator</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">values</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">string</span>
  <span class="na">matchLabels</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<h3 id="virtual-machine-template-spec"><a href="https://kubevirt.io/user-guide/virtual_machines/replicaset/#using-virtualmachineinstancereplicaset">Virtual Machine Template Spec</a></h3>

<p>The <code class="language-plaintext highlighter-rouge">VMTemplateSpec</code> is the definition of a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> objects that will be created.</p>

<p>In the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> section the <code class="language-plaintext highlighter-rouge">.spec</code> <code class="language-plaintext highlighter-rouge">VirtualMachineSpec</code> describes the available parameters for that object.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">template</span><span class="pi">:</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">annotations</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">labels</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">string</span>
    <span class="na">namespace</span><span class="pi">:</span> <span class="s">string</span>
  <span class="na">spec</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<h3 id="replica-status">Replica Status</h3>

<p>Like the other objects we already have discussed <a href="http://kubevirt.io/api-reference/master/definitions.html#_v1_vmreplicasetstatus">VMReplicaSetStatus</a> is an important object to use for automation.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">status</span><span class="pi">:</span>
  <span class="na">readyReplicas</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div></div>

<p>Example using <code class="language-plaintext highlighter-rouge">--template</code> to retrieve the <code class="language-plaintext highlighter-rouge">.status.readyReplicas</code> and <code class="language-plaintext highlighter-rouge">.status.replicas</code> of <code class="language-plaintext highlighter-rouge">VirtualMachineReplicaSet</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get vmrs replica --template "{{.status.readyReplicas}}" -n nodejs-ex
2
$ kubectl get vmrs replica --template "{{.status.replicas}}" -n nodejs-ex
2
</code></pre></div></div>

<h3 id="examples-1">Examples</h3>

<ul>
  <li><a href="https://kubevirt.io/user-guide/virtual_machines/replicaset/#example">https://kubevirt.io/user-guide/virtual_machines/replicaset/#example</a></li>
</ul>

<h2 id="virtualmachinepreset">VirtualMachinePreset</h2>

<p>This is used to define a <code class="language-plaintext highlighter-rouge">DomainSpec</code> that can be used for multiple virtual machines.</p>

<p>To configure a <code class="language-plaintext highlighter-rouge">DomainSpec</code> for multiple <code class="language-plaintext highlighter-rouge">VirtualMachine</code> objects the <code class="language-plaintext highlighter-rouge">selector</code> defines which <code class="language-plaintext highlighter-rouge">VirtualMachine</code> the <code class="language-plaintext highlighter-rouge">VirtualMachinePreset</code> should be applied to.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get vmpreset -n nodejs-ex
NAME       AGE
m1.small   17s
</code></pre></div></div>

<h3 id="domain-spec">Domain Spec</h3>

<p>See the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> section above for annotated details of the <code class="language-plaintext highlighter-rouge">DomainSpec</code> object.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spec:
  domain: {}
</code></pre></div></div>

<h3 id="preset-selector">Preset Selector</h3>

<p>The <a href="https://kubevirt.io/user-guide/virtual_machines/presets/#virtualmachine-selector">selector</a> is optional but if not defined will be applied to all <code class="language-plaintext highlighter-rouge">VirtualMachine</code> objects; which is probably not the intended purpose so I recommend always including a selector.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">selector</span><span class="pi">:</span>
  <span class="na">matchExpressions</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">operator</span><span class="pi">:</span> <span class="s">string</span>
      <span class="na">values</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">string</span>
  <span class="na">matchLabels</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<h3 id="examples-2">Examples</h3>

<ul>
  <li><a href="https://kubevirt.io/user-guide/virtual_machines/presets/#examples">https://kubevirt.io/user-guide/virtual_machines/presets/#examples</a></li>
</ul>

<p>We provided an annotated view into the KubeVirt objects - VirtualMachine, OfflineVirtualMachine, VirtualMachineReplicaSet and VirtualMachinePreset. Hopefully this will help a user of KubeVirt to understand the options and parameters that are currently available when creating a virtual machine on Kubernetes.</p>]]></content><author><name>jcpowermac</name></author><category term="uncategorized" /><category term="custom resources" /><category term="kubevirt objects" /><category term="objects" /><category term="VirtualMachine" /><summary type="html"><![CDATA[In this post we will go over the objects provided by KubeVirt]]></summary></entry><entry><title type="html">Deploying Vms On Kubernetes Glusterfs Kubevirt</title><link href="https://kubevirt.io//2018/Deploying-VMs-on-Kubernetes-GlusterFS-KubeVirt.html" rel="alternate" type="text/html" title="Deploying Vms On Kubernetes Glusterfs Kubevirt" /><published>2018-05-07T00:00:00+00:00</published><updated>2018-05-07T00:00:00+00:00</updated><id>https://kubevirt.io//2018/Deploying-VMs-on-Kubernetes-GlusterFS-KubeVirt</id><content type="html" xml:base="https://kubevirt.io//2018/Deploying-VMs-on-Kubernetes-GlusterFS-KubeVirt.html"><![CDATA[<p>Kubernetes is traditionally used to deploy and manage containerized applications. Did you know Kubernetes can also be used to deploy and manage virtual machines? This guide will walk you through installing a Kubernetes environment backed by GlusterFS for storage and the KubeVirt add-on to enable deployment and management of VMs.</p>

<h2 id="contents">Contents</h2>

<ul>
  <li>Prerequisites</li>
  <li>Known Issues</li>
  <li>Installing Kubernetes</li>
  <li>Installing GlusterFS and Heketi using gk-deploy</li>
  <li>Installing KubeVirt</li>
  <li>Deploying Virtual Machines</li>
</ul>

<h2 id="prerequisites">Prerequisites</h2>

<p>You should have access to at least three baremetal servers. One server will be the master Kubernetes node and other two servers will be the worker nodes. Each server should have a block device attached for GlusterFS, this is in addition to the ones used by the OS.</p>

<p>You may use virtual machines in lieu of baremetal servers. Performance may suffer and you will need to ensure your hardware supports nested virtualization and that the relevant kernel modules are loaded in the OS.</p>

<p>For reference, I used the following components and versions:</p>

<ul>
  <li>baremetal servers with CentOS version 7.4 as the base OS</li>
  <li>latest version of Kubernetes (at the time v1.10.1)</li>
  <li>Weave Net as the Container Network Interface (CNI), v2.3.0</li>
  <li><a href="https://github.com/gluster/gluster-kubernetes">gluster-kubernetes</a> master commit 2a2a68ce5739524802a38f3871c545e4f57fa20a</li>
  <li>KubeVirt v0.4.1.</li>
</ul>

<h2 id="known-issues">Known Issues</h2>

<ul>
  <li>You may need to set <code class="language-plaintext highlighter-rouge">SELinux</code> to permissive mode prior to running “kubeadm init” if you see failures attributed to etcd in <code class="language-plaintext highlighter-rouge">/var/log/audit.log</code>.</li>
  <li>Prior to installing GlusterFS, you may need to disable firewalld until this issue is resolved: https://github.com/gluster/gluster-kubernetes/issues/471</li>
  <li>kubevirt-ansible install may fail in storage-glusterfs role: https://github.com/kubevirt/kubevirt-ansible/issues/219</li>
</ul>

<h2 id="installing-kubernetes">Installing Kubernetes</h2>

<p>Create the Kubernetes cluster by using kubeadm. Detailed instructions can be found at https://kubernetes.io/docs/setup/independent/install-kubeadm/.</p>

<p>Use Weave Net as the CNI. Other CNIs may work, but I have only tested Weave Net.</p>

<p>If you are using only 2 servers as workers, then you will need to allow scheduling of pods on the master node because GlusterFS requires at least three nodes. To schedule pods on the master node, see “Master Isolation” in the kubeadm guide or execute this command:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl taint nodes <span class="nt">--all</span> node-role.kubernetes.io/master-
</code></pre></div></div>

<p>Move onto the next step when your master and worker nodes are Ready.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master ~]# kubectl get nodes
NAME                     STATUS    ROLES     AGE       VERSION
master.somewhere.com     Ready     master    6d        v1.10.1
worker1.somewhere.com    Ready     &lt;none&gt;    6d        v1.10.1
worker2.somewhere.com    Ready     &lt;none&gt;    6d        v1.10.1
</code></pre></div></div>

<p>And all of the pods in the kube-system namespace are Running.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master ~]# kubectl get pods -n kube-system
NAME                                           READY     STATUS    RESTARTS   AGE
etcd-master.somewhere.com                      1/1       Running   0          6d
kube-apiserver-master.somewhere.com            1/1       Running   0          6d
kube-controller-manager-master.somewhere.com   1/1       Running   0          6d
kube-dns-86f4d74b45-glv4k                      3/3       Running   0          6d
kube-proxy-b6ksg                               1/1       Running   0          6d
kube-proxy-jjxs5                               1/1       Running   0          6d
kube-proxy-kw77k                               1/1       Running   0          6d
kube-scheduler-master.somewhere.com            1/1       Running   0          6d
weave-net-ldlh7                                2/2       Running   0          6d
weave-net-pmhlx                                2/2       Running   1          6d
weave-net-s4dp6                                2/2       Running   0          6d
</code></pre></div></div>

<h3 id="installing-glusterfs-and-heketi-using-gluster-kubernetes">Installing GlusterFS and Heketi using gluster-kubernetes</h3>

<p>The next step is to deploy GlusterFS and Heketi onto Kubernetes.</p>

<p><a href="https://github.com/gluster/glusterfs">GlusterFS</a> provides the storage system on which the virtual machine images are stored. <a href="https://github.com/heketi/heketi">Heketi</a> provides the REST API that Kubernetes uses to provision GlusterFS volumes. The <a href="https://github.com/gluster/gluster-kubernetes">gk-deploy tool</a> is used to deploy both of these components as pods in the Kubernetes cluster.</p>

<p>There is a detailed <a href="https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md">setup guide for gk-deploy</a>. Note each node must have a raw block device that is reserved for use by heketi and they must not contain any data or be pre-formatted. You can reset your block device to a useable state by running:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wipefs -a &lt;path to device&gt;
</code></pre></div></div>

<p>To aid you, below are the commands you will need to run if you are following the setup guide.</p>

<p>On all nodes:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Open ports for GlusterFS communications</span>
<span class="nb">sudo </span>iptables <span class="nt">-I</span> INPUT 1 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 2222 <span class="nt">-j</span> ACCEPT
<span class="nb">sudo </span>iptables <span class="nt">-I</span> INPUT 1 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 24007 <span class="nt">-j</span> ACCEPT
<span class="nb">sudo </span>iptables <span class="nt">-I</span> INPUT 1 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 24008 <span class="nt">-j</span> ACCEPT
<span class="nb">sudo </span>iptables <span class="nt">-I</span> INPUT 1 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 49152:49251 <span class="nt">-j</span> ACCEPT
<span class="c"># Load kernel modules</span>
<span class="nb">sudo </span>modprobe dm_snapshot
<span class="nb">sudo </span>modprobe dm_thin_pool
<span class="nb">sudo </span>modprobe dm_mirror
<span class="c"># Install glusterfs-fuse and git packages</span>
<span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> glusterfs-fuse git
</code></pre></div></div>

<p>On the master node:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># checkout gluster-kubernetes repo</span>
git clone https://github.com/gluster/gluster-kubernetes
<span class="nb">cd </span>gluster-kubernetes/deploy
</code></pre></div></div>

<p>Before running the gk-deploy script, we need to first create a topology.json file that maps the nodes present in the GlusterFS cluster and the block devices attached to each node. The block devices should be raw and unformatted. Below is a sample topology.json file for a 3 node cluster all operating in the same zone. The gluster-kubernetes/deploy directory also contains a sample topology.json file.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">#</span><span class="w"> </span><span class="err">topology.json</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="nl">"clusters"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"nodes"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"node"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"hostnames"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
              </span><span class="nl">"manage"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"master.somewhere.com"</span><span class="w">
              </span><span class="p">],</span><span class="w">
              </span><span class="nl">"storage"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"192.168.10.100"</span><span class="w">
              </span><span class="p">]</span><span class="w">
            </span><span class="p">},</span><span class="w">
            </span><span class="nl">"zone"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w">
          </span><span class="p">},</span><span class="w">
          </span><span class="nl">"devices"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="s2">"/dev/vdb"</span><span class="w">
          </span><span class="p">]</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"node"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"hostnames"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
              </span><span class="nl">"manage"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"worker1.somewhere.com"</span><span class="w">
              </span><span class="p">],</span><span class="w">
              </span><span class="nl">"storage"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"192.168.10.101"</span><span class="w">
              </span><span class="p">]</span><span class="w">
            </span><span class="p">},</span><span class="w">
            </span><span class="nl">"zone"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w">
          </span><span class="p">},</span><span class="w">
          </span><span class="nl">"devices"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="s2">"/dev/vdb"</span><span class="w">
          </span><span class="p">]</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"node"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"hostnames"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
              </span><span class="nl">"manage"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"worker2.somewhere.com"</span><span class="w">
              </span><span class="p">],</span><span class="w">
              </span><span class="nl">"storage"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"192.168.10.102"</span><span class="w">
              </span><span class="p">]</span><span class="w">
            </span><span class="p">},</span><span class="w">
            </span><span class="nl">"zone"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w">
          </span><span class="p">},</span><span class="w">
          </span><span class="nl">"devices"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="s2">"/dev/vdb"</span><span class="w">
          </span><span class="p">]</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Under “hostnames”, the node’s hostname is listed under “manage” and its IP address is listed under “storage”. Multiple block devices can be listed under “devices”. If you are using VMs, the second block device attached to the VM will usually be /dev/vdb. For multi-path, the device path will usually be /dev/mapper/mpatha. If you are using a second disk drive, the device path will usually be /dev/sdb.</p>

<p>Once you have your topology.json file and saved it in gluster-kubernetes/deploy, we can execute gk-deploy to create the GlusterFS and Heketi pods. You will need to specify an admin-key which will be used in the next step and will be discovered during the KubeVirt installation.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># from gluster-kubernetes/deploy</span>
./gk-deploy <span class="nt">-g</span> <span class="nt">-v</span> <span class="nt">-n</span> kube-system <span class="nt">--admin-key</span> my-admin-key
</code></pre></div></div>

<p>Add the end of the installation, you will see:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>heketi is now running and accessible via http://10.32.0.4:8080 . To run
administrative commands you can install 'heketi-cli' and use it as follows:

  # heketi-cli -s http://10.32.0.4:8080 --user admin --secret '&lt;ADMIN_KEY&gt;' cluster list

You can find it at https://github.com/heketi/heketi/releases . Alternatively,
use it from within the heketi pod:

  # /usr/bin/kubectl -n kube-system exec -i heketi-b96c7c978-dcwlw -- heketi-cli -s http://localhost:8080 --user admin --secret '&lt;ADMIN_KEY&gt;' cluster list

For dynamic provisioning, create a StorageClass similar to this:\
</code></pre></div></div>

<p>Take note of the URL for Heketi which will be used next step.</p>

<p>If successful, 4 additional pods will be shown as Running in the kube-system namespace.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master deploy]# kubectl get pods -n kube-system
NAME                                                              READY     STATUS    RESTARTS   AGE
...snip...
glusterfs-h4nwf                                                   1/1       Running   0          6d
glusterfs-kfvjk                                                   1/1       Running   0          6d
glusterfs-tjm2f                                                   1/1       Running   0          6d
heketi-b96c7c978-dcwlw                                            1/1       Running   0          6d
...snip...
</code></pre></div></div>

<h3 id="installing-kubevirt-and-setting-up-storage">Installing KubeVirt and setting up storage</h3>

<p>The final component to install and which will enable us to deploy VMs on Kubernetes is KubeVirt.
We will use <a href="https://github.com/kubevirt/kubevirt-ansible/">kubevirt-ansible</a> to deploy KubeVirt which will also help us configure a Secret and a StorageClass that will allow us to provision Persistent Volume Claims (PVCs) on GlusterFS.</p>

<p>Let’s first clone the kubevirt-ansible repo.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/kubevirt/kubevirt-ansible
<span class="nb">cd </span>kubevirt-ansible
</code></pre></div></div>

<p>Edit the <a href="https://github.com/kubevirt/kubevirt-ansible/blob/master/inventory">inventory</a> file in the kubevirt-ansible checkout. Modify the section that starts with “#BEGIN CUSTOM SETTINGS”. As an example using the servers from above:</p>

<div class="language-conf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># BEGIN CUSTOM SETTINGS
</span>[<span class="n">masters</span>]
<span class="c"># Your master FQDN
</span><span class="n">master</span>.<span class="n">somewhere</span>.<span class="n">com</span>

[<span class="n">etcd</span>]
<span class="c"># Your etcd FQDN
</span><span class="n">master</span>.<span class="n">somewhere</span>.<span class="n">com</span>

[<span class="n">nodes</span>]
<span class="c"># Your nodes FQDN's
</span><span class="n">worker1</span>.<span class="n">somewhere</span>.<span class="n">com</span>
<span class="n">worker2</span>.<span class="n">somewhere</span>.<span class="n">com</span>

[<span class="n">nfs</span>]
<span class="c"># Your nfs server FQDN
</span>
[<span class="n">glusterfs</span>]
<span class="c"># Your glusterfs nodes FQDN
# Each node should have the "glusterfs_devices" variable, which
# points to the block device that will be used by gluster.
</span><span class="n">master</span>.<span class="n">somewhere</span>.<span class="n">com</span>
<span class="n">worker1</span>.<span class="n">somewhere</span>.<span class="n">com</span>
<span class="n">worker1</span>.<span class="n">somewhere</span>.<span class="n">com</span>

<span class="c">#
# If you run openshift deployment
# You can add your master as schedulable node with option openshift_schedulable=true
# Add at least one node with lable to run on it router and docker containers
# openshift_node_labels="{'region': 'infra','zone': 'default'}"
# END CUSTOM SETTINGS
</span></code></pre></div></div>

<p>Now let’s run the <code class="language-plaintext highlighter-rouge">kubevirt.yml</code> playbook:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook <span class="nt">-i</span> inventory playbooks/kubevirt.yml <span class="nt">-e</span> <span class="nv">cluster</span><span class="o">=</span>k8s <span class="nt">-e</span> <span class="nv">storage_role</span><span class="o">=</span>storage-glusterfs <span class="nt">-e</span> <span class="nv">namespace</span><span class="o">=</span>kube-system <span class="nt">-e</span> <span class="nv">glusterfs_namespace</span><span class="o">=</span>kube-system <span class="nt">-e</span> <span class="nv">glusterfs_name</span><span class="o">=</span> <span class="nt">-e</span> <span class="nv">heketi_url</span><span class="o">=</span>http://10.32.0.4:8080 <span class="nt">-v</span>
</code></pre></div></div>

<p>If successful, we should see 7 additional pods as Running in the kube-system namespace.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master kubevirt-ansible]# kubectl get pods -n kube-system
NAME                                                              READY     STATUS    RESTARTS   AGE
virt-api-785fd6b4c7-rdknl                                         1/1       Running   0          6d
virt-api-785fd6b4c7-rfbqv                                         1/1       Running   0          6d
virt-controller-844469fd89-c5vrc                                  1/1       Running   0          6d
virt-controller-844469fd89-vtjct                                  0/1       Running   0          6d
virt-handler-78wsb                                                1/1       Running   0          6d
virt-handler-csqbl                                                1/1       Running   0          6d
virt-handler-hnlqn                                                1/1       Running   0          6d
</code></pre></div></div>

<h2 id="deploying-virtual-machines">Deploying Virtual Machines</h2>

<p>To deploy a VM, we must first grab a VM image in raw format, place the image into a PVC, define the VM in a yaml file, source the VM definition into Kubernetes, and then start the VM.</p>

<p>The <a href="https://github.com/kubevirt/containerized-data-importer">containerized data importer (CDI)</a> is usually used to import VM images into Kubernetes, but there are some patches and additional testing to be done before the CDI can work smoothly with GlusterFS. For now, we will be placing the image into the PVC using a Pod that curls the image from the local filesystem using httpd.</p>

<p>On master or on a node where kubectl is configured correctly install and start httpd.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> httpd
<span class="nb">sudo </span>systemctl start httpd
</code></pre></div></div>

<p>Download the cirros cloud image and convert it into raw format.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img <span class="nt">-o</span> /var/www/html/cirros-0.4.0-x86_64-disk.img
<span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> qemu-img
qemu-img convert /var/www/html/cirros-0.4.0-x86_64-disk.img /var/www/html/cirros-0.4.0-x86_64-disk.raw
</code></pre></div></div>

<p>Create the PVC to store the cirros image.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl create -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
 <span class="na">name</span><span class="pi">:</span> <span class="s">gluster-pvc-cirros</span>
 <span class="na">annotations</span><span class="pi">:</span>
   <span class="na">volume.beta.kubernetes.io/storage-class</span><span class="pi">:</span> <span class="s">kubevirt</span>
<span class="na">spec</span><span class="pi">:</span>
 <span class="na">accessModes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
 <span class="na">resources</span><span class="pi">:</span>
   <span class="na">requests</span><span class="pi">:</span>
     <span class="na">storage</span><span class="pi">:</span> <span class="s">5Gi</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>Check the PVC was created and has “Bound” status.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master ~]# kubectl get pvc
NAME                 STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
gluster-pvc-cirros   Bound     pvc-843bd508-4dbf-11e8-9e4e-149ecfc53021   5Gi        RWO            kubevirt       2m
</code></pre></div></div>

<p>Create a Pod to curl the cirros image into the PVC.
Note: You will need to substitute <hostname> with actual hostname or IP address.</hostname></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl create -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">image-importer-cirros</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">OnFailure</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">image-importer-cirros</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirtci/disk-importer</span>
    <span class="na">env</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">CURL_OPTS</span>
        <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">-L"</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">INSTALL_TO</span>
        <span class="na">value</span><span class="pi">:</span> <span class="s">/storage/disk.img</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">URL</span>
        <span class="na">value</span><span class="pi">:</span> <span class="s">http://&lt;hostname&gt;/cirros-0.4.0-x86_64-disk.raw</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">storage</span>
      <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/storage</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">storage</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">gluster-pvc-cirros</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>Check and wait for the image-importer-cirros Pod to complete.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master ~]# kubectl get pods
NAME                         READY     STATUS      RESTARTS   AGE
image-importer-cirros        0/1       Completed   0          28s
</code></pre></div></div>

<p>Create a Virtual Machine definition for your VM and source it into Kubernetes.
Note the PVC containing the cirros image must be listed as the first disk under spec.domain.devices.disks.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl create -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha2</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/ovm</span><span class="pi">:</span> <span class="s">cirros</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cirros</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Halted</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">kubevirt.io/ovm</span><span class="pi">:</span> <span class="s">cirros</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
            <span class="na">volumeName</span><span class="pi">:</span> <span class="s">cirros-pvc</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
            <span class="na">volumeName</span><span class="pi">:</span> <span class="s">cloudinitvolume</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">64M</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">userDataBase64</span><span class="pi">:</span> <span class="s">IyEvYmluL3NoCgplY2hvICdwcmludGVkIGZyb20gY2xvdWQtaW5pdCB1c2VyZGF0YScK</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitvolume</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cirros-pvc</span>
        <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
          <span class="na">claimName</span><span class="pi">:</span> <span class="s">gluster-pvc-cirros</span>
<span class="na">status</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<p>Finally start the VM.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export VERSION=v0.4.1
curl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-linux-amd64
chmod +x virtctl
./virtctl start cirros
</code></pre></div></div>

<p>Wait for the VM pod to be in “Running” status.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master ~]# kubectl get pods
NAME                         READY     STATUS      RESTARTS   AGE
image-importer-cirros        0/1       Completed   0          28s
virt-launcher-cirros-krvv2   0/1       Running     0          13s
</code></pre></div></div>

<p>Once it is running, we can then connect to its console.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./virtctl console cirros
</code></pre></div></div>

<p>Press enter if a login prompt doesn’t appear.</p>]]></content><author><name>rwsu</name></author><category term="uncategorized" /><category term="glusterfs" /><category term="heketi" /><category term="virtual machine" /><category term="weavenet" /><summary type="html"><![CDATA[Deploying Virtual Machines on Kubernetes with GlusterFS+Heketi and KubeVirt]]></summary></entry></feed>