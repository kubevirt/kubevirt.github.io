<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed/news.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2023-03-29T19:07:28+00:00</updated><id>https://kubevirt.io//feed/news.xml</id><title type="html">KubeVirt.io | News</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">Secondary networks for KubeVirt VMs using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks.html" rel="alternate" type="text/html" title="Secondary networks for KubeVirt VMs using OVN-Kubernetes" /><published>2023-03-06T00:00:00+00:00</published><updated>2023-03-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>OVN (Open Virtual Network) is a series of daemons for the Open vSwitch that
translate virtual network configurations into OpenFlow. It provides virtual
networking capabilities for any type of workload on a virtualized platform
(virtual machines and containers) using the same API.</p>

<p>OVN provides a higher-layer of abstraction than Open vSwitch, working with
logical routers and logical switches, rather than flows.
More details can be found in the OVN architecture
<a href="https://man7.org/linux/man-pages/man7/ovn-architecture.7.html#DESCRIPTION">man page</a>.</p>

<p>In this post we will repeat the scenario of
<a href="https://kubevirt.io/2020/Multiple-Network-Attachments-with-bridge-CNI.html">its bridge CNI equivalent</a>,
using this SDN approach, which uses virtual networking infrastructure: thus, it
is <strong>not</strong> required to provision VLANs or other physical network resources.</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, you will need a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li>OVN-Kubernetes</li>
  <li>multus-cni</li>
  <li>KubeVirt</li>
</ul>

<p>The <a href="#environment-setup">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
and upstream latest multus-cni deployed. Please <strong>skip</strong> this section if your
cluster already features these components (e.g. Openshift).</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster, configured to use OVN-Kubernetes as
the default cluster network, configuring the multi-homing OVN-Kubernetes feature
gate, and deploying
<a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a> in the cluster.</p>

<h4 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h4>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v0.59.0). Please skip this
section if you already have a running cluster with KubeVirt installed in it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="define-the-overlay-network">Define the overlay network</h3>
<p>Provision the following yaml to define the overlay which will configure the
secondary attachment for the KubeVirt VMs. Please refer to the OVN-Kubernetes
user
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/multi-homing.md#switched---layer-2---topology">documentation</a>
for details into each of the knobs.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: l2-network
  namespace: default
spec:
  config: |2
    {
            "cniVersion": "0.3.1",
            "name": "l2-network",
            "type": "ovn-k8s-cni-overlay",
            "topology":"layer2",
            "netAttachDefName": "default/l2-network"
    }
</span><span class="no">EOF
</span></code></pre></div></div>

<p>The above example will configure a cluster-wide overlay <strong>without</strong> a subnet
defined. This means the users will have to define static IPs for their VMs.</p>

<p>It is also worth to point out the value of the <code class="language-plaintext highlighter-rouge">netAttachDefName</code> attribute
must match the <code class="language-plaintext highlighter-rouge">&lt;namespace&gt;/&lt;name&gt;</code> of the surrounding
<code class="language-plaintext highlighter-rouge">NetworkAttachmentDefinition</code> object.</p>

<h3 id="spin-up-the-vms">Spin up the VMs</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: vm-server
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            masquerade: {}
          - name: flatl2-overlay
            bridge: {}
        machine:
          type: ""
        resources:
          requests:
            memory: 1024M
      networks:
      - name: default
        pod: {}
      - name: flatl2-overlay
        multus:
          networkName: l2-network
      terminationGracePeriodSeconds: 0
      volumes:
        - name: containerdisk
          containerDisk:
            image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel
        - name: cloudinitdisk
          cloudInitNoCloud:
            networkData: |
              version: 2
              ethernets:
                eth1:
                  addresses: [ 192.0.2.20/24 ]
            userData: |-
              #cloud-config
              password: fedora
              chpasswd: { expire: False }
---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: vm-client
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            masquerade: {}
          - name: flatl2-overlay
            bridge: {}
        machine:
          type: ""
        resources:
          requests:
            memory: 1024M
      networks:
      - name: default
        pod: {}
      - name: flatl2-overlay
        multus:
          networkName: l2-network
      terminationGracePeriodSeconds: 0
      volumes:
        - name: containerdisk
          containerDisk:
            image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel
        - name: cloudinitdisk
          cloudInitNoCloud:
            networkData: |
              version: 2
              ethernets:
                eth1:
                  addresses: [ 192.0.2.10/24 ]
            userData: |-
              #cloud-config
              password: fedora
              chpasswd: { expire: False }
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Provision these two Virtual Machines, and wait for them to boot up.</p>

<h3 id="test-connectivity">Test connectivity</h3>
<p>To verify connectivity over our layer 2 overlay, we need first to ensure the IP
address of the server VM; let’s query the VMI status for that:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get vmi vm-server <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"10.244.2.8"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"10.244.2.8"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"52:54:00:23:1c:c2"</span>,
    <span class="s2">"name"</span>: <span class="s2">"default"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>,
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth1"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.0.2.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.0.2.20"</span>,
      <span class="s2">"fe80::7cab:88ff:fe5b:39f"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"7e:ab:88:5b:03:9f"</span>,
    <span class="s2">"name"</span>: <span class="s2">"flatl2-overlay"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>
</code></pre></div></div>

<p>You can afterwards connect to them via console and ping <code class="language-plaintext highlighter-rouge">vm-server</code>:</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>The user and password for this VMs is fedora; check the VM template spec cloudinit userData</p>


</div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virtctl console vm-client
ip a <span class="c"># confirm the IP address is the one set via cloud-init</span>
<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    <span class="nb">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000
    <span class="nb">link</span>/ether 52:54:00:29:de:53 brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 10.0.2.2/24 brd 10.0.2.255 scope global dynamic noprefixroute eth0
       valid_lft 86313584sec preferred_lft 86313584sec
    inet6 fe80::5054:ff:fe29:de53/64 scope <span class="nb">link
       </span>valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000
    <span class="nb">link</span>/ether 36:f9:29:65:66:55 brd ff:ff:ff:ff:ff:ff
    altname enp2s0
    inet 192.0.2.10/24 brd 192.0.2.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::34f9:29ff:fe65:6655/64 scope <span class="nb">link
       </span>valid_lft forever preferred_lft forever

<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ping <span class="nt">-c4</span> 192.0.2.20 <span class="c"># ping the vm-server static IP</span>
PING 192.0.2.20 <span class="o">(</span>192.0.2.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.05 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.05 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.995 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.902 ms

<span class="nt">---</span> 192.0.2.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 0.902/0.997/1.046/0.058 ms
</code></pre></div></div>
<h2 id="conclusion">Conclusion</h2>
<p>In this post we have seen how to use OVN-Kubernetes to create an overlay to
connect VMs in different nodes using secondary networks, without having to
configure any physical networking infrastructure.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><summary type="html"><![CDATA[This post explains how to configure cluster-wide overlays as secondary networks for KubeVirt virtual machines.]]></summary></entry><entry><title type="html">KubeVirt Summit 2023!</title><link href="https://kubevirt.io//2023/KubeVirt-Summit-2023.html" rel="alternate" type="text/html" title="KubeVirt Summit 2023!" /><published>2023-03-03T00:00:00+00:00</published><updated>2023-03-03T00:00:00+00:00</updated><id>https://kubevirt.io//2023/KubeVirt-Summit-2023</id><content type="html" xml:base="https://kubevirt.io//2023/KubeVirt-Summit-2023.html"><![CDATA[<p>The third online <a href="/summit/">KubeVirt Summit</a> starts March 29, 2023!</p>

<h2 id="when">When</h2>

<p>The event will take place online over two half-days:</p>

<ul>
  <li>Dates: March 29 and 30, 2023.</li>
  <li>Time: 14:00 – 19:00 UTC (9:00–14:00 EST, 15:00–20:00 CET)</li>
</ul>

<h2 id="register">Register</h2>

<p><a href="/summit/">KubeVirt Summit</a> is hosted on Community.CNCF.io. This is a free event but you need to register in order to attend.</p>

<p><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2023/">Register for KubeVirt Summit 2023</a></p>

<p>If this is your first time attending, you will need to create an account with CNCF.io.</p>

<h2 id="schedule">Schedule</h2>

<p>The schedule is available on the <a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2023/">CNCF Community Events page</a> where you register, as well as on the <a href="/summit/">KubeVirt Summit page</a>.</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our <a href="/community">community page</a>.</p>

<p>See you there!</p>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's third annual dedicated online event]]></summary></entry><entry><title type="html">Simplifying KubeVirt’s `VirtualMachine` UX with Instancetypes and Preferences</title><link href="https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes.html" rel="alternate" type="text/html" title="Simplifying KubeVirt’s `VirtualMachine` UX with Instancetypes and Preferences" /><published>2022-08-12T00:00:00+00:00</published><updated>2022-08-12T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes.html"><![CDATA[<p>KubeVirt’s <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> API contains many advanced options for tuning a virtual machine’s resources and performance that go beyond what typical users need to be aware of. Users have until now been unable to simply define the storage/network they want assigned to their VM and then declare in broad terms what quality of resources and kind of performance they need for their VM. Instead, the user has to be keenly aware how to request specific compute resources alongside all of the performance tunings available on the <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> API and how those tunings impact their guest’s operating system in order to get a desired result.</p>

<p>A common pattern for IaaS is to have abstractions separating the resource sizing and performance of a workload from the user-defined values related to launching their custom application. This pattern is evident across all the major cloud providers (also known as hyperscalers) as well as open source IaaS projects like OpenStack. AWS has <a href="https://aws.amazon.com/ec2/instance-types/">instance types</a>, GCP has <a href="https://cloud.google.com/compute/docs/machine-types#custom_machine_types">machine types</a>, Azure has <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes">instance VM sizes</a>, and OpenStack has <a href="https://docs.openstack.org/nova/latest/user/flavors.html">flavors</a>.</p>

<p>Let’s take AWS for example to help visualize what this abstraction enables. Launching an EC2 instance only requires a few top level arguments; the disk image, instance type, keypair, security group, and subnet:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws ec2 run-instances <span class="nt">--image-id</span> ami-xxxxxxxx <span class="se">\</span>
                        <span class="nt">--count</span> 1 <span class="se">\</span>
                        <span class="nt">--instance-type</span> c4.xlarge <span class="se">\</span>
                        <span class="nt">--key-name</span> MyKeyPair <span class="se">\</span>
                        <span class="nt">--security-group-ids</span> sg-903004f8 <span class="se">\</span>
                        <span class="nt">--subnet-id</span> subnet-6e7f829e
</code></pre></div></div>

<p>When creating the EC2 instance the user doesn’t define the amount of resources, what processor to use, how to optimize the performance of the instance, or what hardware to schedule the instance on. Instead, all of that information is wrapped up in that single <code class="language-plaintext highlighter-rouge">--instance-type c4.xlarge</code> CLI argument. <code class="language-plaintext highlighter-rouge">c4</code> denotes a specific performance profile version, in this case from the <code class="language-plaintext highlighter-rouge">Compute Optimized</code> family and <code class="language-plaintext highlighter-rouge">xlarge</code> denotes a specific amount of compute resources provided by the instance type, in this case 4 vCPUs, 7.5 GiB of RAM, 750 Mbps EBS bandwidth, etc.</p>

<p>While hyperscalers can provide predefined types with performance profiles and compute resources already assigned IaaS and virtualization projects such as OpenStack and KubeVirt can only provide the raw abstractions for operators, admins, and even vendors to then create instances of these abstractions specific to each deployment.</p>

<h2 id="instancetype-api">Instancetype API</h2>

<p>The recently renamed instancetype API and associated <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> aim to address this by providing KubeVirt users with a set of APIs and abstractions that allow them to make fewer choices when creating a <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> while still ending up with a working, performant guest at runtime.</p>

<h2 id="virtualmachineinstancetype">VirtualMachineInstancetype</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-instancetype</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">memory</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="s">128Mi</span>
</code></pre></div></div>

<p>KubeVirt now provides two instancetype based <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a>, a cluster wide <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineclusterinstancetype"><code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code></a> and a namespaced <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetype"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetype</code></a>. These <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> encapsulate the following resource related characteristics of a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> through a shared <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetypespec"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetypeSpec</code></a>:</p>

<ul>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_cpuinstancetype">CPU</a> : Required number of vCPUs presented to the guest</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_memoryinstancetype">Memory</a> : Required amount of memory presented to the guest</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_gpu">GPUs</a> : Optional list of vGPUs to passthrough</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_hostdevice">HostDevices</a>: Optional list of HostDevices to passthrough</li>
  <li><a href="`string`">IOThreadsPolicy</a> : Optional IOThreadsPolicy to be used</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_launchsecurity">LaunchSecurity</a>: Optional LaunchSecurity to be used</li>
</ul>

<p>Anything provided within an instancetype cannot be overridden within a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a>. For example, <code class="language-plaintext highlighter-rouge">CPU</code> and <code class="language-plaintext highlighter-rouge">Memory</code> are both required attributes of an instancetype. If a user makes any requests for <code class="language-plaintext highlighter-rouge">CPU</code> or <code class="language-plaintext highlighter-rouge">Memory</code> resources within their <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a>, the instancetype will conflict and the request will be rejected.</p>

<h2 id="virtualmachinepreference">VirtualMachinePreference</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-preference</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">devices</span><span class="pi">:</span>
    <span class="na">preferredDiskBus</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredInterfaceModel</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>

<p>KubeVirt also provides two further preference based <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a>, again a cluster-wide <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineclusterpreference"><code class="language-plaintext highlighter-rouge">VirtualMachineClusterPreference</code></a> and namespaced <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreference"><code class="language-plaintext highlighter-rouge">VirtualMachinePreference</code></a>. These <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> encapsulate the preferred value of any remaining attributes of a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> required to run a given workload, again this is through a shared <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreferencespec"><code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code></a>.</p>

<p>Unlike instancetypes, preferences only represent the preferred values and as such can be overridden by values in the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> provided by the user.</p>

<h2 id="virtualmachineinstancetypepreferencematcher">VirtualMachine{Instancetype,Preference}Matcher</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-vm</span>
<span class="na">spec</span><span class="pi">:</span>
<span class="pi">[</span><span class="nv">..</span><span class="pi">]</span>
  <span class="na">instancetype</span><span class="pi">:</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-instancetype</span>
  <span class="na">preference</span><span class="pi">:</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-preference</span>
<span class="pi">[</span><span class="nv">..</span><span class="pi">]</span>
</code></pre></div></div>

<p>The previous instancetype and preference CRDs are matched to a given <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> through the use of a matcher. Each matcher consists of the following:</p>

<ul>
  <li>Name (string): Name of the resource being referenced</li>
  <li>Kind (string):  Optional, defaults to the cluster wide CRD kinds of <code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code> or <code class="language-plaintext highlighter-rouge">VirtualMachineClusterPreference</code></li>
  <li>RevisionName (string) : Optional, name of a <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/controller-revision-v1/">ControllerRevision</a> containing a copy of the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetypespec"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetypeSpec</code></a> or <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreferencespec"><code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code></a> taken when the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> is first started.</li>
</ul>

<h2 id="virtualmachineinstancepreset-deprecation">VirtualMachineInstancePreset Deprecation</h2>

<p>The new instancetype API and CRDs conflict somewhat with the existing <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachineinstancepreset"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancePreset</code></a> CRD. The approach taken by the CRD has also been removed in core k8s so, as advertised on the <a href="https://groups.google.com/g/kubevirt-dev/c/eM7JaDV_EU8">mailing list</a>, I have started the <a href="https://github.com/kubevirt/kubevirt/pull/8069">process of deprecating</a> <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachineinstancepreset"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancePreset</code></a> in favor of the Instancetype CRDs listed above.</p>

<h2 id="examples">Examples</h2>

<p>The following example is taken from the <a href="https://kubevirt.io/user-guide/virtual_machines/instancetypes/">KubeVirt User Guide</a>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">$ cat &lt;&lt; EOF | kubectl apply -f -</span> 
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cmedium</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">memory</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="s">1Gi</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">devices</span><span class="pi">:</span>
    <span class="na">preferredDiskBus</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredInterfaceModel</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredRng</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">features</span><span class="pi">:</span>
    <span class="na">preferredAcpi</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">preferredSmm</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">firmware</span><span class="pi">:</span>
    <span class="na">preferredUseEfi</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">preferredUseSecureBoot</span><span class="pi">:</span> <span class="no">true</span>    
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">instancetype</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">cmedium</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">virtualMachineInstancetype</span>
  <span class="na">preference</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">virtualMachinePreference</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/containerdisks/fedora:latest</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">users:</span>
              <span class="s">- name: admin</span>
                <span class="s">sudo: ALL=(ALL) NOPASSWD:ALL</span>
                <span class="s">ssh_authorized_keys:</span>
                  <span class="s">- ssh-rsa AAAA...</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>We can compare the original <code class="language-plaintext highlighter-rouge">VirtualMachine</code> spec with that of the running <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> to confirm our instancetype and preferences have been applied using the following <code class="language-plaintext highlighter-rouge">diff</code> command:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>diff <span class="nt">--color</span> <span class="nt">-u</span> &lt;<span class="o">(</span> kubectl get vms/fedora <span class="nt">-o</span> json | jq .spec.template.spec<span class="o">)</span> &lt;<span class="o">(</span> kubectl get vmis/fedora <span class="nt">-o</span> json | jq .spec<span class="o">)</span>
<span class="o">[</span>..]
 <span class="o">{</span>
   <span class="s2">"domain"</span>: <span class="o">{</span>
-    <span class="s2">"devices"</span>: <span class="o">{}</span>,
+    <span class="s2">"cpu"</span>: <span class="o">{</span>
+      <span class="s2">"cores"</span>: 1,
+      <span class="s2">"model"</span>: <span class="s2">"host-model"</span>,
+      <span class="s2">"sockets"</span>: 1,
+      <span class="s2">"threads"</span>: 1
+    <span class="o">}</span>,
+    <span class="s2">"devices"</span>: <span class="o">{</span>
+      <span class="s2">"disks"</span>: <span class="o">[</span>
+        <span class="o">{</span>
+          <span class="s2">"disk"</span>: <span class="o">{</span>
+            <span class="s2">"bus"</span>: <span class="s2">"virtio"</span>
+          <span class="o">}</span>,
+          <span class="s2">"name"</span>: <span class="s2">"containerdisk"</span>
+        <span class="o">}</span>,
+        <span class="o">{</span>
+          <span class="s2">"disk"</span>: <span class="o">{</span>
+            <span class="s2">"bus"</span>: <span class="s2">"virtio"</span>
+          <span class="o">}</span>,
+          <span class="s2">"name"</span>: <span class="s2">"cloudinit"</span>
+        <span class="o">}</span>
+      <span class="o">]</span>,
+      <span class="s2">"interfaces"</span>: <span class="o">[</span>
+        <span class="o">{</span>
+          <span class="s2">"bridge"</span>: <span class="o">{}</span>,
+          <span class="s2">"model"</span>: <span class="s2">"virtio"</span>,
+          <span class="s2">"name"</span>: <span class="s2">"default"</span>
+        <span class="o">}</span>
+      <span class="o">]</span>,
+      <span class="s2">"rng"</span>: <span class="o">{}</span>
+    <span class="o">}</span>,
+    <span class="s2">"features"</span>: <span class="o">{</span>
+      <span class="s2">"acpi"</span>: <span class="o">{</span>
+        <span class="s2">"enabled"</span>: <span class="nb">true</span>
+      <span class="o">}</span>,
+      <span class="s2">"smm"</span>: <span class="o">{</span>
+        <span class="s2">"enabled"</span>: <span class="nb">true</span>
+      <span class="o">}</span>
+    <span class="o">}</span>,
+    <span class="s2">"firmware"</span>: <span class="o">{</span>
+      <span class="s2">"bootloader"</span>: <span class="o">{</span>
+        <span class="s2">"efi"</span>: <span class="o">{</span>
+          <span class="s2">"secureBoot"</span>: <span class="nb">true</span>
+        <span class="o">}</span>
+      <span class="o">}</span>,
+      <span class="s2">"uuid"</span>: <span class="s2">"98f07cdd-96da-5880-b6c7-1a5700b73dc4"</span>
+    <span class="o">}</span>,
     <span class="s2">"machine"</span>: <span class="o">{</span>
       <span class="s2">"type"</span>: <span class="s2">"q35"</span>
     <span class="o">}</span>,
-    <span class="s2">"resources"</span>: <span class="o">{}</span>
+    <span class="s2">"memory"</span>: <span class="o">{</span>
+      <span class="s2">"guest"</span>: <span class="s2">"1Gi"</span>
+    <span class="o">}</span>,
+    <span class="s2">"resources"</span>: <span class="o">{</span>
+      <span class="s2">"requests"</span>: <span class="o">{</span>
+        <span class="s2">"memory"</span>: <span class="s2">"1Gi"</span>
+      <span class="o">}</span>
+    <span class="o">}</span>
   <span class="o">}</span>,
+  <span class="s2">"networks"</span>: <span class="o">[</span>
+    <span class="o">{</span>
+      <span class="s2">"name"</span>: <span class="s2">"default"</span>,
+      <span class="s2">"pod"</span>: <span class="o">{}</span>
+    <span class="o">}</span>
+  <span class="o">]</span>,
   <span class="s2">"volumes"</span>: <span class="o">[</span>
     <span class="o">{</span>
       <span class="s2">"containerDisk"</span>: <span class="o">{</span>
-        <span class="s2">"image"</span>: <span class="s2">"quay.io/containerdisks/fedora:latest"</span>
+        <span class="s2">"image"</span>: <span class="s2">"quay.io/containerdisks/fedora:latest"</span>,
+        <span class="s2">"imagePullPolicy"</span>: <span class="s2">"Always"</span>
       <span class="o">}</span>,
       <span class="s2">"name"</span>: <span class="s2">"containerdisk"</span>
     <span class="o">}</span>,
</code></pre></div></div>

<h2 id="future-work">Future work</h2>

<p>There’s still plenty of work required before the API and CRDs can move from their current <code class="language-plaintext highlighter-rouge">alpha</code> version to <code class="language-plaintext highlighter-rouge">beta</code>. We have a specific <a href="https://github.com/kubevirt/kubevirt/issues/8235"><code class="language-plaintext highlighter-rouge">kubevirt/kubevirt</code> issue tracking our progress to <code class="language-plaintext highlighter-rouge">beta</code></a>. As set out there and in the <a href="https://github.com/kubevirt/community/blob/main/docs/api-graduation-guidelines.md">KubeVirt community API Graduation Phase Expecations</a>, part of this work is to seek feedback from the wider community so please do feel free to chime in there with any and all feedback on the API and CRDs.</p>

<p>You can also track our work on this API through the <a href="https://github.com/kubevirt/kubevirt/labels/area%2Finstancetype"><code class="language-plaintext highlighter-rouge">area/instancetype</code> tag</a> or my <a href="https://blog.yarwood.me.uk/tags/instancetypes/">personal blog</a> where I will be posting <a href="https://blog.yarwood.me.uk/2022/07/21/kubevirt_instancetype_update_2/">regular updates</a> and <a href="https://blog.yarwood.me.uk/2022/08/03/kubevirt_instancetype_demo_2/">demos</a> for instancetypes.</p>]]></content><author><name>Lee Yarwood</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="instancetypes" /><category term="preferences" /><category term="VirtualMachine" /><category term="VirtualMachineInstancetype" /><category term="VirtualMachinePreference" /><summary type="html"><![CDATA[An introduction to Instancetypes and preferences in KubeVirt]]></summary></entry><entry><title type="html">KubeVirt: installing Microsoft Windows 11 from an ISO</title><link href="https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html" rel="alternate" type="text/html" title="KubeVirt: installing Microsoft Windows 11 from an ISO" /><published>2022-08-02T00:00:00+00:00</published><updated>2022-08-02T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html"><![CDATA[<p>This blog post describes a simple way to deploy a Windows 11 VM with KubeVirt, using an installation ISO as a starting point.<br />
Although only tested with Windows 11, the steps described here should also work to deploy other recent versions of Windows.</p>

<h2 id="pre-requisites">Pre-requisites</h2>

<ul>
  <li>You’ll need a Kubernetes cluster with worker node(s) that have at least 6GB of available memory</li>
  <li><a href="https://kubevirt.io/user-guide">KubeVirt</a> and <a href="https://github.com/kubevirt/containerized-data-importer/blob/main/README.md">CDI</a> both deployed on the cluster</li>
  <li>A storage backend, such as <a href="https://ceph.com/">Rook Ceph</a></li>
  <li>A Windows iso. One can be found at <a href="https://www.microsoft.com/software-download/windows11">https://www.microsoft.com/software-download/windows11</a></li>
</ul>

<p>A suitable test cluster can easily be deployed thanks to KubeVirtCI by running the following commands from the <a href="https://github.com/kubevirt/kubevirt">KubeVirt source repository</a>:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBEVIRT_MEMORY_SIZE</span><span class="o">=</span>8192M
<span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBEVIRT_STORAGE</span><span class="o">=</span>rook-ceph-default
<span class="nv">$ </span>make cluster-up <span class="o">&amp;&amp;</span> make cluster-sync
</code></pre></div></div>

<h2 id="preparation">Preparation</h2>

<p>Before the virtual machine can be created, we need to setup storage volumes for the ISO and the drive, and write the appropriate VM(I) yaml.</p>

<ol>
  <li>
    <p>Uploading the ISO to a PVC</p>

    <p>KubeVirt provides a simple tool that is able to do that for us: <code class="language-plaintext highlighter-rouge">virtctl</code>.<br />
Here’s the command to upload the ISO, just replace <code class="language-plaintext highlighter-rouge">/storage/win11.iso</code> with the path to your Windows 11 ISO:
<code class="language-plaintext highlighter-rouge">virtctl image-upload pvc win11cd-pvc --size 6Gi --image-path=/storage/win11.iso --insecure</code></p>
  </li>
  <li>
    <p>Creating a persistent volume to use as the Windows drive</p>

    <p>This will depend on the storage configuration of your cluster.
The following yaml, to apply to the cluster using <code class="language-plaintext highlighter-rouge">kubectl create</code>, should work just fine on a KubeVirtCI cluster:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">task-pv-volume</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">local</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">hostpath</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">15Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/tmp/hostImages/win11"</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>Microsoft actually <a href="https://docs.microsoft.com/en-us/windows/whats-new/windows-11-requirements">recommends</a> at least 64GB of storage.
But, unlike some other requirements, the installer will accept smaller disks.
This is convenient when testing with KubeVirtCI, as nodes only have about 20GB of free space.
However, please bear in mind that such a small drive should only be used for testing purposes, and might lead to instabilities.</p>


</div></div>
<ol>
  <li>
    <p>Creating a persistent volume claim (PVC) for the drive</p>

    <p>Once again, your milage may vary, but the following PVC yaml works fine on KubeVirtCI:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">disk-windows</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">15Gi</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">hostpath</span>
</code></pre></div>    </div>

    <p>The name of PVC, <code class="language-plaintext highlighter-rouge">disk-windows</code> here, will be used in the yaml of the VM(I) as the main volume.</p>
  </li>
  <li>
    <p>Creating the VM(I) yaml file</p>

    <p>KubeVirt already includes an example <a href="https://github.com/kubevirt/kubevirt/blob/main/examples/vmi-windows.yaml">Windows VMI yaml file</a>, which we’ll use as a starting point here for convenience.<br />
Using a VMI yaml is more than enough for testing purposes, however for more serious applications you might want to consider changing it into a VM.</p>

    <p>First, in the yaml above, bump the memory up to 4Gi, which is a hard requirement of Windows 11. (Windows 10 is happy with 2Gi).</p>

    <p>Then, let’s add the ISO created above.
Add is as a cdrom in the disks section:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
    <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
</code></pre></div>    </div>
    <p>And the corresponding volume at the bottom:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">win11cd-pvc</span>
</code></pre></div>    </div>
    <p>Note that the names should match, and that the <code class="language-plaintext highlighter-rouge">claimName</code> is what we used in the <code class="language-plaintext highlighter-rouge">virtctl</code> command above.</p>

    <p>Here is what the VMI looks like after those changes:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstance</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">special</span><span class="pi">:</span> <span class="s">vmi-windows</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vmi-windows</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">domain</span><span class="pi">:</span>
    <span class="na">clock</span><span class="pi">:</span>
      <span class="na">timer</span><span class="pi">:</span>
        <span class="na">hpet</span><span class="pi">:</span>
          <span class="na">present</span><span class="pi">:</span> <span class="no">false</span>
        <span class="na">hyperv</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">pit</span><span class="pi">:</span>
          <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">delay</span>
        <span class="na">rtc</span><span class="pi">:</span>
          <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">catchup</span>
      <span class="na">utc</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">cpu</span><span class="pi">:</span>
      <span class="na">cores</span><span class="pi">:</span> <span class="m">2</span>
    <span class="na">devices</span><span class="pi">:</span>
      <span class="na">disks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
          <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
      <span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
          <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
      <span class="na">interfaces</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">model</span><span class="pi">:</span> <span class="s">e1000</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
      <span class="na">tpm</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">features</span><span class="pi">:</span>
      <span class="na">acpi</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">apic</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">hyperv</span><span class="pi">:</span>
        <span class="na">relaxed</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">spinlocks</span><span class="pi">:</span>
          <span class="na">spinlocks</span><span class="pi">:</span> <span class="m">8191</span>
        <span class="na">vapic</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">smm</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">firmware</span><span class="pi">:</span>
      <span class="na">bootloader</span><span class="pi">:</span>
        <span class="na">efi</span><span class="pi">:</span>
          <span class="na">secureBoot</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">uuid</span><span class="pi">:</span> <span class="s">5d307ca9-b3ef-428c-8861-06e72d69f223</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s">4Gi</span>
  <span class="na">networks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
    <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">disk-windows</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">win11cd-pvc</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>When customizing this VMI definition or creating your own, please keep in mind that the TPM device and the UEFI firmware with SecureBoot are both hard requirements of Windows 11.
Not having them will cause the Windows 11 installation to fail early. Please also note that the SMM CPU feature is required for UEFI + SecureBoot.
However, they can all be omitted in the case of a Windows 10 VM(I).
Finally, we do not currently support TPM persistence, so any secret stored in the emulated TPM will be lost next time you boot the VMI.
For example, do not enable BitLocker, as it will fail to find the encryption key next boot and you will have to manually enter the (55 characters!) recovery key each boot.</p>


</div></div>
<h2 id="windows-installation">Windows installation</h2>

<p>You should now be able to create the VMI and start the Windows installation process.<br />
Just use kubectl to start the VMI created above: <code class="language-plaintext highlighter-rouge">kubectl create -f vmi-windows.yaml</code>.<br />
Shortly after, open a VNC session to it using <code class="language-plaintext highlighter-rouge">virtctl vnc vmi-windows</code> (keep trying until the VMI is running and the VNC session pops up).<br />
You should now see the boot screen, and shortly after a prompt to “Press any key to boot from CD or DVD…”. You have a few seconds to do so or the VM will fail to boot.
Then just follow the steps to install Windows.</p>

<h2 id="virtio-drivers-installation-optional">VirtIO drivers installation (optional)</h2>

<p>Once Windows is installed, it’s a good ideas to install the <a href="http://www.linux-kvm.org/page/Virtio">VirtIO</a> drivers inside the VM, as they can drastically improve performance.
The latest version can be downloaded <a href="https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/latest-virtio/">here</a>.
<code class="language-plaintext highlighter-rouge">virtio-win-gt-x64.msi</code> is the simplest package to install, as you just have to run it as Administrator.</p>

<p>Alternatively, KubeVirt has a containerdisk image that can be mounted inside the VM.<br />
To use it, just add a simple cdrom disk to the VMI, like:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
    <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>
<p>and the volume:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirt/virtio-container-disk</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>
<p>When using KubeVirtCI, a local copy of the image is also available at <code class="language-plaintext highlighter-rouge">registry:5000/kubevirt/virtio-container-disk:devel</code>.</p>

<h2 id="further-performance-improvements">Further performance improvements</h2>

<p>Windows is quite resource-hungry, and you might find that the VM created above is too slow, even with the VirtIO drivers installed.<br />
Here are a few steps you can take to improve things:</p>
<ul>
  <li>Increasing the RAM is always a good idea, if you have enough available of course.</li>
  <li>Increasing the number of CPUs, and/or using CPUManager to assign dedicated CPU to the VM should also help a lot.</li>
  <li>Once the VirtIO drivers are installed, the main drive can also be switched from <code class="language-plaintext highlighter-rouge">sata</code> to <code class="language-plaintext highlighter-rouge">virtio</code>, and the attached CDROMs can be removed.</li>
</ul>]]></content><author><name>Jed Lejosne</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="Microsoft Windows kubernetes" /><category term="Microsoft Windows container" /><category term="Windows" /><summary type="html"><![CDATA[This blog post describes how to create a Microsoft Windows 11 virtual machine with KubeVirt]]></summary></entry><entry><title type="html">KubeVirt at KubeCon EU 2022</title><link href="https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022.html" rel="alternate" type="text/html" title="KubeVirt at KubeCon EU 2022" /><published>2022-06-28T00:00:00+00:00</published><updated>2022-06-28T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022.html"><![CDATA[<p>KubeCon EU was in Valencia, Spain this year from May 16-20. For many of the 7000+ physical attendees, it was their first in-person conference in several years. With luck, it was the first of many more to come, as KubeCon is a rare opportunity to learn about, from, and with a rich variety of adopters, communities, and vendors that make up the open source and cloud native ecosystem.</p>

<p>The KubeVirt community presented two sessions, both on Wednesday May 18th:</p>

<ol>
  <li>A Virtual Open Office Hours session, and</li>
  <li>A Maintainer Track session: ‘It’s All for the Users. More Durable, Secure, and Pluggable. KubeVirt v0.53’</li>
</ol>

<h2 id="virtual-open-office-hours">Virtual Open Office Hours</h2>

<p>This was a 45-minute project virtual session, hosted by the CNCF. This was on the Bevy platform (which will be familiar to KubeVirt Summit attendees from the past two years) and we had five lovely people from the KubeVirt community ready with a variety of demos and presentations and to answer questions from attendees:+
Alice Frosi, Itamar Holder, Miguel Duarte de Mora Barroso, Luboslav Pivarc, and Bartosz Rybacki</p>

<p>This was an opportunity for KubeCon attendees (virtual and physical) to ask questions and discuss any topics, and our presenters covered the following: Introduction to KubeVirt, live migration, Istio integration, and CDI hotplug/resize.
Despite some initial technical issues and improvised changes, this session went really well. We had about ~25 consistent attendees, and we received a good range of Q&amp;A and interaction with the attendees on all topics presented. It was a very solid 45 minutes.
Unfortunately, due to a miscommunication, there is no recording of this session.</p>

<p>A huge thanks to the presenters for their time and collaboration in preparing for this.</p>

<h2 id="maintainer-track">Maintainer Track</h2>

<p>Later that day, on the Maintainer Track, Alice also gave an in-depth breakdown of a whole slew of new KubeVirt features and showed a demo with the KubeVirt Cluster API: deploying Kubernetes on top of Kubernetes.
You can watch <a href="https://youtu.be/L9H0pz5PpKo">the CNCF recording here</a>, and download the <a href="https://kccnceu2022.sched.com/event/ytu1">demo video and slides</a> that are available from the schedule.</p>

<p>There was a healthy amount of questions, both during Q&amp;A and after the talk. The participants were particularly interested to know how to prepare and customize VM disks with KubeVirt, how to run Windows VM, especially combined with GPUs, and how to expose the Kubernetes API service of a deployed cluster to the KubeVirt cluster API provider outside of the KubeVirt VM. There were additional questions on the status of TPM support and VM migration when the hosting node goes down.</p>

<h2 id="thank-you">Thank you!</h2>

<p>Big thanks again to our presenters: Alice Frosi, Itamar Holder, Miguel Duarte de Mora Barroso, Luboslav Pivarc, and Bartosz Rybacki.
And everyone who attended the sessions, listened, and asked great questions.</p>

<h2 id="want-to-see-more-from-kubecon-eu-2022">Want to see more from KubeCon EU 2022?</h2>

<p>If you’re interested in seeing more photos and recordings from the event:</p>

<ul>
  <li><a href="https://www.flickr.com/photos/143247548@N03/albums/72177720298987342">CNCF’s Photo album (Flickr) of the event</a>.</li>
  <li><a href="https://www.youtube.com/c/cloudnativefdn">The CNCF video recordings of the sessions on Youtube</a>.</li>
  <li>And <a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/schedule/">the event schedule</a> to help you find sessions.</li>
</ul>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><category term="KubeCon" /><summary type="html"><![CDATA[A short report on the two sessions KubeVirt presented at KubeCon EU 2022]]></summary></entry><entry><title type="html">Load-balancer for virtual machines on bare metal Kubernetes clusters</title><link href="https://kubevirt.io//2022/Virtual-Machines-with-MetalLB.html" rel="alternate" type="text/html" title="Load-balancer for virtual machines on bare metal Kubernetes clusters" /><published>2022-04-03T00:00:00+00:00</published><updated>2022-04-03T00:00:00+00:00</updated><id>https://kubevirt.io//2022/Virtual-Machines-with-MetalLB</id><content type="html" xml:base="https://kubevirt.io//2022/Virtual-Machines-with-MetalLB.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Over the last year, Kubevirt and MetalLB have shown to be powerful duo in order to support fault-tolerant access to an application on virtual machines through an external IP address. 
As a Cluster administrator using an on-prem cluster without a network load-balancer, now it’s possible to use MetalLB operator to provide load-balancer capabilities (with Services of type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>) to virtual machines.</p>

<h2 id="metallb">MetalLB</h2>

<p><a href="https://metallb.universe.tf/">MetalLB</a> allows you to create Kubernetes services of type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>, and provides network load-balancer implementation in on-prem clusters that don’t run on a cloud provider.
MetalLB is responsible for assigning/unassigning an external IP Address to your service, using IPs from pre-configured pools. In order for the external IPs to be announced externally, MetalLB works in 2 modes, Layer 2 and BGP:</p>

<ul>
  <li>
    <p>Layer 2 mode (ARP/NDP):</p>

    <p>This mode - which actually does not implement real load-balancing behavior - provides a failover mechanism where a single node owns the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service, until it fails, triggering another node to be chosen as the service owner. This configuration mode makes the IPs reachable from the local network.<br />
In this method, the MetalLB speaker pod announces the IPs in ARP (for IPv4) and NDP (for IPv6) protocols over the host network. From a network perspective, the node owning the service appears to have multiple IP addresses assigned to a network interface. After traffic is routed to the node, the service proxy sends the traffic to the application pods.</p>
  </li>
  <li>
    <p>BGP mode:</p>

    <p>This mode provides real load-balancing behavior, by establishing BGP peering sessions with the network routers - which advertise the external IPs of the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service, distributing the load over the nodes.</p>
  </li>
</ul>

<p>To read more on MetalLB concepts, implementation and limitations, please read <a href="https://metallb.universe.tf/concepts/">its documentation</a>.</p>

<h2 id="demo-virtual-machine-with-external-ip-and-metallb-load-balancer">Demo: Virtual machine with external IP and MetalLB load-balancer</h2>

<p>With the following recipe we will end up with a nginx server running on a virtual machine, accessible outside the cluster using MetalLB load-balancer with Layer 2 mode.</p>

<h3 id="demo-environment-setup">Demo environment setup</h3>

<p>We are going to use <a href="https://kind.sigs.k8s.io">kind</a> provider as an ephemeral Kubernetes cluster.</p>

<p>Prerequirements:</p>
<ul>
  <li>First install kind on your machine following its <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">installation guide</a>.</li>
  <li>To use kind, you will also need to <a href="https://docs.docker.com/install/">install docker</a>.</li>
</ul>

<h4 id="external-ips-on-macos-and-windows">External IPs on macOS and Windows</h4>

<p>This demo runs Docker on Linux, which allows sending traffic directly to the load-balancer’s external IP if the IP space is within the docker IP space.
On macOS and Windows however, docker does not expose the docker network to the host, rendering the external IP unreachable from other kind nodes. In order to workaround this, one could expose pods and services using extra port mappings as shown in the extra port mappings section of kind’s <a href="https://kind.sigs.k8s.io/docs/user/configuration#extra-port-mappings">Configuration Guide</a>.</p>

<h3 id="deploying-cluster">Deploying cluster</h3>

<p>To start a kind cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kind create cluster
</code></pre></div></div>

<p>In order to interact with the specific cluster created:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl cluster-info <span class="nt">--context</span> kind-kind
</code></pre></div></div>

<h3 id="installing-components">Installing components</h3>

<h4 id="installing-metallb-on-the-cluster">Installing MetalLB on the cluster</h4>

<p>There are <a href="https://metallb.universe.tf/installation/">many ways</a> to install MetalLB. For the sake of this example, we will install MetalLB via manifests. To do this, follow this <a href="https://metallb.universe.tf/installation/#installation-by-manifest">guide</a>. 
Confirm successful installation by waiting for MetalLB pods to have a status of Running:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-n</span> metallb-system <span class="nt">--watch</span>
</code></pre></div></div>

<h4 id="installing-kubevirt-on-the-cluster">Installing Kubevirt on the cluster</h4>

<p>Following Kubevirt <a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a> to install released version v0.51.0</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span>v0.51.0
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="network-resources-configuration">Network resources configuration</h3>

<h4 id="setting-address-pool-to-be-used-by-the-loadbalancer">Setting Address Pool to be used by the LoadBalancer</h4>

<p>In order to complete the Layer 2 mode configuration, we need to set a range of IP addresses for the LoadBalancer to use.
On Linux we can use the docker kind network (macOS and Windows users see <a href="#external-ips-on-macos-and-windows">External IPs Prerequirement</a>), so by using this command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network inspect <span class="nt">-f</span> <span class="s1">''</span> kind
</code></pre></div></div>

<p>You should get the subclass you can set the IP range from. The output should contain a cidr such as 172.18.0.0/16.
Using this result we will create the following Layer 2 address pool with 172.18.1.1-172.18.1.16 range:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metallb-system</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">config</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">address-pools:</span>
    <span class="s">- name: addresspool-sample1</span>
      <span class="s">protocol: layer2</span>
      <span class="s">addresses:</span>
      <span class="s">- 172.18.1.1-172.18.1.16</span>
<span class="s">EOF</span>
</code></pre></div></div>

<h3 id="network-utilization">Network utilization</h3>

<h4 id="spin-up-a-virtual-machine-running-nginx">Spin up a Virtual Machine running Nginx</h4>

<p>Now it’s time to start-up a virtual machine running nginx using the following yaml.
The virtual machine has a <code class="language-plaintext highlighter-rouge">metallb-service=nginx</code> we created to use when creating the service.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora-nginx</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
          <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirt/fedora-cloud-container-disk-demo</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
        <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
              <span class="s">packages:</span>
                <span class="s">- nginx</span>
              <span class="s">runcmd:</span>
                <span class="s">- [ "systemctl", "enable", "--now", "nginx" ]</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="s">EOF</span>
</code></pre></div></div>

<h4 id="expose-the-virtual-machine-with-a-typed-loadbalancer-service">Expose the virtual machine with a typed <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service</h4>

<p>When creating the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> typed service, we need to remember annotating the address-pool we want to use 
<code class="language-plaintext highlighter-rouge">addresspool-sample1</code> and also add the selector <code class="language-plaintext highlighter-rouge">metallb-service: nginx</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">metallb-nginx-svc</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">metallb.universe.tf/address-pool</span><span class="pi">:</span> <span class="s">addresspool-sample1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">externalTrafficPolicy</span><span class="pi">:</span> <span class="s">Local</span>
  <span class="na">ipFamilies</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">IPv4</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tcp-5678</span>
      <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">5678</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>Notice that the service got assigned with an external IP from the range assigned by the address pool:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get service <span class="nt">-n</span> default metallb-nginx-svc
</code></pre></div></div>

<p>Example output:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
metallb-nginx-svc   LoadBalancer   10.96.254.136   172.18.1.1    5678:32438/TCP   53s
</code></pre></div></div>

<h4 id="access-the-virtual-machine-from-outside-the-cluster">Access the virtual machine from outside the cluster</h4>

<p>Finally, we can check that the nginx server is accessible from outside the cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-s</span> <span class="nt">-o</span> /dev/null 172.18.1.1:5678 <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"URL exists"</span>
</code></pre></div></div>

<p>Example output:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>URL exists
</code></pre></div></div>
<p>Note that it may take a short while for the URL to work after setting the service.</p>

<h2 id="doing-this-on-your-own-cluster">Doing this on your own cluster</h2>

<p>Moving outside the demo example, one who would like use MetalLB on their real life cluster, should also take other considerations in mind:</p>
<ul>
  <li>User privileges: you should have <code class="language-plaintext highlighter-rouge">cluster-admin</code> privileges on the cluster - in order to install MetalLB.</li>
  <li>IP Ranges for MetalLB: getting IP Address pools allocation for MetalLB depends on your cluster environment:
    <ul>
      <li>If you’re running a bare-metal cluster in a shared host environment, you need to first reserve this IP Address pool from your hosting provider.</li>
      <li>Alternatively, if you’re running on a private cluster, you can use one of the private IP Address spaces (a.k.a RFC1918 addresses). Such addresses are free, and work fine as long as you’re only providing cluster services to your LAN.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog post we used MetalLB to expose a service using an external IP assigned to a virtual machine. 
This illustrates how virtual machine traffic can be load-balanced via a service.</p>]]></content><author><name>Ram Lavi</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="load-balancer" /><category term="MetalLB" /><summary type="html"><![CDATA[This post illustrates setting up a virtual machine with MetalLB LoadBalancer service.]]></summary></entry><entry><title type="html">Dedicated migration network in KubeVirt</title><link href="https://kubevirt.io//2022/Dedicated-migration-network.html" rel="alternate" type="text/html" title="Dedicated migration network in KubeVirt" /><published>2022-01-25T00:00:00+00:00</published><updated>2022-01-25T00:00:00+00:00</updated><id>https://kubevirt.io//2022/Dedicated-migration-network</id><content type="html" xml:base="https://kubevirt.io//2022/Dedicated-migration-network.html"><![CDATA[<p>Since version 0.49, KubeVirt supports live migrating VMIs over a separate network than the one Kubernetes is running on.</p>

<p>Running migrations over a dedicated network is a great way to increase migration bandwidth and reliability.</p>

<p>This article gives an overview of the feature as well as a concrete example. For more technical information, refer to the <a href="https://kubevirt.io/user-guide/operations/live_migration/#using-a-different-network-for-migrations">KubeVirt documentation</a>.</p>

<h2 id="hardware-configuration">Hardware configuration</h2>

<p>The simplest way to use the feature is to find an unused NIC on every worker node, and to connect them all to the same switch.</p>

<p>All NICs must have the same name. If they don’t, they should be permanently renamed.
The process for renaming NICs varies depending on your operating system, refer to its documentation if you need help.</p>

<p>Adding servers to the network for services like DHCP or DNS is an option but it is not required.
If a DHCP is running, it is best if it doesn’t provide routes to other networks / the internet, to keep the migration network isolated.</p>

<h2 id="cluster-configuration">Cluster configuration</h2>

<p>The interface between the physical network and KubeVirt is a NetworkAttachmentDefinition (NAD), created in the namespace where KubeVirt is installed.</p>

<p>The implementation of the NAD is up to the admin, as long as it provides a link to the secondary network.
The admin must also ensure that the NAD is able to provide cluster-wide IPs, either through a physical DHCP, or with another CNI plugin like <a href="https://github.com/k8snetworkplumbingwg/whereabouts">whereabouts</a></p>

<p>Important: the subnet used here must be completely distinct from the ones used by the main Kubernetes network, to ensure proper routing.</p>

<h2 id="testing">Testing</h2>

<p>If you just want to test the feature, KubeVirtCI supports the creation of multiple nodes, as well as secondary networks.
All you need is to define the right environment variables before starting the cluster.</p>

<p>See the example below for more info (note that text in the “video” can actually be selected and copy/pasted).</p>

<h2 id="example">Example</h2>

<p>Here is a quick <a href="https://asciinema.org/a/464272">example</a> of a dual-node KubeVirtCI cluster running a migration over a secondary network.</p>

<p>The description of the clip includes more detailed information about the steps involved.</p>]]></content><author><name>Jed Lejosne</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="live migration" /><category term="dedicated network" /><summary type="html"><![CDATA[KubeVirt now supports using a separate network for live migrations]]></summary></entry><entry><title type="html">KubeVirt Summit is coming back!</title><link href="https://kubevirt.io//2022/KubeVirt-Summit-2022.html" rel="alternate" type="text/html" title="KubeVirt Summit is coming back!" /><published>2022-01-24T00:00:00+00:00</published><updated>2022-01-24T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-Summit-2022</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-Summit-2022.html"><![CDATA[<p>The second online <a href="/summit/">KubeVirt Summit</a> is coming on February 16, 2022!</p>

<h2 id="when">When</h2>

<p>The event will take place online during two half-days:</p>

<ul>
  <li>Dates: February 16 and 17, 2022.</li>
  <li>Time: 14:00 – 19:00 UTC (9:00–14:00 EST, 15:00–20:00 CET)</li>
</ul>

<h2 id="register">Register</h2>

<p><a href="/summit/">KubeVirt Summit</a> is hosted on Community.CNCF.io. Because of how that platform works, you need to register for each of the two days of the summit independantly:</p>

<ul>
  <li><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2022-day-1/">Register for Day 1</a></li>
  <li><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2022-day-2/">Register for Day 2</a></li>
</ul>

<p>You will need to create an account with CNCF.io if you have not before. Attendance is free.</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our <a href="/community">community page</a>.</p>

<p>We are looking forward to meeting you there!</p>]]></content><author><name>Chandler Wilkerson</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's second annual dedicated online event]]></summary></entry><entry><title type="html">Running real-time workloads with improved performance</title><link href="https://kubevirt.io//2021/Running-Realtime-Workloads.html" rel="alternate" type="text/html" title="Running real-time workloads with improved performance" /><published>2021-10-13T00:00:00+00:00</published><updated>2021-10-13T00:00:00+00:00</updated><id>https://kubevirt.io//2021/Running-Realtime-Workloads</id><content type="html" xml:base="https://kubevirt.io//2021/Running-Realtime-Workloads.html"><![CDATA[<h2 id="motivation">Motivation</h2>

<p>It has been possible in KubeVirt for some time already to run a VM running with a RT kernel, however the performance of such workloads never achieved parity against running on top of a bare metal host virtualized. With the availability of NUMA and CPUManager as features in KubeVirt, we were close to a point where we had almost all the ingredients to deliver the <a href="https://www.libvirt.org/kbase/kvm-realtime.html">recommended</a> tunings in libvirt for achieving the low CPU latency needed for such workloads. We were missing two important settings:</p>

<ul>
  <li>The ability to configure the VCPUs to run with real-time scheduling policy.</li>
  <li>Lock the VMs huge pages in RAM to prevent swapping.</li>
</ul>

<h2 id="setting-up-the-environment">Setting up the Environment</h2>

<p>To achieve the lowest latency possible in a given environment, first it needs to be configured to allow its resources to be consumed efficiently.</p>

<h3 id="the-cluster">The Cluster</h3>

<p>The target node has to be configured to reserve memory for hugepages and the kernel to allow threads to run with real-time scheduling policy. The memory can be reserved as a <a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html">kernel boot parameter</a> or by changing the kernel’s page count at <a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html">runtime</a>.</p>

<p>The kernel’s runtime scheduling limit can be adjusted either by installing a real-time kernel in the node (the recommended option), or changing the kernel’s setting <code class="language-plaintext highlighter-rouge">kernel.sched_rt_runtime_us</code> to equal -1, to allow for unlimited runtime of real-time scheduled threads. This kernel setting defines the time period to be devoted to running real-time threads. KubeVirt will detect if the node has been configured with unlimited runtime and will label the node with <code class="language-plaintext highlighter-rouge">kubevirt.io/realtime</code> to highlight the capacity of running real-time workloads. Later on we’ll come back to this label when we talk about how the workload is scheduled.</p>

<p>It is also recommended tuning the node’s BIOS settings for optimal real-time performance is also recommended to achieve even lower CPU latencies. Consult with your hardware provider to obtain the information on how to best tune your equipment.</p>

<h3 id="kubevirt">KubeVirt</h3>

<p>The VM will require to be granted fully dedicated CPUs and be able to use huge pages. These requirements can be achieved in KubeVirt by enabling the feature gates of CPUManager and NUMA in the KubeVirt CR. There is no dedicated feature gate to enable the new real-time optimizations.</p>

<h2 id="the-manifest">The Manifest</h2>

<p>With the cluster configured to provide the dedicated resources for the workload, it’s time to review an example of a VM manifest using the optimizations for low CPU latency. The first focus is to reduce the VM’s I/O by limiting it’s devices to only serial console:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.devices.autoattachSerialConsole</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">spec.domain.devices.autoattachMemBalloon</span><span class="pi">:</span> <span class="no">false</span>
<span class="na">spec.domain.devices.autoattachGraphicsDevice</span><span class="pi">:</span> <span class="no">false</span>
</code></pre></div></div>

<p>The pod needs to have a guaranteed QoS for its memory and CPU resources, to make sure that the CPU manager will dedicate the requested CPUs to the pod.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.resources.request.cpu</span><span class="pi">:</span> <span class="m">2</span>
<span class="na">spec.domain.resources.request.memory</span><span class="pi">:</span> <span class="s">1Gi</span>
<span class="na">spec.domain.resources.limits.cpu</span><span class="pi">:</span> <span class="m">2</span>
<span class="na">spec.domain.resources.limits.memory</span><span class="pi">:</span> <span class="s">1Gi</span>
</code></pre></div></div>

<p>Still on the CPU front, we add the settings to instruct the KVM to give a clear visibility of the host’s features to the guest, request the CPU manager in the node to isolate the assigned CPUs and to make sure that the emulator and IO threads in the VM run in their own dedicated VCPU rather than sharing the computational time with the workload.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.cpu.model</span><span class="pi">:</span> <span class="s">host-passthrough</span>
<span class="na">spec.domain.cpu.dedicateCpuPlacement</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">spec.domain.cpu.isolateEmulatorThread</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">spec.domain.cpu.ioThreadsPolicy</span><span class="pi">:</span> <span class="s">auto</span>
</code></pre></div></div>

<p>We also request the huge pages size and guaranteed NUMA topology that will pin the CPU and memory resources to a single NUMA node in the host. The Kubernetes scheduler will perform due diligence to schedule the pod in a node with enough free huge pages of the given size.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.cpu.numa.guestMappingPassthrough</span><span class="pi">:</span> <span class="pi">{}</span>
<span class="na">spec.domain.memory.hugepages.pageSize</span><span class="pi">:</span> <span class="s">1Gi</span>
</code></pre></div></div>

<p>Lastly, we define the new real-time settings to instruct KubeVirt to apply the real-time scheduling policy for the pinned VCPUs and lock the process memory to avoid from being swapped by the host. In this example, we’ll configure the workload to only apply the real-time scheduling policy to VCPU 0.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.cpu.realtime.mask</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div></div>

<p>Alternatively, if no <code class="language-plaintext highlighter-rouge">mask</code> value is specified, all requested CPUs will be configured for real-time scheduling.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.cpu.realtime</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<p>The following yaml is a complete manifest including all the settings we just reviewed.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">fedora-realtime</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora-realtime</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">poc</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">fedora-realtime</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">autoattachSerialConsole</span><span class="pi">:</span> <span class="no">true</span>
          <span class="na">autoattachMemBalloon</span><span class="pi">:</span> <span class="no">false</span>
          <span class="na">autoattachGraphicsDevice</span><span class="pi">:</span> <span class="no">false</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1Gi</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="m">2</span>
          <span class="na">limits</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1Gi</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="m">2</span>
        <span class="na">cpu</span><span class="pi">:</span>
          <span class="na">model</span><span class="pi">:</span> <span class="s">host-passthrough</span>
          <span class="na">dedicatedCpuPlacement</span><span class="pi">:</span> <span class="no">true</span>
          <span class="na">isolateEmulatorThread</span><span class="pi">:</span> <span class="no">true</span>
          <span class="na">ioThreadsPolicy</span><span class="pi">:</span> <span class="s">auto</span>
          <span class="na">features</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tsc-deadline</span>
              <span class="na">policy</span><span class="pi">:</span> <span class="s">require</span>
          <span class="na">numa</span><span class="pi">:</span>
            <span class="na">guestMappingPassthrough</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">realtime</span><span class="pi">:</span>
            <span class="na">mask</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0"</span>
        <span class="na">memory</span><span class="pi">:</span>
          <span class="na">hugepages</span><span class="pi">:</span>
            <span class="na">pageSize</span><span class="pi">:</span> <span class="s">1Gi</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-realtime-container-disk:20211008_5a22acb18</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">password: fedora</span>
            <span class="s">chpasswd: { expire: False }</span>
            <span class="s">bootcmd:</span>
              <span class="s">- tuned-adm profile realtime</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
</code></pre></div></div>

<h2 id="the-deployment">The Deployment</h2>

<p>Because the manifest has enabled the real-time setting, when deployed KubeVirt applies the node label selector so that the Kubernetes scheduler will place the deployment in a node that is able to run threads with real-time scheduling policy (node label <code class="language-plaintext highlighter-rouge">kubevirt.io/realtime</code>). But there’s more, because the manifest also specifies the pod’s resource need of dedicated CPUs, KubeVirt will also add the node selector of <code class="language-plaintext highlighter-rouge">cpumanager=true</code> to guarantee that the pod is able to use the assigned CPUs alone. And finally, the scheduler also takes care of guaranteeing that the target node has sufficient free huge pages of the specified size (1Gi in our example) to satisfy the memory requested. With all these validations checked, the pod is successfully scheduled.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>Being able to run real-time workloads in KubeVirt with lower CPU latency opens new possibilities and expands the use cases where KubeVirt can assist in migrating legacy VMs into the cloud. Real-time workloads are extremely sensitive to the amount of layers between the bare metal and its runtime: the more layers in between, the higher the latency will be. The changes introduced in KubeVirt help reduce such waste and provide lower CPU latencies as the hardware is more efficiently tuned.</p>]]></content><author><name>Jordi Gil</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="real-time" /><category term="NUMA" /><category term="CPUManager" /><summary type="html"><![CDATA[This blog post details the various enhancements made to improve the performance of real-time workloads in KubeVirt]]></summary></entry><entry><title type="html">Import AWS AMIs as KubeVirt Golden Images</title><link href="https://kubevirt.io//2021/Importing-EC2-to-KubeVirt.html" rel="alternate" type="text/html" title="Import AWS AMIs as KubeVirt Golden Images" /><published>2021-09-21T00:00:00+00:00</published><updated>2021-09-21T00:00:00+00:00</updated><id>https://kubevirt.io//2021/Importing-EC2-to-KubeVirt</id><content type="html" xml:base="https://kubevirt.io//2021/Importing-EC2-to-KubeVirt.html"><![CDATA[<h2 id="breaking-out">Breaking Out</h2>

<p>There comes a point where an operations team has invested so heavily in a Iaas platform that they are effectively locked into that platform. For example, here’s one scenario outlining how this can happen. An operations team has created automation around building VM images and keeping images up-to-date. In AWS that automation likely involves starting an EC2 instance, injecting some application logic into that instance, sealing the instance’s boot source as an AMI, and finally copying that AMI around to all the AWS regions the team deploys in.</p>

<p>If the team was interested in evaluating KubeVirt as an alternative Iaas platform to AWS’s EC2, given the team’s existing tooling there’s not a clear path for doing this. It’s that scenario where the tooling in the <a href="https://github.com/davidvossel/kubevirt-cloud-import">kubevirt-cloud-import</a> project comes into play.</p>

<h2 id="kubevirt-cloud-import">Kubevirt Cloud Import</h2>

<p>The <a href="https://github.com/davidvossel/kubevirt-cloud-import">KubeVirt Cloud Import</a> project explores the practicality of transitioning VMs from various cloud providers into KubeVirt. As of writing this, automation for exporting AMIs from EC2 into KubeVirt works, and it’s really not all that complicated.</p>

<p>This blog post will explore the fundamentals of how AMIs are exported, and how the KubeVirt Cloud Import project leverages these techniques to build automation pipelines.</p>

<h2 id="nuts-and-bolts-of-importing-amis">Nuts and Bolts of Importing AMIs</h2>

<h3 id="official-aws-ami-export-support">Official AWS AMI Export Support</h3>

<p>AWS supports an <a href="https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html">api</a> for exporting AMIs as a file to an s3 bucket. This support works quite well, however there’s a long list of <a href="https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html#limits-image-export">limitations</a> that impact what AMIs are eligible for export. The most limiting of those items is the one that prevents any image built from an AMI on the marketplace from being eligible for the official export support.</p>

<h3 id="unofficial-aws-export-support">Unofficial AWS export Support</h3>

<p>Regardless of what AWS officially supports or not, there’s absolutely nothing preventing someone from exporting an AMI’s contents themselves. The technique just involves creating an EC2 instance, attaching an EBS volume (containing the AMI contents) as a block device, then streaming that block devices contents where ever you want.</p>

<p>Theoretically, the steps roughly look like this.</p>

<ul>
  <li>Convert AMI to a volume by finding the underlying AMI’s snapshot and converting it to an EBS volume.</li>
  <li>Create an EC2 instance with the EBS volume containing the AMI contents as a secondary data device.</li>
  <li>Within the EC2 guest, copy the EBS device’s contents as a disk img <code class="language-plaintext highlighter-rouge">dd if=/dev/xvda of=/tmp/disk/disk.img</code></li>
  <li>Then upload the disk image to an object store like s3. <code class="language-plaintext highlighter-rouge">aws s3 cp /tmp/disk/disk.img s3://my-b1-bucket/ upload: ../tmp/disk/disk.img to s3://my-b1-bucket/disk.img</code></li>
</ul>

<h3 id="basics-of-importing-data-into-kubevirt">Basics of Importing Data into KubeVirt</h3>

<p>Once a disk image is in s3, a KubeVirt companion project called the <a href="https://github.com/kubevirt/containerized-data-importer">Containerized Data Importer</a> (or CDI for short) can be used to import the disk from s3 into a PVC within the KubeVirt cluster. This import flow can be expressed as a CDI DataVolume custom resource.</p>

<p>Below is an example yaml for importing s3 contents into a PVC using a DataVolume</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cdi.kubevirt.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">DataVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">example-import-dv"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">source</span><span class="pi">:</span>
      <span class="na">s3</span><span class="pi">:</span>
         <span class="na">url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">https://s3.us-west-2.amazonaws.com/my-ami-exports/kubevirt-image-exports/export-ami-0dc4e69702f74df50.vmdk"</span>
         <span class="na">secretRef</span><span class="pi">:</span> <span class="s2">"</span><span class="s">my-s3-credentials"</span>
  <span class="na">pvc</span><span class="pi">:</span>
    <span class="na">accessModes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">storage</span><span class="pi">:</span> <span class="s2">"</span><span class="s">6Gi"</span>
</code></pre></div></div>

<p>Once the AMI file content is stored in a PVC, CDI can be used further to clone that AMI’s PVC on a per VM basis. This effectively recreates the AMI to EC2 relationship that exists in AWS. You can find more information about CDI <a href="https://github.com/kubevirt/containerized-data-importer">here</a></p>

<h2 id="automating-ami-import">Automating AMI import</h2>

<p>Using the technique of exporting an AMI to an s3 bucket and importing the AMI from s3 into a KubeVirt cluster using CDI, the Kubevirt Cloud Import project provides the glue necessary for tying all of these pieces together in the form of the <code class="language-plaintext highlighter-rouge">import-ami</code> cli command and a Tekton task.</p>

<h2 id="automation-using-the-import-ami-cli-command">Automation using the import-ami CLI command</h2>

<p>The <code class="language-plaintext highlighter-rouge">import-ami</code> takes a set of arguments related to the AMI you wish to import into KubeVirt and the name of the PVC you’d like the AMI to be imported into. Upon execution, import-ami will call all the appropriate AWS and KubeVirt APIs to make this work. The result is a PVC with the AMI contents that is capable of being launched by a KubeVirt VM.</p>

<p>In the example below, A publicly shared <a href="https://alt.fedoraproject.org/cloud/">fedora34 AMI</a> is imported into the KubeVirt cluster as a PVC called fedora34-golden-image</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nb">export </span><span class="nv">S3_BUCKET</span><span class="o">=</span>my-bucket
<span class="nb">export </span><span class="nv">S3_SECRET</span><span class="o">=</span>s3-readonly-cred
<span class="nb">export </span><span class="nv">AWS_REGION</span><span class="o">=</span>us-west-2
<span class="nb">export </span><span class="nv">AMI_ID</span><span class="o">=</span>ami-00a4fdd3db8bb2851
<span class="nb">export </span><span class="nv">PVC_STORAGECLASS</span><span class="o">=</span>rook-ceph-block
<span class="nb">export </span><span class="nv">PVC_NAME</span><span class="o">=</span>fedora34-golden-image

import-ami <span class="nt">--s3-bucket</span> <span class="nv">$S3_BUCKET</span> <span class="nt">--region</span> <span class="nv">$AWS_REGION</span> <span class="nt">--ami-id</span> <span class="nv">$AMI_ID</span> <span class="nt">--pvc-storageclass</span> <span class="nv">$PVC_STORAGECLASS</span> <span class="nt">--s3-secret</span> <span class="nv">$S3_SECRET</span> <span class="nt">--pvc-name</span> <span class="nv">$PVC_NAME</span>

</code></pre></div></div>

<h2 id="automation-using-the-import-ami-tekton-task">Automation using the import-ami Tekton Task</h2>

<p>In addition to the <code class="language-plaintext highlighter-rouge">import-ami</code> cli command, the KubeVirt Cloud Import project also includes a <a href="https://github.com/davidvossel/kubevirt-cloud-import/blob/main/tasks/import-ami/manifests/import-ami.yaml">Tekton task</a> which wraps the cli command and allows integrating AMI import into a Tekton pipeline.</p>

<p>Using a Tekton pipeline, someone can combine the task of importing an AMI into KubeVirt with the task of starting a VM using that AMI. An example pipeline can be found <a href="https://raw.githubusercontent.com/davidvossel/kubevirt-cloud-import/main/examples/create-vm-from-ami-pipeline.yaml">here</a> which outlines how this is accomplished.</p>

<p>Below is a pipeline run that uses the example pipeline to import the publicly shared fedora34 AMI into a PVC, then starts a VM using that imported AMI.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; pipeline-run.yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: my-vm-creation-pipeline
  namespace: default
spec:
  serviceAccountName: my-kubevirt-service-account
  pipelineRef:
    name: create-vm-pipeline 
  params:
    - name: vmName
      value: vm-fedora34
    - name: s3Bucket
      value: my-kubevirt-exports
    - name: s3ReadCredentialsSecret
      value: my-s3-read-only-credentials
    - name: awsRegion
      value: us-west-2
    - name: amiId 
      value: ami-00a4fdd3db8bb2851
    - name: pvcStorageClass 
      value: rook-ceph-block
    - name: pvcName
      value: fedora34
    - name: pvcNamespace
      value: default
    - name: pvcSize
      value: 6Gi
    - name: pvcAccessMode
      value: ReadWriteOnce
    - name: awsCredentialsSecret
      value: my-aws-credentials
</span><span class="no">EOF

</span>kubectl create <span class="nt">-f</span> pipeline-run.yaml
</code></pre></div></div>

<p>After posting the pipeline run, watch for the pipeline run to complete.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pipelinerun
selecting docker as container runtime
NAME                      SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME
my-vm-creation-pipeline   True        Succeeded   11m         9m54s
</code></pre></div></div>

<p>Then observe that the resulting VM is online</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi
selecting docker as container runtime
NAME          AGE   PHASE     IP               NODENAME   READY
vm-fedora34   11m   Running   10.244.196.175   node01     True
</code></pre></div></div>

<p>For more detailed and up-to-date information about how to automate AMI import using Tekton, view the KubeVirt Cloud Import <a href="https://github.com/davidvossel/kubevirt-cloud-import/blob/main/README.md">README.md</a></p>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>The portability of workloads across different environments is becoming increasingly important and operations teams need to be vigilant about avoiding vendor lock in. For containers, Kubernetes is an attractive option because it provides a consistent API layer that can run across multiple cloud platforms. KubeVirt can provide that same level of consistency for VMs. As a community we need to invest further into automation tools that allow people to make the transition to KubeVirt.</p>]]></content><author><name>David Vossel</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="AWS" /><category term="EC2" /><category term="AMI" /><summary type="html"><![CDATA[This blog post outlines the fundamentals for how to import VMs from AWS into KubeVirt]]></summary></entry></feed>