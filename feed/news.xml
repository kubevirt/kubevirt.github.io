<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kubevirt.io//feed/news.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2025-11-26T11:27:28+00:00</updated><id>https://kubevirt.io//feed/news.xml</id><title type="html">KubeVirt.io | News</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">Announcing the results of our Security Audit</title><link href="https://kubevirt.io//2025/Announcing-KubeVirt-Security-Audit-Results.html" rel="alternate" type="text/html" title="Announcing the results of our Security Audit" /><published>2025-11-07T00:00:00+00:00</published><updated>2025-11-07T00:00:00+00:00</updated><id>https://kubevirt.io//2025/Announcing-KubeVirt-Security-Audit-Results</id><content type="html" xml:base="https://kubevirt.io//2025/Announcing-KubeVirt-Security-Audit-Results.html"><![CDATA[<p>The KubeVirt Community is very pleased to share the results of our security audit, completed through the guidance of the Open Source Technology Improvement Fund (OSTIF) and the technical expertise of Quarkslab.</p>

<p>This is a critical step in KubeVirt moving to Graduation within the CNCF framework, and is the first time the project has been publicly audited.</p>

<p>The audit was conducted by Quarkslab earlier this year, beginning with an architectural review of KubeVirt and the creation of a threat model that identified threat actors, attack scenarios, and attack surfaces of the project. These were used to then test, prod, and poke to uncover and exploit any weak points.</p>

<p>The audit found the following:</p>

<ul>
  <li>15 findings with a Security Impact:
    <ul>
      <li>0 Critical</li>
      <li>1 High
        <ul>
          <li><a href="https://github.com/kubevirt/kubevirt/security/advisories/GHSA-46xp-26xh-hpqh">CVE-2025-64324</a></li>
        </ul>
      </li>
      <li>7 Medium
        <ul>
          <li><a href="https://github.com/kubevirt/kubevirt/security/advisories/GHSA-38jw-g2qx-4286">CVE-2025-64432</a></li>
          <li><a href="https://github.com/kubevirt/kubevirt/security/advisories/GHSA-qw6q-3pgr-5cwq">CVE-2025-64433</a></li>
          <li><a href="https://github.com/kubevirt/kubevirt/security/advisories/GHSA-ggp9-c99x-54gp">CVE-2025-64434</a></li>
          <li><a href="https://github.com/kubevirt/kubevirt/security/advisories/GHSA-9m94-w2vq-hcf9">CVE-2025-64435</a></li>
          <li><a href="https://github.com/kubevirt/kubevirt/security/advisories/GHSA-7xgm-5prm-v5gc">CVE-2025-64436</a></li>
          <li><a href="https://github.com/kubevirt/kubevirt/security/advisories/GHSA-2r4r-5x78-mvqf">CVE-2025-64437</a></li>
        </ul>
      </li>
      <li>4 Low</li>
      <li>3 Informational</li>
    </ul>
  </li>
</ul>

<p>Quarkslab also provided us with a Custom Threat Model and Fix Recommendations, and kept in touch after delivering the audit to help us understand and address the weaknesses they found. One of their team even volunteered their time to help remediate some of these issues, which we greatly appreciated!</p>

<p>These findings were provided to the project maintainers privately with an agreed response time to allow KubeVirt to address them prior to publication.</p>

<p>The KubeVirt maintainers are very happy with these results, as they demonstrate not only the strength and security focus of our community, as well as the payoff of our earlier investment of moving to non-privileged by default, and by being compliant with the standard Kubernetes Security Model, which includes SELinux policies, seccomp and Pod Security Standards. It is worth noting that Kubernetes is also maturing and providing more security features, allowing KubeVirt and other projects in the ecosystem to inherently increase our security.</p>

<p>This all highlights the unique benefits and additional isolation of running virtual machines as containers in addition to the benefits of using virtual machines.</p>

<p>Having your project audited is both nerve-inducing and extremely comforting. The KubeVirt project is deeply invested in following security best practices, and part of these best practices is having your project audited by a third party to find any possible weaknesses before a malicious actor. KubeVirt maintainers appreciate the OSTIF initiative in promoting security of CNCF projects.</p>

<p>You can read the <a href="https://ostif.org/wp-content/uploads/2025/10/KubeVirt_OSTIF_Report_25-06-2150-REP_v1.2.pdf">full Audit Report here</a>.</p>

<p><a href="https://blog.quarkslab.com/kubevirt-security-audit.html">Quarkslab’s blog on the process here</a>.</p>

<p>And <a href="https://ostif.org/kubevirt-audit-complete/">OSTIF’s blog here</a>.</p>

<p>A huge thanks to everyone involved:<br />
<strong>Quarkslab</strong>: Sébastien Rolland, Mihail Kirov, and Pauline Sauder<br />
<strong>OSTIF</strong>: Helen Woeste and Amir Montazery<br />
<strong>KubeVirt</strong>: Jed Lejosne, Ľuboslav Pivarč, Vladik Romanovsky, Federico Fossemò, Stu Gott, Roman Mohr, Fabian Deutsch, and Andrew Burden</p>

<p>We recommend users update their clusters to the latest supported z-stream version of KubeVirt.<br />
See our <a href="https://github.com/kubevirt/sig-release/blob/main/releases/k8s-support-matrix.md">KubeVirt to Kubernetes version support matrix</a> for more information on supported KubeVirt versions.</p>]]></content><author><name>KubeVirt Maintainers</name></author><category term="news" /><category term="KubeVirt" /><category term="graduation" /><category term="security" /><category term="community" /><category term="cncf" /><category term="milestone" /><category term="party time" /><summary type="html"><![CDATA[As part of our application to Graduate, KubeVirt has a security audit performed by a third-party, organised through the CNCF and OSTIF.]]></summary></entry><entry><title type="html">Dedicated Migration Networks for Cross-Cluster Live Migration with KubeVirt and EVPN</title><link href="https://kubevirt.io//2025/Dedicated-migration-network-for-cross-cluster-live-migration.html" rel="alternate" type="text/html" title="Dedicated Migration Networks for Cross-Cluster Live Migration with KubeVirt and EVPN" /><published>2025-10-31T00:00:00+00:00</published><updated>2025-10-31T00:00:00+00:00</updated><id>https://kubevirt.io//2025/Dedicated-migration-network-for-cross-cluster-live-migration</id><content type="html" xml:base="https://kubevirt.io//2025/Dedicated-migration-network-for-cross-cluster-live-migration.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In our <a href="https://kubevirt.io/2025/Stretched-layer2-network-between-clusters.html">previous post</a>,
we explored how to stretch Layer 2 networks across multiple KubeVirt clusters
using EVPN and OpenPERouter. While this enables cross-cluster connectivity, VMs
often need to move between clusters. This happens during disaster recovery,
cluster maintenance, resource optimization, or compliance requirements.</p>

<p>Cross cluster live migration moves a running VM from one cluster to another
without stopping it. This generates substantial network traffic and needs
reliable, high-bandwidth connectivity. When you use the same network for both
application traffic and migration, you risk network congestion and security
issues from mixing migration traffic with user data.</p>

<p>A dedicated migration network solves this problem. By configuring a separate
Layer 2 Virtual Network Interface (L2VNI) for migration traffic, you isolate
this critical operation from application networking, improving both security and
performance. Furthermore, the cluster/network admins’ lives are simplified by
making the dedicated migration network an overlay: instead of physically
running and maintaining new cables, configuring switches, and adding network
interfaces to each Kubernetes node (a complex and time-consuming underlay
network expansion), an L2VNI builds upon the existing physical network
infrastructure - admins can define and manage this overlay network logically,
making it a much more agile (and less disruptive) solution for dedicated
migration paths.</p>

<h2 id="why-should-you-have-a-dedicated-migration-network">Why should you have a dedicated migration network</h2>

<p>Dedicated migration networks provide several key advantages:</p>

<ul>
  <li>
    <p><strong>Traffic Isolation</strong>: Migration data flows through a separate network path,
preventing interference with application traffic and allowing for independent
network policies and monitoring.</p>
  </li>
  <li>
    <p><strong>Security Boundaries</strong>: Migration traffic can be encrypted and routed through
dedicated security zones, reducing the attack surface and enabling fine-grained
access controls.</p>
  </li>
  <li>
    <p><strong>Performance Optimization</strong>: Migration networks can be configured with
specific bandwidth allocations, MTU settings, and QoS policies optimized for
bulk data transfer.</p>
  </li>
  <li>
    <p><strong>Operational Visibility</strong>: Separate networks enable dedicated monitoring and
troubleshooting of migration operations without impacting application network
analysis.</p>
  </li>
</ul>

<h2 id="configuring-the-dedicated-migration-network">Configuring the Dedicated Migration Network</h2>

<p>Building on our previous multi-cluster setup, we’ll now add a dedicated
migration network using a separate L2VNI. This configuration assumes you
already have the base clusters and stretched L2 network from the
<a href="https://kubevirt.io/2025/Stretched-layer2-network-between-clusters.html">previous article</a>.</p>

<h3 id="prerequisites">Prerequisites</h3>

<p>Ensure you have:</p>
<ul>
  <li>The multi-cluster testbed from the previous post deployed using
<code class="language-plaintext highlighter-rouge">make deploy-multi-cluster</code></li>
  <li>KubeVirt 1.6.2 or higher installed (included in
<code class="language-plaintext highlighter-rouge">make deploy-multi-cluster</code>)</li>
  <li>Whereabouts IPAM CNI installed (included in <code class="language-plaintext highlighter-rouge">make deploy-multi-cluster</code>)</li>
  <li>The <code class="language-plaintext highlighter-rouge">DecentralizedLiveMigration</code> feature gate enabled (included in
<code class="language-plaintext highlighter-rouge">make deploy-multi-cluster</code>)</li>
</ul>

<h3 id="configuring-the-migration-l2vni">Configuring the Migration L2VNI</h3>

<p>Now we’ll create a separate L2VNI dedicated to migration traffic. Note that
we’re using VNI 666 and VRF “rouge” to distinguish this from our application
network (VNI 110, VRF “red”).</p>

<p><strong>NOTE:</strong> this dedicated migration network (implemented by this L2VNI) is
pre-provisioned when you run <code class="language-plaintext highlighter-rouge">make deploy-multi-cluster</code>.</p>

<p><strong>Cluster A Migration Network:</strong></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L2VNI
metadata:
  name: migration
  namespace: openperouter-system
spec:
  hostmaster:
    autocreate: true
    type: bridge
  l2gatewayip: 192.170.10.1/24
  vni: 666
  vrf: rouge
</span><span class="no">EOF
</span></code></pre></div></div>

<p><strong>Cluster B Migration Network:</strong></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L2VNI
metadata:
  name: migration
  namespace: openperouter-system
spec:
  hostmaster:
    autocreate: true
    type: bridge
  l2gatewayip: 192.170.10.1/24
  vni: 666
  vrf: rouge
</span><span class="no">EOF
</span></code></pre></div></div>

<h3 id="creating-migration-network-attachment-definitions">Creating Migration Network Attachment Definitions</h3>

<p>Next, we create Network Attachment Definitions (NADs) for the migration
network. Note the reduced MTU of 1400 to account for VXLAN overhead:</p>

<p><strong>Cluster A Migration NAD:</strong></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: migration-evpn
  namespace: kubevirt
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "migration-evpn",
      "type": "bridge",
      "bridge": "br-hs-666",
      "mtu": 1400,
      "ipam": {
        "type": "whereabouts",
        "range": "192.170.10.0/24",
        "exclude": [
          "192.170.10.1/32",
          "192.170.10.128/25"
        ]
      }
    }
</span><span class="no">EOF
</span></code></pre></div></div>

<p><strong>Cluster B Migration NAD:</strong></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: migration-evpn
  namespace: kubevirt
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "migration-evpn",
      "type": "bridge",
      "bridge": "br-hs-666",
      "mtu": 1400,
      "ipam": {
        "type": "whereabouts",
        "range": "192.170.10.0/24",
        "exclude": [
          "192.170.10.1/32",
          "192.170.10.0/25"
        ]
      }
    }
</span><span class="no">EOF
</span></code></pre></div></div>

<p><strong>NOTE:</strong> these NADs are already pre-provisioned when you run
<code class="language-plaintext highlighter-rouge">make deploy-multi-cluster</code>.</p>

<h4 id="understanding-the-ip-range-strategy">Understanding the IP Range Strategy</h4>

<p>Both clusters define the same 192.170.10.0/24 range but use different
exclusion patterns to avoid IP conflicts:</p>

<ul>
  <li><strong>Cluster A</strong> excludes <code class="language-plaintext highlighter-rouge">192.170.10.128/25</code> (192.170.10.128 to 192.170.10.255),
giving it access to IPs 192.170.10.2 to 192.170.10.127</li>
  <li><strong>Cluster B</strong> excludes <code class="language-plaintext highlighter-rouge">192.170.10.0/25</code> (192.170.10.0 to 192.170.10.127),
giving it access to IPs 192.170.10.128 to 192.170.10.255</li>
  <li>Both exclude <code class="language-plaintext highlighter-rouge">192.170.10.1/32</code> (the gateway IP)</li>
</ul>

<p>This approach ensures that VMs in each cluster get IPs from non-overlapping
ranges while maintaining the same L2 network, allowing seamless migration
without IP conflicts or the need for IP reassignment during the migration
process.</p>

<p>Since all the prerequisites including certificate exchange are handled by
<code class="language-plaintext highlighter-rouge">make deploy-multi-cluster</code>, we can proceed directly to preparing the VM to be
migrated. All the manifests and instructions are available in
<a href="https://github.com/openperouter/openperouter/blob/main/website/content/docs/examples/evpnexamples/kubevirt-multi-cluster.md#l2-vni-as-kubevirt-dedicated-migration-network-for-cross-cluster-live-migration">OpenPERouter cross-cluster live migration examples</a>.</p>

<h2 id="cross-cluster-live-migration-in-action">Cross-Cluster Live Migration in Action</h2>

<p>Now let’s demonstrate cross-cluster live migration using our dedicated
migration network. We’ll create VMs that use both the application network
(evpn) and have an EVPN <code class="language-plaintext highlighter-rouge">L2VNI</code> as the migration network. Keep in mind that the
latter network is <strong>not</strong> plumbed into the VMs! It is used by the KubeVirt
agents (privileged components, which run in the Kubernetes nodes) to move
the migration between the different nodes (which happen to run in different
clusters).</p>

<h3 id="creating-migration-ready-vms">Creating Migration-Ready VMs</h3>

<p><strong>VM in Cluster A (Migration Source):</strong></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-1
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-1
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      domain:
        devices:
          interfaces:
          - bridge: {}
            name: evpn
            macAddress: 02:03:04:05:06:07
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
        resources:
          requests:
            memory: 2048M
        machine:
          type: ""
      networks:
      - multus:
          networkName: evpn
        name: evpn
      terminationGracePeriodSeconds: 0
      volumes:
      - containerDisk:
          image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.6.2
        name: containerdisk
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth0:
                addresses:
                - 192.170.1.3/24
                gateway4: 192.170.1.1
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<p><strong>VM in Cluster B (Migration Target):</strong></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-1
spec:
  runStrategy: WaitAsReceiver
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-1
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      domain:
        devices:
          interfaces:
          - bridge: {}
            name: evpn
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
        resources:
          requests:
            memory: 2048M
        machine:
          type: ""
      networks:
      - multus:
          networkName: evpn
        name: evpn
      terminationGracePeriodSeconds: 0
      volumes:
      - containerDisk:
          image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.6.2
        name: containerdisk
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth0:
                addresses:
                - 192.170.1.3/24
                gateway4: 192.170.1.1
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>As you can see, both VM definitions are the same - except the <code class="language-plaintext highlighter-rouge">runStrategy</code>.</p>

<h3 id="performing-the-cross-cluster-live-migration">Performing the Cross Cluster Live Migration</h3>

<p>To live-migrate the VM between clusters, we first need to wait for the source
VM to be ready:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl <span class="nb">wait </span>vm vm-1 <span class="se">\</span>
  <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Ready <span class="nt">--timeout</span><span class="o">=</span>60s
</code></pre></div></div>

<p>After that, we can create the migration receiver in cluster B:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: migration-target
spec:
  receive:
    migrationID: "cross-cluster-demo"
  vmiName: vm-1
</span><span class="no">EOF
</span></code></pre></div></div>

<p>We need to get the URL for the destination cluster migration agent. This
information will be required to provision the source cluster migration CR.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">TARGET_IP</span><span class="o">=</span><span class="si">$(</span><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl get vmim <span class="se">\</span>
  migration-target <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.synchronizationAddresses[0]}'</span><span class="si">)</span>
<span class="nb">echo</span> <span class="s2">"Target migration IP: </span><span class="nv">$TARGET_IP</span><span class="s2">"</span>
</code></pre></div></div>

<p>Now that we know the IP of the destination migration controller, we can initiate
the migration from cluster A:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: migration-source
spec:
  sendTo:
    connectURL: "</span><span class="k">${</span><span class="nv">TARGET_IP</span><span class="k">}</span><span class="sh">:9185"
    migrationID: "cross-cluster-demo"
  vmiName: vm-1
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Monitor the migration progress:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Watch migration status in cluster A</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl get vmim <span class="se">\</span>
  migration-source <span class="nt">-w</span>

<span class="c"># Watch VM status in cluster B</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl get vm vm-1 <span class="nt">-w</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>Dedicated migration networks are essential for production KubeVirt deployments
that require VM mobility. Without traffic isolation, live migrations compete
with application workloads for bandwidth, potentially degrading service
performance and creating security risks by mixing operational traffic with user
data.</p>

<p>In this post, we have built upon the foundation laid in our
<a href="https://kubevirt.io/2025/Stretched-layer2-network-between-clusters.html">previous article</a>
and enhanced our multi-cluster KubeVirt deployment with cross-cluster live
migration capabilities. We have configured a secondary <code class="language-plaintext highlighter-rouge">L2VNI</code> (VNI 666, VRF
“rouge”) as a dedicated migration network between KubeVirt clusters. This
overlay network provides isolated, high-performance connectivity for migration
operations without requiring additional physical infrastructure. By using EVPN
and OpenPERouter, we demonstrated how cross-cluster live migration works in
practice while maintaining complete separation from application networking.</p>

<p>This setup enables organizations to achieve workload mobility across clusters
with the security, performance, and operational visibility required for
production environments. The overlay approach simplifies management by avoiding
the complexity of physical network expansion while providing the dedicated
bandwidth and monitoring capabilities that enterprise migrations demand.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="evpn" /><category term="bgp" /><category term="openperouter" /><category term="network" /><category term="networking" /><category term="live-migration" /><summary type="html"><![CDATA[Learn how to configure a separate L2VNI for dedicated migration networks to enable secure, efficient cross-cluster live migration with KubeVirt and OpenPERouter.]]></summary></entry><entry><title type="html">Stretching a Layer 2 network over multiple KubeVirt clusters</title><link href="https://kubevirt.io//2025/Stretched-layer2-network-between-clusters.html" rel="alternate" type="text/html" title="Stretching a Layer 2 network over multiple KubeVirt clusters" /><published>2025-10-13T00:00:00+00:00</published><updated>2025-10-13T00:00:00+00:00</updated><id>https://kubevirt.io//2025/Stretched-layer2-network-between-clusters</id><content type="html" xml:base="https://kubevirt.io//2025/Stretched-layer2-network-between-clusters.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>KubeVirt enables you to run virtual machines (VMs) within Kubernetes clusters,
but networking VMs across multiple clusters presents significant challenges.
Current KubeVirt networking relies on cluster-local solutions, which cannot
extend Layer 2 broadcast domains beyond cluster boundaries. This limitation
forces applications that require L2 connectivity to either remain within a
single cluster or undergo complex network reconfiguration when distributed
across clusters.</p>

<p>Integrating with EVPN addresses this fundamental limitation in distributed
KubeVirt deployments: the inability to maintain L2 adjacency between VMs
running on different clusters. By leveraging EVPN’s BGP-based control plane and
advanced MAC/IP advertisement mechanisms, we can now stretch Layer 2 broadcast
domains across geographically distributed KubeVirt clusters, creating a unified
network fabric that treats multiple clusters as a single, cohesive
infrastructure.</p>

<h3 id="why-stretch-l2-networks-across-different-clusters-">Why Stretch L2 Networks across different clusters ?</h3>
<p>The ability to extend L2 domains between KubeVirt clusters unlocks several
critical capabilities that were previously difficult to achieve.
Traditional cluster networking creates isolation boundaries that, while
beneficial for security and resource management, can become barriers when
applications require tight coupling or when operational requirements demand
flexibility in workload placement.</p>

<p>All in all, stretching an L2 domain across cluster boundaries enables use cases
that are fundamental to infrastructure reliability and flexibility, which include:</p>
<ul>
  <li><strong>Cross Cluster Live Migration:</strong> VMs must migrate between clusters without
requiring IP address changes, DNS updates, or application reconfiguration. This
capability is essential for disaster recovery scenarios where VMs must failover
to geographically distant clusters while still maintaining their network
identity and established connections.</li>
  <li><strong>Legacy enterprise applications availability:</strong> many mission-critical
workloads were designed with assumptions about L2 adjacency, such as database
clusters requiring heartbeat mechanisms over broadcast domains, application
servers expecting multicast discovery, or network-attached storage systems
relying on L2 protocols.</li>
  <li><strong>Resource optimization and capacity planning:</strong> organizations can distribute
VM workloads based on compute availability, cost considerations, or compliance
requirements while maintaining the network simplicity that applications expect.
This flexibility becomes particularly valuable in hybrid cloud scenarios where
workloads may need to seamlessly span on-premises KubeVirt clusters and
cloud-hosted instances.</li>
</ul>

<p>This is where the power of EVPN comes into play: by integrating EVPN into the
KubeVirt ecosystem, we can create a sophisticated L2 overlay. Think of it as a
virtual network fabric that stretches across your data centers or cloud
regions, enabling the workloads running in KubeVirt clusters to attach to a
single, unified L2 domain.</p>

<p>In this post, we’ll dive into how this powerful combination works and how it
unlocks true application mobility for your virtualized workloads on Kubernetes.</p>

<h2 id="prerequisites">Prerequisites</h2>
<p>The list below is required to run this demo. This will enable you to run
multiple Kubernetes clusters in your own laptop, interconnected by EVPN using
<a href="https://openperouter.github.io/">openperouter</a>.</p>
<ul>
  <li>container runtime - docker - installed in your system</li>
  <li>git</li>
  <li>make</li>
</ul>

<h2 id="the-testbed">The testbed</h2>
<p>The testbed will be implemented using a physical network deployed in
leaf/spine topology, which is a common two-layer network architecture used in
data centers. It consists of leaf switches that connect to end devices, and
spine switches that interconnect all leaf switches. This way, workloads will
always be (at most) two hops away from one another.</p>

<p align="center">
  <img src="../assets/2025-10-13-evpn-integration/01-evpn-integration-testbed.png" alt="The testbed" width="100%" />
</p>

<p>The diagram highlights the autonomous system (AS) numbers each of the
components will use.</p>

<p>We can infer from the AS numbers provided above that the testbed will feature
eBGP configuration, thus providing routing between different autonomous
systems.</p>

<p>We will setup the testbed using <a href="https://containerlab.dev/">containerlab</a>, and
the Kubernetes clusters are deployed using <a href="https://kind.sigs.k8s.io/">KinD</a>.
The BGP speakers (routers) in each leaf are implemented using
<a href="https://frrouting.org/">FRR</a>.</p>

<h3 id="spawning-the-testbed-on-your-laptop">Spawning the testbed on your laptop</h3>
<p>To spawn the tested in your laptop, you should clone the openperouter repo.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/openperouter/openperouter.git
git checkout c9d591a
<span class="nb">cd </span>openperouter
</code></pre></div></div>

<p>Assuming you have all the <a href="#prerequisites">requirements</a> installed in your
laptop, all you need to do is build the router component, and execute the
<code class="language-plaintext highlighter-rouge">deploy-multi</code> make target. Then, you should be ready to go!</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sysctl <span class="nt">-w</span> fs.inotify.max_user_instances<span class="o">=</span>1024 <span class="c"># might need sudo</span>
make docker-build <span class="o">&amp;&amp;</span> make deploy-multi
</code></pre></div></div>

<p>After running this make target, you should have the testbed deployed as shown
in the testbed’s <a href="#the-testbed">diagram</a>. One thing is missing though: the
autonomous systems in the kind clusters are not configured yet! This will be
configured in the <a href="#configuring-the-kubevirt-clusters">next section</a>.</p>

<p>The kubeconfigs to connect to each cluster can be found in <code class="language-plaintext highlighter-rouge">openperouter</code>’s
<code class="language-plaintext highlighter-rouge">bin</code> directory:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-<span class="k">*</span>
/root/github/openperouter/bin/kubeconfig-pe-kind-a  /root/github/openperouter/bin/kubeconfig-pe-kind-b
</code></pre></div></div>

<p>Before moving to the configuration section, let’s install KubeVirt in both
clusters:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>kubeconfig <span class="k">in</span> <span class="si">$(</span><span class="nb">ls </span>bin/kubeconfig-<span class="k">*</span><span class="si">)</span><span class="p">;</span> <span class="k">do
    </span><span class="nb">echo</span> <span class="s2">"Installing KubeVirt in cluster using KUBECONFIG=</span><span class="nv">$kubeconfig</span><span class="s2">"</span>
    <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$kubeconfig</span> kubectl apply <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/v1.5.2/kubevirt-operator.yaml
    <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$kubeconfig</span> kubectl apply <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/v1.5.2/kubevirt-cr.yaml
    <span class="c"># Patch KubeVirt to allow scheduling on control-planes, so we can test live migration between two nodes</span>
    <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$kubeconfig</span> kubectl patch <span class="nt">-n</span> kubevirt kubevirt kubevirt <span class="nt">--type</span> merge <span class="nt">--patch</span> <span class="s1">'{"spec": {"workloads": {"nodePlacement": {"tolerations": [{"key": "node-role.kubernetes.io/control-plane", "operator": "Exists", "effect": "NoSchedule"}]}}}}'</span>
    <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$kubeconfig</span> kubectl <span class="nb">wait</span> <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Available kubevirt/kubevirt <span class="nt">-n</span> kubevirt <span class="nt">--timeout</span><span class="o">=</span>10m
    <span class="nb">echo</span> <span class="s2">"Finished installing KubeVirt in cluster using KUBECONFIG=</span><span class="nv">$kubeconfig</span><span class="s2">"</span>
<span class="k">done</span>
</code></pre></div></div>

<h2 id="configuring-the-kubevirt-clusters">Configuring the KubeVirt clusters</h2>
<p>As indicated in the <a href="#introduction">introduction</a> section, the end goal is to
stretch a layer 2 network across both Kubernetes clusters, using EVPN. Please
refer to the image below for a simple diagram.</p>

<p align="center">
  <img src="../assets/2025-10-13-evpn-integration/02-stretched-l2-evpn.png" alt="Layer 2 network stretched across both clusters" width="100%" />
</p>

<p>In order to stretch an L2 overlay across both cluster we need to:</p>
<ul>
  <li>configure the underlay network</li>
  <li>configure the EVPN VXLAN VNI</li>
</ul>

<p>We will rely on <a href="https://openperouter.github.io/">openperouter</a> for both of
these.</p>

<p>Let’s start with the underlay network, in which we will connect the Kubernetes
clusters to each cluster’s top of rack BGP/EVPN speaker.</p>

<h3 id="configuring-the-underlay-network">Configuring the underlay network</h3>
<p>The first thing we need to do is to finish setting up the testbed by
peering our two Kubernetes clusters with the BGP/EVPN speaker in each cluster’s
top of rack:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">kindleaf-a</code> for cluster-a</li>
  <li><code class="language-plaintext highlighter-rouge">kindleaf-b</code> for cluster-b</li>
</ul>

<p>This will require you to specify the expected AS numbers, to define the VXLAN
tunnel endpoint addresses, and also specify which node interface will be used
to connect to external routers.</p>

<p>For that,
you will need to provision the following CRs:</p>

<ul>
  <li>Cluster A.</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: Underlay
metadata:
  name: underlay
  namespace: openperouter-system
spec:
  asn: 64514
  evpn:
    vtepcidr:  100.65.0.0/24
  nics:
    - toswitch
  neighbors:
    - asn: 64512
      address: 192.168.11.2
</span><span class="no">EOF
</span></code></pre></div></div>

<ul>
  <li>Cluster B.</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: Underlay
metadata:
  name: underlay
  namespace: openperouter-system
spec:
  asn: 64518
  evpn:
    vtepcidr: 100.65.1.0/24
  routeridcidr: 10.0.1.0/24
  nics:
    - toswitch
  neighbors:
    - asn: 64516
      address: 192.168.12.2
</span><span class="no">EOF
</span></code></pre></div></div>

<h3 id="configuring-the-evpn-vni">Configuring the EVPN VNI</h3>
<p>Once we have configured both Kubernetes cluster’s peering with the external
routers in <code class="language-plaintext highlighter-rouge">kindleaf-a</code> and <code class="language-plaintext highlighter-rouge">kindleaf-b</code>, we can now focus on defining the
<code class="language-plaintext highlighter-rouge">layer2</code> EVPN. For that, we will use openperouter’s <code class="language-plaintext highlighter-rouge">L2VNI</code> CRD.</p>

<p>Execute the following commands to provision the <code class="language-plaintext highlighter-rouge">L2VNI</code> in both clusters:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># provision L2VNI in cluster: pe-kind-a</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L2VNI
metadata:
  name: layer2
  namespace: openperouter-system
spec:
  hostmaster:
    autocreate: true
    type: bridge
  l2gatewayip: 192.170.1.1/24
  vni: 110
  vrf: red
</span><span class="no">EOF

</span><span class="c"># provision L2VNI in cluster: pe-kind-b</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L2VNI
metadata:
  name: layer2
  namespace: openperouter-system
spec:
  hostmaster:
    autocreate: true
    type: bridge
  l2gatewayip: 192.170.1.1/24
  vni: 110
  vrf: red
</span><span class="no">EOF
</span></code></pre></div></div>

<p>After this step, we will have created an L2 overlay network on top of the
network fabric. We now need to enable it to be plumbed to the workloads.
Execute the commands below to provision a network attachment definition in both
clusters:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># provision NAD in cluster: pe-kind-a</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: evpn
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "evpn",
      "type": "bridge",
      "bridge": "br-hs-110",
      "macspoofchk": false,
      "disableContainerInterface": true
    }
</span><span class="no">EOF

</span><span class="c"># provision NAD in cluster: pe-kind-b</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: evpn
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "evpn",
      "type": "bridge",
      "bridge": "br-hs-110",
      "macspoofchk": false,
      "disableContainerInterface": true
    }
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Now that we have set up networking for the workloads, we can proceed with
actually instantiating the VMs which will attach to this network overlay.</p>

<h3 id="provisioning-and-running-the-vm-workloads">Provisioning and running the VM workloads</h3>

<p>You will have one VM running in cluster A (vm-1), and another VM running in
cluster B (vm-2).</p>

<p>The VMs will each have one network interface, attached to the layer2 overlay.
The VMs are using bridge binding, and they attach to the overlay using bridge-cni.
Both VMs have static IPs, configured over cloud-init. They are:</p>

<table>
  <thead>
    <tr>
      <th>VM name</th>
      <th>Cluster</th>
      <th>IP address</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>vm-1</td>
      <td>pe-kind-a</td>
      <td>192.170.1.3</td>
    </tr>
    <tr>
      <td>vm-2</td>
      <td>pe-kind-b</td>
      <td>192.170.1.30</td>
    </tr>
  </tbody>
</table>

<p>To provision these, follow these steps:</p>

<ol>
  <li>Provision <code class="language-plaintext highlighter-rouge">vm-1</code> in cluster <code class="language-plaintext highlighter-rouge">pe-kind-a</code>:</li>
</ol>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-1
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-1
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      domain:
        devices:
          interfaces:
          - bridge: {}
            name: evpn
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
        resources:
          requests:
            memory: 2048M
        machine:
          type: ""
      networks:
      - multus:
          networkName: evpn
        name: evpn
      terminationGracePeriodSeconds: 0
      volumes:
      - containerDisk:
          image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.5.2
        name: containerdisk
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth0:
                addresses:
                - 192.170.1.3/24
                gateway4: 192.170.1.1
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<ol>
  <li>Provision <code class="language-plaintext highlighter-rouge">vm-2</code> in cluster <code class="language-plaintext highlighter-rouge">pe-kind-b</code>:</li>
</ol>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-2
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-2
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      domain:
        devices:
          interfaces:
          - bridge: {}
            name: evpn
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
        resources:
          requests:
            memory: 2048M
        machine:
          type: ""
      networks:
      - multus:
          networkName: evpn
        name: evpn
      terminationGracePeriodSeconds: 0
      volumes:
      - containerDisk:
          image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.5.2
        name: containerdisk
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth0:
                addresses:
                - 192.170.1.30/24
                gateway4: 192.170.1.1
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>We will use <code class="language-plaintext highlighter-rouge">vm-2</code> (which runs in cluster <strong>B</strong>) as the “server”, and <code class="language-plaintext highlighter-rouge">vm-1</code>
(which runs in cluster <strong>A</strong>) as the “client”; however, we first need to wait
for the VMs to become <code class="language-plaintext highlighter-rouge">Ready</code>:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span>bin/kubeconfig-pe-kind-a kubectl <span class="nb">wait </span>vm vm-1 <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Ready <span class="nt">--timeout</span><span class="o">=</span>60s
<span class="nv">KUBECONFIG</span><span class="o">=</span>bin/kubeconfig-pe-kind-b kubectl <span class="nb">wait </span>vm vm-2 <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Ready <span class="nt">--timeout</span><span class="o">=</span>60s
</code></pre></div></div>

<p>Now that we know the VMs are <code class="language-plaintext highlighter-rouge">Ready</code>, let’s confirm the IP address for <code class="language-plaintext highlighter-rouge">vm-2</code>,
and reach into it from the <code class="language-plaintext highlighter-rouge">vm-1</code> VM, which is available in cluster A.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span>bin/kubeconfig-pe-kind-b kubectl get vmi vm-2 <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{.status.interfaces[0].ipAddress}"</span>
192.170.1.30
</code></pre></div></div>

<p>Let’s now serve some data. We will use a toy python webserver for that, which serves some files:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span><span class="nb">touch</span> <span class="si">$(</span><span class="nb">date</span><span class="si">)</span>
<span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-la</span>
total 12
drwx------. 1 fedora fedora 122 Oct 13 12:08 <span class="nb">.</span>
drwxr-xr-x. 1 root   root    12 Sep 13  2024 ..
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 12:08:15
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 13
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 2025
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora  18 Jul 21  2021 .bash_logout
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora 141 Jul 21  2021 .bash_profile
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora 492 Jul 21  2021 .bashrc
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 Mon
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 Oct
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 PM
drwx------. 1 fedora fedora  30 Sep 13  2024 .ssh
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 fedora fedora   0 Oct 13 12:08 UTC
<span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span>python3 <span class="nt">-m</span> http.server 8090
Serving HTTP on 0.0.0.0 port 8090 <span class="o">(</span>http://0.0.0.0:8090/<span class="o">)</span> ...
</code></pre></div></div>

<p>And let’s try to access that from the VM which runs in the other cluster:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span>bin/kubeconfig-pe-kind-a virtctl console vm-1
<span class="c"># password to access the VM is fedora/fedora</span>
<span class="o">[</span>fedora@vm-1 ~]<span class="nv">$ </span>curl 192.170.1.30:8090
&lt;<span class="o">!</span>DOCTYPE HTML PUBLIC <span class="s2">"-//W3C//DTD HTML 4.01//EN"</span> <span class="s2">"http://www.w3.org/TR/html4/strict.dtd"</span><span class="o">&gt;</span>
&lt;html&gt;
&lt;<span class="nb">head</span><span class="o">&gt;</span>
&lt;meta http-equiv<span class="o">=</span><span class="s2">"Content-Type"</span> <span class="nv">content</span><span class="o">=</span><span class="s2">"text/html; charset=utf-8"</span><span class="o">&gt;</span>
&lt;title&gt;Directory listing <span class="k">for</span> /&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Directory listing <span class="k">for</span> /&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">".bash_logout"</span><span class="o">&gt;</span>.bash_logout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">".bash_profile"</span><span class="o">&gt;</span>.bash_profile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">".bashrc"</span><span class="o">&gt;</span>.bashrc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">".ssh/"</span><span class="o">&gt;</span>.ssh/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"12%3A08%3A15"</span><span class="o">&gt;</span>12:08:15&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"13"</span><span class="o">&gt;</span>13&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"2025"</span><span class="o">&gt;</span>2025&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"Mon"</span><span class="o">&gt;</span>Mon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"Oct"</span><span class="o">&gt;</span>Oct&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"PM"</span><span class="o">&gt;</span>PM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"UTC"</span><span class="o">&gt;</span>UTC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre></div></div>

<p>As you can see, the VM running in cluster A was able to successfully reach into
the VM running in cluster B.</p>

<h3 id="bonus-track-connecting-to-provider-networks-using-an-l3vni">Bonus track: connecting to provider networks using an L3VNI</h3>
<p>This extra (optional) step showcases how you can import provider network routes
into the Kubernetes clusters - essentially creating an L3 overlay - using
<code class="language-plaintext highlighter-rouge">openperouter</code>s L3VNI CRD.</p>

<p>We will use it to reach into the webserver hosted in <code class="language-plaintext highlighter-rouge">hostA</code> (attached to
<code class="language-plaintext highlighter-rouge">leafA</code> in the <a href="#the-testbed">diagram</a>) from the VMs running in both clusters.
Please refer to the image below to get a better understanding of the scenario.</p>

<p align="center">
  <img src="../assets/2025-10-13-evpn-integration/03-wrap-l3vni-over-stretched-l2.png" alt="Wrap an L3VNI over a stretched L2 EVPN" width="100%" />
</p>

<p>Since we already have configured the <code class="language-plaintext highlighter-rouge">underlay</code> in a
<a href="#configuring-the-underlay-network">previous step</a>, all we need to do is to
configure the <code class="language-plaintext highlighter-rouge">L3VNI</code>; for that, provision the following CR in <strong>both</strong>
clusters:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># provision L3VNI in cluster: pe-kind-a</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-a kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L3VNI
metadata:
  name: red
  namespace: openperouter-system
spec:
  vni: 100
  vrf: red
</span><span class="no">EOF

</span><span class="c"># provision L3VNI in cluster: pe-kind-b</span>
<span class="nv">KUBECONFIG</span><span class="o">=</span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/bin/kubeconfig-pe-kind-b kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: openpe.openperouter.github.io/v1alpha1
kind: L3VNI
metadata:
  name: red
  namespace: openperouter-system
spec:
  vni: 100
  vrf: red
</span><span class="no">EOF
</span></code></pre></div></div>

<p>This will essentially wrap the existing <code class="language-plaintext highlighter-rouge">L2VNI</code> with an L3 domain - i.e. a
separate Virtual Routing Function (VRF), whose Virtual Network Identifier (VNI)
is 100. This will enable the Kubernetes clusters to reach into services located in
the <code class="language-plaintext highlighter-rouge">red</code> VRF (which have VNI = 100). Services on <code class="language-plaintext highlighter-rouge">hostA</code> and/or <code class="language-plaintext highlighter-rouge">hostB</code> with
VNI = 200 are not accessible, since we haven’t exposed them over EVPN (using an
<code class="language-plaintext highlighter-rouge">L3VNI</code>).</p>

<p>Once we’ve provisioned the aforementioned <code class="language-plaintext highlighter-rouge">L3VNI</code>, we can now check accessing
the webserver located in the host in <code class="language-plaintext highlighter-rouge">leafA</code> - <code class="language-plaintext highlighter-rouge">clab-kind-leafA</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec </span>clab-kind-hostA_red ip <span class="nt">-4</span> addr show dev eth1
228: eth1@if227: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9500 qdisc noqueue state UP group default  link-netnsid 1
    inet 192.168.20.2/24 scope global eth1
       valid_lft forever preferred_lft forever
</code></pre></div></div>

<p>Let’s also check the same thing for the <code class="language-plaintext highlighter-rouge">blue</code> VRF - for which we do <strong>not</strong>
have any VNI configuration.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec </span>clab-kind-hostA_blue ip <span class="nt">-4</span> addr show dev eth1
273: eth1@if272: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9500 qdisc noqueue state UP group default  link-netnsid 1
    inet 192.168.21.2/24 scope global eth1
       valid_lft forever preferred_lft forever
</code></pre></div></div>

<p>And let’s now access these services from the VMs we have in both clusters.</p>

<p>From <code class="language-plaintext highlighter-rouge">vm-1</code>, in cluster A:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># username/password =&gt; fedora/fedora</span>
virtctl console vm-1
Successfully connected to vm-1 console. The escape sequence is ^]

vm-1 login: fedora
Password:
<span class="o">[</span>fedora@vm-1 ~]<span class="nv">$ </span>curl 192.168.20.2:8090/clientip <span class="c"># we have access to the RED VRF</span>
192.170.1.3:35146
<span class="o">[</span>fedora@vm-1 ~]<span class="nv">$ </span>curl 192.168.21.2:8090/clientip <span class="c"># we do NOT have access to the BLUE VRF</span>
curl: <span class="o">(</span>28<span class="o">)</span> Failed to connect to 192.168.21.2 port 8090 after 128318 ms: Connection timed out
</code></pre></div></div>

<p>From <code class="language-plaintext highlighter-rouge">vm-2</code>, in cluster B:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># username/password =&gt; fedora/fedora</span>
virtctl console vm-2
Successfully connected to vm-2 console. The escape sequence is ^]

vm-2 login: fedora
Password:
<span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span>curl 192.168.20.2:8090/clientip <span class="c"># we have access to the RED VRF</span>
192.170.1.30:52924
<span class="o">[</span>fedora@vm-2 ~]<span class="nv">$ </span>curl 192.168.21.2:8090/clientip <span class="c"># we do NOT have access to the BLUE VRF</span>
curl: <span class="o">(</span>28<span class="o">)</span> Failed to connect to 192.168.21.2 port 8090 after 130643 ms: Connection timed out
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>
<p>In this article we have explained EVPN and which virtualization use cases it
can provide.</p>

<p>We have also shown how the <a href="https://openperouter.github.io/">openperouter</a>
<code class="language-plaintext highlighter-rouge">L2VNI</code> CRD can be used to stretch a Layer 2 overlay across multiple Kubernetes
clusters.</p>

<p>Finally, we have also seen how <code class="language-plaintext highlighter-rouge">openperouter</code> <code class="language-plaintext highlighter-rouge">L3VNI</code> can be used to create
Layer 3 overlays, which allows the VMs running in the Kubernetes clusters to
access services in the exposed provider networks.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="evpn" /><category term="bgp" /><category term="openperouter" /><category term="network" /><category term="networking" /><summary type="html"><![CDATA[Explore EVPN and see how openperouter creates Layer 2 and Layer 3 overlays across Kubernetes clusters.]]></summary></entry><entry><title type="html">Building VM golden images with Packer</title><link href="https://kubevirt.io//2025/Building-VM-golden-image-with-Packer.html" rel="alternate" type="text/html" title="Building VM golden images with Packer" /><published>2025-09-15T00:00:00+00:00</published><updated>2025-09-15T00:00:00+00:00</updated><id>https://kubevirt.io//2025/Building-VM-golden-image-with-Packer</id><content type="html" xml:base="https://kubevirt.io//2025/Building-VM-golden-image-with-Packer.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Creating and maintaining VM golden images can be time-consuming, often
requiring local virtualization tools and manual setup. With <a href="https://kubevirt.io">KubeVirt</a>
running inside your <a href="https://kubernetes.io">Kubernetes</a> cluster, you can
manage virtual machines alongside your containers, but it lacks automation
for creating consistent, reusable VM images.</p>

<p>That’s where <a href="https://packer.io">Packer</a> and the new KubeVirt plugin come
in. The plugin lets you build VM images directly in Kubernetes, enabling you
to automate OS installation from ISO, customize the VM during build, and produce
a reusable bootable volume, all without leaving your cluster.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before you begin, make sure you have the following installed:</p>

<ul>
  <li><a href="https://developer.hashicorp.com/packer/install">Packer</a></li>
  <li><a href="https://kubernetes.io/docs/tasks/tools">Kubernetes</a></li>
  <li><a href="https://kubevirt.io/user-guide/cluster_admin/installation">KubeVirt</a></li>
  <li><a href="https://kubevirt.io/user-guide/storage/containerized_data_importer/#install-cdi">Containerized Data Importer (CDI)</a></li>
</ul>

<h2 id="plugin-features">Plugin Features</h2>

<p>The Packer plugin for KubeVirt offers a variety of features that simplify
the VM golden image creation process:</p>

<ul>
  <li><strong>HCL Template</strong>: Define infrastructure as code for easy versioning and reuse using <a href="https://developer.hashicorp.com/packer/docs/templates/hcl_templates">HCL templates</a>.</li>
  <li><strong>ISO Installation</strong>: Build VM golden images from an ISO file using the <code class="language-plaintext highlighter-rouge">kubevirt-iso</code> builder.</li>
  <li><strong>ISO Media Files</strong>: Include additional files (e.g., configs, scripts, and more) during the installation process.</li>
  <li><strong>Boot Command</strong>: Automate the VM boot process via a <a href="https://en.wikipedia.org/wiki/VNC">VNC</a> connection with a predefined set of commands.</li>
  <li><strong>Integrated SSH/WinRM Access</strong>: Provision and customize VMs via <a href="https://man7.org/linux/man-pages/man1/ssh.1.html">SSH</a> or <a href="https://learn.microsoft.com/en-us/windows/win32/winrm/portal">WinRM</a>.</li>
</ul>

<p><strong>Note</strong>: This plugin is currently in pre-release and actively under development by
<a href="https://www.redhat.com">Red Hat</a> and <a href="https://www.hashicorp.com">HashiCorp</a> together.</p>

<h2 id="plugin-components">Plugin Components</h2>

<p>The core component of this plugin is the <code class="language-plaintext highlighter-rouge">kubevirt-iso</code> builder. This builder
allows you to start from an ISO file and create a VM golden image directly
on your Kubernetes cluster.</p>

<h3 id="builder-design">Builder Design</h3>

<p align="center">
  <img src="../assets/2025-09-15-Packer-Plugin/kubevirt-iso-builder-design.png" alt="Design" width="1125" />
</p>

<p>This diagram shows the workflow for building a bootable volume in a
Kubernetes cluster using Packer with the KubeVirt plugin.</p>

<ol>
  <li>Creates a temporary VM from an ISO image.</li>
  <li>Runs provisioning using either the <a href="https://developer.hashicorp.com/packer/docs/provisioners/shell">Shell</a> or <a href="https://developer.hashicorp.com/packer/integrations/hashicorp/ansible/latest/components/provisioner/ansible">Ansible</a> provisioner.</li>
  <li>Clones the VM’s disk to create a reusable bootable volume (<a href="https://kubevirt.io/user-guide/storage/disks_and_volumes/#datavolume">DataVolume and DataSource</a>).</li>
</ol>

<p>This bootable volume can then be reused to instantiate new VMs without
repeating the installation.</p>

<h2 id="step-by-step-example-building-a-fedora-vm-image">Step-by-Step Example: Building a Fedora VM Image</h2>

<p>The following Packer template (Fedora 42) demonstrates key features:</p>

<ul>
  <li>ISO-based installation using the <code class="language-plaintext highlighter-rouge">kubevirt-iso</code> builder.</li>
  <li>Embedded configuration file to automate the installation.</li>
  <li>Sending boot commands to inject <code class="language-plaintext highlighter-rouge">ks.cfg</code> in GRUB.</li>
  <li>SSH provisioning with a <a href="https://developer.hashicorp.com/packer/docs/provisioners/shell">Shell</a> provisioner.</li>
  <li>Full integration with <a href="https://kubevirt.io/user-guide/user_workloads/instancetypes">InstanceTypes and Preferences</a>.</li>
</ul>

<p>Follow these steps to build a Fedora VM image inside your Kubernetes cluster.</p>

<h3 id="step-1-export-kubeconfig-variable">Step 1: Export KubeConfig Variable</h3>

<p>Export your <a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#the-kubeconfig-environment-variable">KubeConfig</a>
variable, which is also used by the Packer plugin:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>~/.kube/config
</code></pre></div></div>

<p>This is required to communicate with your Kubernetes cluster.</p>

<h3 id="step-2-deploy-iso-datavolume">Step 2: Deploy ISO DataVolume</h3>

<p>Create a <a href="https://kubevirt.io/user-guide/storage/disks_and_volumes/#datavolume">DataVolume</a> to import the Fedora ISO into your cluster’s storage:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: fedora-42-x86-64-iso
  annotations:
    #
    # This annotation triggers immediate binding of the PVC,
    # speeding up provisioning.
    #
    cdi.kubevirt.io/storage.bind.immediate.requested: "true"
spec:
  source:
    http:
      #
      # Please check if this URL link is valid, in case the import fails.
      # If so, please modify the URL here below.
      #
      url: "https://download.fedoraproject.org/pub/fedora/linux/releases/42/Server/x86_64/iso/Fedora-Server-dvd-x86_64-42-1.1.iso"
  pvc:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 3Gi
</span><span class="no">EOF
</span></code></pre></div></div>

<h4 id="alternative-upload-a-local-iso">Alternative: Upload a local ISO</h4>

<p>Instead of importing from a URL, you can upload a local ISO
using the <a href="https://kubevirt.io/user-guide/user_workloads/virtctl_client_tool">virtctl</a> client tool:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virtctl image-upload dv fedora-42-x86-64-iso <span class="se">\</span>
  <span class="nt">--size</span><span class="o">=</span>3Gi <span class="se">\</span>
  <span class="nt">--image-path</span><span class="o">=</span>./Fedora-Server-dvd-x86_64-42-1.1.iso <span class="se">\</span>
</code></pre></div></div>

<p>The <a href="https://fedoraproject.org/server/download">Fedora Server 42 ISO</a> is available on Fedora’s official website.</p>

<h3 id="step-3-create-kickstart-file">Step 3: Create Kickstart File</h3>

<p>This <a href="https://en.wikipedia.org/wiki/Kickstart_(Linux)">Kickstart</a> file automates
Fedora installation, enabling unattended VM setup.</p>

<p>Create a file named <code class="language-plaintext highlighter-rouge">ks.cfg</code> with the following configuration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> ks.cfg <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
cdrom
text
firstboot --disable
lang en_US.UTF-8
keyboard us
timezone Europe/Paris --utc
selinux --enforcing
rootpw root
firewall --enabled --ssh
network --bootproto dhcp
user --groups=wheel --name=user --password=root --uid=1000 --gecos="user" --gid=1000

bootloader --location=mbr --append="net.ifnames=0 biosdevname=0 crashkernel=no"

zerombr
clearpart --all --initlabel
autopart --type=lvm

poweroff

%packages --excludedocs
@core
qemu-guest-agent
openssh-server
%end

%post
systemctl enable --now sshd
systemctl enable --now qemu-guest-agent
%end
</span><span class="no">EOF
</span></code></pre></div></div>

<p>This configuration enables SSH to provision the temporary VM, and <a href="https://qemu-project.gitlab.io/qemu/interop/qemu-ga.html">QEMU Guest Agent</a>
to have a better integration with KubeVirt itself.</p>

<h3 id="step-4-create-packer-template">Step 4: Create Packer Template</h3>

<p>Create an example of the Packer template (<code class="language-plaintext highlighter-rouge">fedora.pkr.hcl</code>):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> fedora.pkr.hcl <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
packer {
  required_plugins {
    kubevirt = {
      source  = "github.com/hashicorp/kubevirt"
      version = "&gt;= 0.8.0"
    }
  }
}

variable "kube_config" {
  type    = string
  default = "</span><span class="k">${</span><span class="nv">env</span><span class="p">(</span><span class="s2">"KUBECONFIG"</span><span class="p">)</span><span class="k">}</span><span class="sh">"
}

variable "namespace" {
  type    = string
  default = "vm-images"
}

variable "name" {
  type    = string
  default = "fedora-42-rand-85"
}

source "kubevirt-iso" "fedora" {
  # Kubernetes configuration
  kube_config   = var.kube_config
  name          = var.name
  namespace     = var.namespace

  # ISO configuration
  iso_volume_name = "fedora-42-x86-64-iso"

  # VM type and preferences
  disk_size          = "10Gi"
  instance_type      = "o1.medium"
  preference         = "fedora"
  os_type            = "linux"

  # Default network configuration
  networks {
    name = "default"

    pod {}
  }

  # Files to include in the ISO installation
  media_files = [
    "./ks.cfg"
  ]

  # Boot process configuration
  # A set of commands to send over VNC connection
  boot_command = [
    "&lt;up&gt;e",                            # Modify GRUB entry
    "&lt;down&gt;&lt;down&gt;&lt;end&gt;",                # Navigate to kernel line
    " inst.ks=hd:LABEL=OEMDRV:/ks.cfg", # Set kickstart file location
    "&lt;leftCtrlOn&gt;x&lt;leftCtrlOff&gt;"        # Boot with modified command line
  ]
  boot_wait                 = "10s"     # Time to wait after boot starts
  installation_wait_timeout = "15m"     # Timeout for installation to complete

  # SSH configuration
  communicator      = "ssh"
  ssh_host          = "127.0.0.1"
  ssh_local_port    = 2020
  ssh_remote_port   = 22
  ssh_username      = "user"
  ssh_password      = "root"
  ssh_wait_timeout  = "20m"
}

build {
  sources = ["source.kubevirt-iso.fedora"]

  provisioner "shell" {
    inline = [
      "echo 'Install packages, configure services, or tweak system settings here.'",
    ]
  }
}
</span><span class="no">EOF
</span></code></pre></div></div>

<h3 id="step-5-export-vm-image-optional">Step 5: Export VM Image (Optional)</h3>

<p>Optionally, export the newly created disk image and package it
into a <a href="https://kubevirt.io/user-guide/storage/disks_and_volumes/#containerdisk">containerDisk</a>
so it can be shared across multiple Kubernetes clusters.</p>

<h4 id="required-dependencies">Required Dependencies</h4>

<p>Install these tools on the machine running Packer:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">virtctl</code>: Exports the VM image from the KubeVirt cluster.</li>
  <li><code class="language-plaintext highlighter-rouge">qemu-img</code>: Converts raw images to qcow2 format.</li>
  <li><code class="language-plaintext highlighter-rouge">gunzip</code>: Decompresses exported VM images.</li>
  <li><code class="language-plaintext highlighter-rouge">podman</code>: Builds and pushes container images.</li>
</ul>

<h4 id="example">Example</h4>

<p>Add a <code class="language-plaintext highlighter-rouge">shell-local</code> post-processor to the Packer build, which runs after the build is completed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>variable <span class="s2">"registry"</span> <span class="o">{</span>
  <span class="nb">type</span>    <span class="o">=</span> string
  default <span class="o">=</span> <span class="s2">"quay.io/containerdisks"</span>
<span class="o">}</span>

variable <span class="s2">"registry_username"</span> <span class="o">{</span>
  <span class="nb">type</span>      <span class="o">=</span> string
  sensitive <span class="o">=</span> <span class="nb">true</span>
<span class="o">}</span>

variable <span class="s2">"registry_password"</span> <span class="o">{</span>
  <span class="nb">type</span>      <span class="o">=</span> string
  sensitive <span class="o">=</span> <span class="nb">true</span>
<span class="o">}</span>

variable <span class="s2">"image_tag"</span> <span class="o">{</span>
  <span class="nb">type</span>    <span class="o">=</span> string
  default <span class="o">=</span> <span class="s2">"latest"</span>
<span class="o">}</span>

build <span class="o">{</span>
  ...

  post-processor <span class="s2">"shell-local"</span> <span class="o">{</span>
    inline <span class="o">=</span> <span class="o">[</span>
      <span class="c"># Export VM disk image from PVC</span>
      <span class="s2">"virtctl -n </span><span class="k">${</span><span class="nv">var</span><span class="p">.namespace</span><span class="k">}</span><span class="s2"> vmexport download </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">-export --pvc=</span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2"> --output=</span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.img.gz"</span>,

      <span class="c"># Decompress exported VM image</span>
      <span class="s2">"gunzip -k </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.img.gz"</span>,

      <span class="c"># Convert raw image to qcow2 (smaller and more efficient format)</span>
      <span class="s2">"qemu-img convert -c -O qcow2 </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.img </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.qcow2"</span>,

      <span class="c"># Generate Containerfile</span>
      <span class="s2">"echo 'FROM scratch' &gt; </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.Containerfile"</span>,
      <span class="s2">"echo 'COPY </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.qcow2 /disk/' &gt;&gt; </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.Containerfile"</span>,

      <span class="c"># Login to registry</span>
      <span class="s2">"podman login -u </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry_username</span><span class="k">}</span><span class="s2"> -p </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry_password</span><span class="k">}</span><span class="s2"> </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry</span><span class="k">}</span><span class="s2">"</span>,

      <span class="c"># Build and push image</span>
      <span class="s2">"podman build -t </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">var</span><span class="p">.image_tag</span><span class="k">}</span><span class="s2"> -f </span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">.Containerfile ."</span>,
      <span class="s2">"podman push </span><span class="k">${</span><span class="nv">var</span><span class="p">.registry</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">var</span><span class="p">.name</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">var</span><span class="p">.image_tag</span><span class="k">}</span><span class="s2">"</span>
    <span class="o">]</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Sensitive credentials such as registry usernames and passwords should never be hardcoded in templates.</p>

<h3 id="step-6-initialize-packer-plugin">Step 6: Initialize Packer Plugin</h3>

<p>Run the following command once to install the Packer plugin:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>packer init fedora.pkr.hcl
</code></pre></div></div>

<p>This downloads and sets up the KubeVirt plugin automatically.</p>

<h3 id="step-7-run-packer-build">Step 7: Run Packer Build</h3>

<p>Finally, run a build to create a new VM golden image with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>packer build fedora.pkr.hcl
</code></pre></div></div>

<p>Packer will create a new VM golden image in your Kubernetes cluster.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this walkthrough, you built a Fedora VM golden image inside Kubernetes
using Packer and the KubeVirt plugin. You defined an ISO source, automated
installation with Kickstart configuration and provisioned the VM over SSH
— all within your Kubernetes cluster.</p>

<p>From here, you can:</p>

<ul>
  <li>Reuse the bootable volume to launch new VMs instantly.</li>
  <li>Integrate Packer builds into your CI/CD pipelines.</li>
  <li>Adapt the same process to build images for other operating systems, such as <a href="https://github.com/hashicorp/packer-plugin-kubevirt/tree/main/examples/builder/kubevirt-iso/rhel">RHEL</a> and <a href="https://github.com/hashicorp/packer-plugin-kubevirt/tree/main/examples/builder/kubevirt-iso/windows">Windows</a>.</li>
</ul>

<p>The plugin is still in pre-release, but it already offers a streamlined way
to create consistent VM images inside Kubernetes.</p>

<p>Give it a try and share your feedback or contributions on <a href="https://github.com/hashicorp/packer-plugin-kubevirt">GitHub</a>!</p>]]></content><author><name>Ben Oukhanov</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="packer" /><category term="plugin" /><category term="images" /><summary type="html"><![CDATA[Packer plugin for KubeVirt that builds VM golden images inside Kubernetes.]]></summary></entry><entry><title type="html">Announcing the release of KubeVirt v1.5</title><link href="https://kubevirt.io//2025/KubeVirt-v1-5_release.html" rel="alternate" type="text/html" title="Announcing the release of KubeVirt v1.5" /><published>2025-03-05T00:00:00+00:00</published><updated>2025-03-05T00:00:00+00:00</updated><id>https://kubevirt.io//2025/KubeVirt-v1-5_release</id><content type="html" xml:base="https://kubevirt.io//2025/KubeVirt-v1-5_release.html"><![CDATA[<p>The KubeVirt Community is pleased to announce the release of <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.5.0">KubeVirt v1.5</a>. This release aligns with <a href="https://kubernetes.io/blog/2024/12/11/kubernetes-v1-32-release/">Kubernetes v1.32</a> and is the seventh KubeVirt release to follow the Kubernetes release cadence.</p>

<p>This release sees the project adding some features that are aligned with more traditional virtualization platforms, such as enhanced volume and VM migration, increased CPU performance, and more precise network state control.</p>

<p>You can read the full <a href="https://kubevirt.io/user-guide/release_notes/#v150">release notes</a> in our user-guide, but we have included some highlights in this blog.</p>

<h3 id="breaking-change">Breaking change</h3>
<p>Please be aware that in v1.5 we have <a href="https://github.com/kubevirt/kubevirt/pull/13497">introduced a change</a> that affects permissions of namespace admins to trigger live migrations. As a hardening measure (principle of least privilege), the right of creating, editing and deleting <code class="language-plaintext highlighter-rouge">VirtualMachineInstanceMigrations</code> are no longer assigned by default to namespace admins.</p>

<p>For more information, see our post on the <a href="https://kubevirt.io/2025/Hardening-VMIM.html">KubeVirt blog</a>.</p>

<h3 id="feature-ga">Feature GA</h3>

<p>This release marks the graduation of a number of features to GA; deprecating the feature gate and now enabled by default:</p>

<ul>
  <li><a href="https://kubevirt.io/user-guide/storage/volume_migration/">Migration Update Strategy and Volume Migration</a>: Storage migration can be useful in the cases where the users need to change the underlying storage, for example, if the storage class has been deprecated, or there is a new more performant driver available.</li>
  <li><a href="https://kubevirt.io/user-guide/compute/resources_requests_and_limits/">Auto Resource Limits</a>: Automatically apply CPU limits to a VMI.</li>
  <li>VM Live Update Features: This feature underpins hotplugging of CPU, memory, and volume resources.</li>
  <li><a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network Binding Plugin</a>: A modular plugin which integrates with KubeVirt to implement a network binding.</li>
</ul>

<h3 id="compute">Compute</h3>

<p>You can now specify the number of <a href="https://kubevirt.io/user-guide/storage/disks_and_volumes/#iothreads">IOThreads to use</a> through virtqueue mapping to improve CPU performance. We also added <a href="https://github.com/kubevirt/kubevirt/pull/13606">virtio video support for amd64</a> as well as the <a href="https://github.com/kubevirt/kubevirt/pull/13208">ability to reset VMs</a>, which provides the means to restart the guest OS without requiring a new pod to be scheduled.</p>

<h3 id="networking">Networking</h3>

<p>You can now <a href="https://kubevirt.io/user-guide/network/interfaces_and_networks/#link-state-management">dynamically control the link state</a> (up/down) of a network interface.</p>

<h3 id="scale-and-performance">Scale and Performance</h3>

<p>A comprehensive list of performance and scale benchmarks for the release is <a href="https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md">available here</a>. A notable change added to the benchmarks was the <a href="https://github.com/kubevirt/kubevirt/pull/13250">virt-handler resource utilization metrics</a>. This metric gives the avg, max and min memory/cpu utilization per VMI that is scheduled on the node where virt-handler is running. Another notable shoutout from the benchmark document is changing how <a href="https://github.com/kubevirt/kubevirt/pull/12716">list calls are tracked</a>. KubeVirt clients were misreporting watch calls as list calls, which was fixed in this release.</p>

<h3 id="storage">Storage</h3>

<p>With this release you can now migrate <a href="https://kubevirt.io/user-guide/storage/volume_migration/">hotplugged volumes</a>. You can also migrate VMIs with a volume shared using virtiofs. And we <a href="https://github.com/kubevirt/kubevirt/pull/13713">addressed a recent change in libvirt</a> that was preventing some NFS shared volumes from migrating by providing shared filesystem paths upfront.</p>

<h3 id="thank-you-for-your-contribution">Thank you for your contribution!</h3>
<p>A lot of work from a <a href="https://kubevirt.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.4.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=All&amp;var-country_name=All&amp;var-companies=All">huge amount of people</a> goes into a release. A huge thank you to the 350+ people who contributed to this v1.5 release.</p>

<p>And if you’re interested in contributing to the project and being a part of the next release, please check out our <a href="https://kubevirt.io/user-guide/contributing/">contributing guide</a> and our <a href="https://github.com/kubevirt/community/blob/main/membership_policy.md">community membership guidelines</a>.</p>

<p>Contributing needn’t be designing a new feature or committing to a <a href="https://github.com/kubevirt/enhancements">Virtualization Enhancement Proposal</a>, there is always a need for reviews, help with our docs and website, or submitting good quality bugs. Every little bit counts.</p>]]></content><author><name>KubeVirt Maintainers</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.5" /><category term="release" /><category term="community" /><category term="cncf" /><category term="milestone" /><category term="party time" /><summary type="html"><![CDATA[With the release of KubeVirt v1.5 we see the community adding some features that align with more traditional virtualization platforms.]]></summary></entry><entry><title type="html">VirtualMachineInstanceMigrations RBAC hardening</title><link href="https://kubevirt.io//2025/Hardening-VMIM.html" rel="alternate" type="text/html" title="VirtualMachineInstanceMigrations RBAC hardening" /><published>2025-02-26T00:00:00+00:00</published><updated>2025-02-26T00:00:00+00:00</updated><id>https://kubevirt.io//2025/Hardening-VMIM</id><content type="html" xml:base="https://kubevirt.io//2025/Hardening-VMIM.html"><![CDATA[<h3 id="context">Context</h3>
<p>The request to live migrate a VM is represented by a <code class="language-plaintext highlighter-rouge">VirtualMachineInstanceMigration</code> instance.
A VirtualMachineInstanceMigration (VMIM) is a namespaced CRD, and its instances are expected to be in the namespace of the VM they refer to.</p>

<p>Up to KubeVirt v1.4, by default, a namespace admin (usually a namespace “owner” in a less formal definition) was able to create VMs and also VMIM objects to enqueue a live migration request for a VM within their namespace.
At the same time, live migrations can be triggered as part of critical infrastructure operations like node drains or upgrades, which are the domain of cluster admins. <br />
So, if namespace admins can continuously enqueue migration requests or delete scheduled VMIM objects needed for ongoing infrastructure-critical operations, they could delay or even prevent cluster-critical operations started by cluster admins, a role with greater privileges.</p>

<p>It was therefore possible that a malicious, lesser-privileged user could abuse this, causing a kind of DoS at the cluster level.
Even worse, Kubernetes RBAC permissions are purely additive (there are no “deny” rules), and KubeVirt roles are constantly reconciled by the virt-operator, so even a cluster admin who was aware of the issue was unable to deny these permissions as a precautionary measure.</p>

<p>For this reason, starting from KubeVirt v1.5, create/delete/update rights will no longer be granted by default to all namespace admins, in accordance with the principle of least privilege.
A new convenient ClusterRole named <code class="language-plaintext highlighter-rouge">kubevirt.io:migrate</code> has been introduced to allow cluster admins to easily grant this permission to selected users.</p>

<h3 id="side-effects-on-hotplug-operations">Side effects on hotplug operations</h3>
<p>Device hotplug operations, at least for CPU and memory, implicitly trigger a live migration executed by the virt-controller on behalf of the user. These operations will not be affected by this change. Under some circumstances or cluster configurations, live migrations are not automatically triggered when NIC devices are hotplugged. In such cases, the only option for namespace admins is to request VMIM permissions from a cluster admin to manually trigger the migration or concatenate two device hotplug operations (where the second one will implicitly complete the NIC hotplug).</p>

<h3 id="cluster-admin-tasks">Cluster-admin tasks</h3>
<p>A cluster admin can bind the new kubevirt.io:migrate ClusterRole to selected trusted users/groups at the namespace scope using:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-n</span> usernamespace rolebinding kvmigrate <span class="nt">--clusterrole</span><span class="o">=</span>kubevirt.io:migrate <span class="nt">--user</span><span class="o">=</span>user1 <span class="nt">--user</span><span class="o">=</span>user2 <span class="nt">--group</span><span class="o">=</span>group1
</code></pre></div></div>
<p>or at the cluster scope:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create clusterrolebinding kvmigrate <span class="nt">--clusterrole</span><span class="o">=</span>kubevirt.io:migrate <span class="nt">--user</span><span class="o">=</span>user1 <span class="nt">--user</span><span class="o">=</span>user2 <span class="nt">--group</span><span class="o">=</span>group1
</code></pre></div></div>

<p>A cluster admin can also restore the previous behavior (where all namespace admins are allowed to manage migrations) with:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl label <span class="nt">--overwrite</span> clusterrole kubevirt.io:migrate rbac.authorization.k8s.io/aggregate-to-admin<span class="o">=</span><span class="nb">true</span>
</code></pre></div></div>

<p>A highly cautious cluster admin who does not want any disruption due to the upgrade process could still create a temporary ClusterRole for migration before the upgrade, labeling it with <code class="language-plaintext highlighter-rouge">rbac.authorization.k8s.io/aggregate-to-admin=true</code>.
For example:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="s">rbac.authorization.k8s.io/aggregate-to-admin=true</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kubevirt.io:upgrademigrate</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">subresources.kubevirt.io</span>
  <span class="na">resources</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">virtualmachines/migrate</span>
  <span class="na">verbs</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">update</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">kubevirt.io</span>
  <span class="na">resources</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">virtualmachineinstancemigrations</span>
  <span class="na">verbs</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">get</span>
  <span class="pi">-</span> <span class="s">delete</span>
  <span class="pi">-</span> <span class="s">create</span>
  <span class="pi">-</span> <span class="s">update</span>
  <span class="pi">-</span> <span class="s">patch</span>
  <span class="pi">-</span> <span class="s">list</span>
  <span class="pi">-</span> <span class="s">watch</span>
  <span class="pi">-</span> <span class="s">deletecollection</span>
</code></pre></div></div>
<p>This ClusterRole will be aggregated into the <code class="language-plaintext highlighter-rouge">admin</code> role before the KubeVirt upgrade, and the upgrade process will not modify it, ensuring the previous behavior is maintained.
After the upgrade, the cluster admin will have sufficient time to bind the new <code class="language-plaintext highlighter-rouge">kubevirt.io:migrate</code> ClusterRole to selected users before removing the temporary ClusterRole.</p>]]></content><author><name>tiraboschi</name></author><category term="news" /><category term="VMIM" /><category term="migrate" /><category term="migrations" /><category term="RBAC" /><category term="hardening" /><category term="security" /><category term="v1.5" /><summary type="html"><![CDATA[Apply the principle of least privilege (PoLP) to VirtualMachineInstanceMigrations]]></summary></entry><entry><title type="html">You wanted more? It’s KubeVirt v1.4!</title><link href="https://kubevirt.io//2024/KubeVirt-v1-4.html" rel="alternate" type="text/html" title="You wanted more? It’s KubeVirt v1.4!" /><published>2024-11-12T00:00:00+00:00</published><updated>2024-11-12T00:00:00+00:00</updated><id>https://kubevirt.io//2024/KubeVirt-v1-4</id><content type="html" xml:base="https://kubevirt.io//2024/KubeVirt-v1-4.html"><![CDATA[<p>The KubeVirt Community is proud to announce the release of <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.4.0">v1.4</a>. This release aligns with <a href="https://kubernetes.io/blog/2024/08/13/kubernetes-v1-31-release/">Kubernetes v1.31</a> and is the sixth KubeVirt release to follow the Kubernetes release cadence.</p>

<p>What’s 1/3 of one thousand? Because that’s how many people have <a href="https://kubevirt.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.3.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=All&amp;var-country_name=All&amp;var-companies=All">contributed in some way</a> to this release, with 90 of those 333 people <a href="https://kubevirt.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.3.0%20-%20now&amp;var-metric=commits&amp;var-repogroup_name=All&amp;var-country_name=All&amp;var-companies=All">contributing commits to our repos</a>.</p>

<p>You can read the full <a href="https://kubevirt.io/user-guide/release_notes/#v140">release notes</a> in our user-guide, but we have included some highlights in this blog.</p>

<p>For those of you at KubeCon this week, be sure to check out our <a href="https://sched.co/1hoy6">maintainer talk</a> where our project maintainers will be going into these and other recent enhancements in KubeVirt.</p>

<h3 id="feature-ga">Feature GA</h3>
<p>This release marks the graduation of a number of features to GA; deprecating the feature gate and now enabled by default:</p>

<ul>
  <li><a href="https://kubevirt.io/user-guide/network/hotplug_interfaces/#hotplug-network-interfaces">Network hotplug</a>: Add network interfaces to, and remove them from, running virtual machines.</li>
  <li><a href="https://kubevirt.io/user-guide/user_workloads/instancetypes/">Common Instance types</a>: Simplify virtual machine creation with a predefined set of resource, performance, and runtime settings. We have also introduced a single configurable for cluster admins to explicitly disable this feature if required.</li>
  <li><a href="https://deploy-preview-840--kubevirt-user-guide.netlify.app/compute/numa/">NUMA</a>: Improving performance by mapping host NUMA topology to virtual machine topology.</li>
  <li><a href="https://deploy-preview-840--kubevirt-user-guide.netlify.app/compute/host-devices/#host-devices-assignment">GPU assignment</a>: An oldie but a goodie: Assign GPUs and vGPUs to virtual machines.</li>
</ul>

<p>This version of KubeVirt includes upgraded virtualization technology based on <a href="https://www.libvirt.org/news.html#v10-5-0-2024-07-01">libvirt 10.5.0</a> and <a href="https://www.qemu.org/2024/04/23/qemu-9-0-0/">QEMU 9.0.0</a>. Other KubeVirt-specific features of this release include the following:</p>

<h3 id="virtualization">Virtualization</h3>
<p>In the interest of security, we have restricted the <a href="https://github.com/kubevirt/kubevirt/pull/11982">ability of virt-handler</a> to patch nodes, and removed privileges for the cluster. You can also now <a href="https://github.com/kubevirt/kubevirt/pull/13090">live-update tolerations</a> to a running VM.</p>

<p>Our KubeVirt command line tool, virtctl, also received some love and <a href="https://kubevirt.io/user-guide/release_notes/ba#sig-compute">improved functionality</a> for VM creation, image upload, and source inference.</p>

<h3 id="networking">Networking</h3>
<p>The networking binding plugins have matured to Beta, and we have a new domain attachment type,<a href="https://github.com/kubevirt/kubevirt/pull/13024"><code class="language-plaintext highlighter-rouge">managedTap</code></a>, and the ability to <a href="https://github.com/kubevirt/kubevirt/pull/12235">reserve memory overhead</a> for binding plugins. <a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network binding plugins</a> enable vendors to provide their own VM-to-network plumbing alongside KubeVirt.</p>

<p>We also added support for the <code class="language-plaintext highlighter-rouge">igb</code> network interface model.</p>

<h3 id="storage">Storage</h3>
<p>If you’ve ever wanted to migrate your virtual machine volume from one storage type to another then you’ll be interested in our <a href="https://kubevirt.io/user-guide/storage/volume_migration/">volume migration</a> feature.</p>

<h3 id="scale-and-performance">Scale and Performance</h3>
<p>Our SIG scale and performance team have added performance benchmarks for resource utilization of virt-controller and virt-api components. Furthermore, the test-suite was enhanced by <a href="https://github.com/kubevirt/kubevirt/pull/12117">integrating KWOK with SIG-scale tests</a> to simulate nodes and VMIs to test KubeVirt performance while using minimum resources in test infrastructure. A comprehensive list of performance and scale benchmarks for the release is available <a href="https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md">here</a>.</p>

<h3 id="thanks">Thanks!</h3>
<p>A lot of work from a huge amount of people go into these releases. Some contributions are small, such as raising a bug or attending our community meeting, and others are massive, like working on a feature or reviewing PRs. Whatever your part: we thank you.</p>

<p>And if you’re interested in contributing to the project and being a part of the next release, please check out our <a href="https://kubevirt.io/user-guide/contributing/">contributing guide</a> and our <a href="https://github.com/kubevirt/community/blob/main/membership_policy.md">community membership guidelines</a>.</p>]]></content><author><name>KubeVirt Maintainers</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.4" /><category term="release" /><category term="community" /><category term="cncf" /><category term="milestone" /><category term="party time" /><summary type="html"><![CDATA[Introducing the KubeVirt v1.4 release]]></summary></entry><entry><title type="html">KubeVirt Summit 2024 CfP is open!</title><link href="https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP.html" rel="alternate" type="text/html" title="KubeVirt Summit 2024 CfP is open!" /><published>2024-03-19T00:00:00+00:00</published><updated>2024-03-19T00:00:00+00:00</updated><id>https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP</id><content type="html" xml:base="https://kubevirt.io//2024/KubeVirt-Summit-2024-CfP.html"><![CDATA[<p>We are very pleased to announce the details for this year’s KubeVirt Summit!!</p>

<h2 id="what-is-kubevirt-summit">What is KubeVirt Summit?</h2>

<p>KubeVirt Summit is our annual online conference, now in its fourth year, in which the entire broader community meets to showcase technical architecture, new features, proposed changes, and in-depth tutorials.
We have two tracks to cater for developer talks, and another for end users to share their deployment journey with KubeVirt and their use case(s) at scale. And there’s no reason why a talk can’t be both :)</p>

<h2 id="when-is-it">When is it?</h2>

<p>The event will take place online over two half-days:</p>

<ul>
  <li>Dates: <del>June 25 and 26</del> <strong>June 24 and 25</strong>, 2024</li>
  <li>Time: TBD 
(In the past we have aimed for 1200-1700 UTC but may modify these times slightly depending on our speaker timezones)</li>
</ul>

<h2 id="how-to-submit-a-proposal">How to submit a proposal?</h2>

<ul>
  <li>Please submit through this <a href="https://docs.google.com/forms/d/e/1FAIpQLSeELmfpD_20kZnrciXkdSdDS_MLFLN9xSaZDKptNPjg3JGLaA/viewform">Googleform</a></li>
  <li>CfP closes: <strong>May 20</strong>, 2024</li>
  <li>Schedule will announced at the end of May</li>
</ul>

<p>Do consider proposing a session, and help make our fourth Summit as valuable as possible. We welcome a range of session types, any of which can be simple and intended for beginners or face-meltingly technical. Check out <a href="https://www.youtube.com/playlist?list=PLnLpXX8KHIYwe_V5pCXfXVDs-lY5dX55Q">last year’s talks</a> for some ideas.</p>

<h2 id="have-questions">Have questions?</h2>

<p>Our <a href="/summit/">KubeVirt Summit 2024</a> page will continue to evolve with details as we get closer.</p>

<p>You can also reach out on our <a href="https://kubernetes.slack.com/messages/virtualization">virtualization</a> Slack channel (in the Kubernetes workspace).</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our mailing list, slack channels, weekly meetings, and more, all list in our <a href="https://github.com/kubevirt/community">community repo</a>.</p>

<p>Good luck!</p>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's fourth annual dedicated online event]]></summary></entry><entry><title type="html">Announcing KubeVirt v1.1</title><link href="https://kubevirt.io//2023/Announcing-KubeVirt-v1-1.html" rel="alternate" type="text/html" title="Announcing KubeVirt v1.1" /><published>2023-11-07T00:00:00+00:00</published><updated>2023-11-07T00:00:00+00:00</updated><id>https://kubevirt.io//2023/Announcing-KubeVirt-v1-1</id><content type="html" xml:base="https://kubevirt.io//2023/Announcing-KubeVirt-v1-1.html"><![CDATA[<p>The KubeVirt Community is very pleased to announce the release of KubeVirt v1.1. This comes 17 weeks after our celebrated v1.0 release, and follows the predictable schedule we moved to three releases ago to follow the Kubernetes release cadence.</p>

<p>You can read the full <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.1.0">v1.1 release notes here</a>, but we’ve asked the KubeVirt SIGs to summarize their largest successes, as well as one of the community members from Arm to list their integration accomplishments for this release.</p>

<h2 id="sig-compute">SIG-compute</h2>
<p>SIG-compute covers the core functionality of KubeVirt. This includes scheduling VMs, the API, and all KubeVirt operators.</p>

<p>For the v1.1 release, we have added quite a few features. This includes memory hotplug, as a follow up to CPU hotplug, which was part of the 1.0 release. Basic KSM support was already part of KubeVirt, but we have now extended that with more tuning parameters and KubeVirt can also dynamically configure KSM based on system pressure. We’ve added persistent NVRAM support (requires that a VM use UEFI) so that settings are preserved across reboots.</p>

<p>We’ve also added host-side USB passthrough support, so that USB devices on a cluster node can be made available to workloads. KubeVirt can now automatically apply limits to a VM running in a namespace with quotas. We’ve also added refinements to VM cloning, as well as the ability to create clones using the virtctl CLI tool. And you can now stream guest’s console logs.</p>

<p>Finally, on the confidential computing front, we now have an API for SEV attestation.</p>

<h2 id="sig-infra">SIG-infra</h2>
<p>SIG-infra takes care of KubeVirt’s own infrastructure, user workloads and other user-focused integrations through automation and the reduction of complexity wherever possible, providing a quality experience for end users.</p>

<p>In this release, two major instance type-related features were added to KubeVirt. The first feature is the deployment of Common InstanceTypes by the virt-operator. This provides users with a useful set of InstanceTypes and Preferences right out of the box and allows them to easily create virtual machines tailored to the needs of their workloads. For now this feature remains behind a feature gate, but in future versions we aim to enable the deployment by default.</p>

<p>Secondly, the inference of InstanceTypes and Preferences has been enabled by default when creating virtual machines with virtctl. This feature was already present in the previous release, but users still needed to explicitly enable it. Now it is enabled by default, being as transparent as possible so as to not let the creation of virtual machines fail if inference should not be possible. This significantly improves usability, as the command line for creating virtual machines is now even simpler.</p>

<h2 id="sig-network">SIG-network</h2>
<p>SIG-network is committed to enhancing and maintaining all aspects of Virtual Machine network connectivity and management in KubeVirt.</p>

<p>For the v1.1 release, we have re-designed the interface hot plug/unplug API, while adding hotplug support for SR-IOV interfaces. On top of that, we have added a network binding option allowing the community to extend the KubeVirt network configuration in the pod by injecting custom CNI plugins to configure the networking stack, and a sidecar to configure the libvirt domain. The existing <code class="language-plaintext highlighter-rouge">slirp</code> network configuration has been extracted from the code and re-designed as one such network binding, and can be used by the community as an example on how to extend KubeVirt bindings.</p>

<h2 id="sig-scale">SIG-scale</h2>
<p>SIG-scale continues to track scale and performance across releases.  The v1.1 testing lanes ran on Kubernetes 1.27 and we observed a slight performance improvement from Kubernetes.  There’s no other notable performance or scale changes in KubeVirt v1.1 as our focus has been on improving our tracking.</p>

<h4 id="vmicreationtorunningsecondsp95">vmiCreationToRunningSecondsP95</h4>
<ul>
  <li>The gray dotted line in the graph is Feb 1, 2023, denoting release of v0.59</li>
  <li>The blue dotted line in the graph is March 1, 2023, denoting release of v0.60</li>
  <li>The green dotted line in the graph is July 6, 2023, denoting release of v1.0.0</li>
  <li>The red dotted line in the graph is September 6, 2023, denoting change in k8s provider from v1.25 to v1.27</li>
</ul>

<p><img src="/assets/2023-11-07-Announcing-KubeVirt-v1-1/vmi-p95-Creation-to-Running.png" alt="Alt text" />
<img src="/assets/2023-11-07-Announcing-KubeVirt-v1-1/vm-p95-Creation-to-Running.png" alt="Alt text" /></p>

<p>Full v1.1 data source: <a href="https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md">https://github.com/kubevirt/kubevirt/blob/main/docs/perf-scale-benchmarks.md</a></p>

<h2 id="sig-storage">SIG-storage</h2>
<p>SIG-storage is focused on providing persistent storage to KubeVirt VMs and managing that storage throughout the lifecycle of the VM. This begins with provisioning and populating PVCs with bootable images but also includes features such as disk hotplug, snapshots, backup and restore, disaster recovery, and virtual machine export.</p>

<p>For this release we aimed to draw closer to Kubernetes principles when it comes to managing storage artifacts. Introducing CDI volume populators, which is CDI’s implementation of importing/uploading/cloning data to PVCs using the <code class="language-plaintext highlighter-rouge">dataSourceRef</code> field. This follows the Kubernetes way of populating PVCs and enables us to populate PVCs directly without the need for DataVolumes, an important but bespoke object that has served the KubeVirt use case for many years.</p>

<p>Speaking of DataVolumes, they will no longer be garbage collected by default, something that violated a fundamental principle of Kubernetes (even though it was very useful for our use case).</p>

<p>And, finally, we can now use snapshots to store operating system “golden images”, to serve as the base image for cloning.</p>

<h2 id="kubevirt-and-arm">KubeVirt and Arm</h2>
<p>We are excited to announce the successful integration of KubeVirt on Arm64 platforms. Here are some key accomplishments:</p>
<ol>
  <li><strong>Building and Compiling</strong>: We have released multi-architecture KubeVirt component images and binaries, while also allowing cross-compiling Arm64 architecture images and binaries on x86_64 platforms.</li>
  <li><strong>Core Functionality</strong>: Our dedicated efforts have focused on enabling the core functionality of KubeVirt on Arm64 platforms.</li>
  <li><strong>Testing Integration</strong>: Quality assurance is of paramount importance. We have integrated unit tests and end-to-end tests on Arm64 servers into the pull request (PR) pre-submit process. This guarantees that KubeVirt maintains its reliability and functionality on Arm64.</li>
  <li><strong>Comprehensive Documentation</strong>: To provide valuable insights into KubeVirt’s capabilities on Arm64 platforms, we have compiled extensive documentation. Explore the status of <a href="https://kubevirt.io/user-guide/operations/feature_gate_status_on_Arm64/">feature gates</a> and dive into <a href="https://kubevirt.io/user-guide/virtual_machines/device_status_on_Arm64/">device status documentation</a>.</li>
  <li><strong>Hybrid Cluster Compatibility Preview</strong>: Hybrid x86_64 and Arm64 clusters can work together now as a preview feature. Try it out and provide feedback.</li>
</ol>

<p>We are thrilled to declare that KubeVirt now offers tier-one support on Arm64 platforms. This milestone represents a culmination of collaborative efforts, unwavering dedication, and a commitment to innovation within the KubeVirt community. KubeVirt is no longer just an option; it has evolved to become a first-class citizen on Arm64 platforms.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Thank you to everyone in the KubeVirt Community who contributed to this release, whether you pitched in on any of the features listed above, helped out with any of the other features or maintenance improvements listed in our release notes, or made any number of non-code contributions to our website, user guide or meetings.</p>]]></content><author><name>KubeVirt Community</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.1.0" /><category term="release" /><category term="community" /><category term="cncf" /><summary type="html"><![CDATA[We are very pleased to announce the release of KubeVirt v1.1!]]></summary></entry><entry><title type="html">Running KubeVirt with Cluster Autoscaler</title><link href="https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes.html" rel="alternate" type="text/html" title="Running KubeVirt with Cluster Autoscaler" /><published>2023-09-06T00:00:00+00:00</published><updated>2023-09-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes</id><content type="html" xml:base="https://kubevirt.io//2023/KubeVirt-on-autoscaling-nodes.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>For this article, we’ll learn about the process of setting up
<a href="https://kubevirt.io/">KubeVirt</a> with <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">Cluster
Autoscaler</a>
on EKS. In addition, we’ll be using bare metal nodes to host KubeVirt VMs.</p>

<h2 id="required-base-knowledge">Required Base Knowledge</h2>

<p>This article will talk about how to make various software systems work together
but introducing each one in detail is outside of its scope. Thus, you must already:</p>

<ol>
  <li>Know how to administer a Kubernetes cluster;</li>
  <li>Be familiar with AWS, specifically IAM and EKS; and</li>
  <li>Have some experience with KubeVirt.</li>
</ol>

<h2 id="companion-code">Companion Code</h2>

<p>All the code used in this article may also be found at
<a href="https://github.com/relaxdiego/kubevirt-cas-baremetal">github.com/relaxdiego/kubevirt-cas-baremetal</a>.</p>

<h2 id="set-up-the-cluster">Set Up the Cluster</h2>

<h3 id="shared-environment-variables">Shared environment variables</h3>

<p>First let’s set some environment variables:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The name of the EKS cluster we're going to create</span>
<span class="nb">export </span><span class="nv">RD_CLUSTER_NAME</span><span class="o">=</span>my-cluster

<span class="c"># The region where we will create the cluster</span>
<span class="nb">export </span><span class="nv">RD_REGION</span><span class="o">=</span>us-west-2

<span class="c"># Kubernetes version to use</span>
<span class="nb">export </span><span class="nv">RD_K8S_VERSION</span><span class="o">=</span>1.27

<span class="c"># The name of the keypair that we're going to inject into the nodes. You</span>
<span class="c"># must create this ahead of time in the correct region.</span>
<span class="nb">export </span><span class="nv">RD_EC2_KEYPAIR_NAME</span><span class="o">=</span>eks-my-cluster
</code></pre></div></div>

<h3 id="prepare-the-clusteryaml-file">Prepare the cluster.yaml file</h3>

<p>Using <a href="https://eksctl.io/">eksctl</a>, prepare an EKS cluster config:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eksctl create cluster <span class="se">\</span>
    <span class="nt">--dry-run</span> <span class="se">\</span>
    <span class="nt">--name</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--nodegroup-name</span> ng-infra <span class="se">\</span>
    <span class="nt">--node-type</span> m5.xlarge <span class="se">\</span>
    <span class="nt">--nodes</span> 2 <span class="se">\</span>
    <span class="nt">--nodes-min</span> 2 <span class="se">\</span>
    <span class="nt">--nodes-max</span> 2 <span class="se">\</span>
    <span class="nt">--node-labels</span> <span class="nv">workload</span><span class="o">=</span>infra <span class="se">\</span>
    <span class="nt">--region</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--ssh-access</span> <span class="se">\</span>
    <span class="nt">--ssh-public-key</span> <span class="k">${</span><span class="nv">RD_EC2_KEYPAIR_NAME</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--version</span> <span class="k">${</span><span class="nv">RD_K8S_VERSION</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--vpc-nat-mode</span> HighlyAvailable <span class="se">\</span>
    <span class="nt">--with-oidc</span> <span class="se">\</span>
<span class="o">&gt;</span> cluster.yaml
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">--dry-run</code> means the command will not actually create the cluster but will
instead output a config to stdout which we then write to <code class="language-plaintext highlighter-rouge">cluster.yaml</code>.</p>

<p>Open the file and look at what it has produced.</p>

<blockquote>
  <p>For more info on the schema used by <code class="language-plaintext highlighter-rouge">cluster.yaml</code>, see the <a href="https://eksctl.io/usage/schema/">Config file
schema</a> page from eksctl.io</p>
</blockquote>

<p>This cluster will start out with a node group that we will use to host our
“infra” services. This is why we are using the cheaper <code class="language-plaintext highlighter-rouge">m5.xlarge</code> rather than
a baremetal instance type. However, we also need to ensure that none of our VMs
will ever be scheduled in these nodes. Thus we need to taint them. In the
generated <code class="language-plaintext highlighter-rouge">cluster.yaml</code> file, append the following taint to the only node
group in the <code class="language-plaintext highlighter-rouge">managedNodeGroups</code> list:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">managedNodeGroups</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">amiFamily</span><span class="pi">:</span> <span class="s">AmazonLinux2</span>
  <span class="s">...</span>
  <span class="na">taints</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">CriticalAddonsOnly</span>
      <span class="na">effect</span><span class="pi">:</span> <span class="s">NoSchedule</span>
</code></pre></div></div>

<h3 id="create-the-cluster">Create the cluster</h3>

<p>We can now create the cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eksctl create cluster <span class="nt">--config-file</span> cluster.yaml
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2023-08-20 07:59:14 [ℹ]  eksctl version ...
2023-08-20 07:59:14 [ℹ]  using region us-west-2 ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2a ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2b ...
2023-08-20 07:59:14 [ℹ]  subnets for us-west-2c ...
...
2023-08-20 08:14:06 [ℹ]  kubectl command should work with ...
2023-08-20 08:14:06 [✔]  EKS cluster "my-cluster" in "us-west-2" is ready
</code></pre></div></div>

<p>Once the command is done, you should be able to query the the kube API. For
example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get nodes
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                      STATUS   ROLES    AGE     VERSION
ip-XXX.compute.internal   Ready    &lt;none&gt;   32m     v1.27.4-eks-2d98532
ip-YYY.compute.internal   Ready    &lt;none&gt;   32m     v1.27.4-eks-2d98532
</code></pre></div></div>

<h3 id="create-the-node-groups">Create the Node Groups</h3>

<p>As per <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">this section of the Cluster Autoscaler
docs</a>:</p>

<blockquote>
  <p>If you’re using Persistent Volumes, your deployment needs to run in the same
AZ as where the EBS volume is, otherwise the pod scheduling could fail if it
is scheduled in a different AZ and cannot find the EBS volume. To overcome
this, either use a single AZ ASG for this use case, or an ASG-per-AZ while
enabling <code class="language-plaintext highlighter-rouge">--balance-similar-node-groups</code>.</p>
</blockquote>

<p>Based on the above, we will create a node group for each of the availability
zones (AZs) that was declared in <code class="language-plaintext highlighter-rouge">cluster.yaml</code> so that the Cluster Autoscaler will
always bring up a node in the AZ where a VM’s EBS-backed PV is located.</p>

<p>To do that, we will first prepare a template that we can then feed to
<code class="language-plaintext highlighter-rouge">envsubst</code>. Save the following in <code class="language-plaintext highlighter-rouge">node-group.yaml.template</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="c1"># See: Config File Schema &lt;https://eksctl.io/usage/schema/&gt;</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">eksctl.io/v1alpha5</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterConfig</span>

<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">${RD_CLUSTER_NAME}</span>
  <span class="na">region</span><span class="pi">:</span> <span class="s">${RD_REGION}</span>

<span class="na">managedNodeGroups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
    <span class="na">amiFamily</span><span class="pi">:</span> <span class="s">AmazonLinux2</span>
    <span class="na">instanceType</span><span class="pi">:</span> <span class="s">c5.metal</span>
    <span class="na">availabilityZones</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">${EKS_AZ}</span>
    <span class="na">desiredCapacity</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">maxSize</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">minSize</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">alpha.eksctl.io/cluster-name</span><span class="pi">:</span> <span class="s">my-cluster</span>
      <span class="na">alpha.eksctl.io/nodegroup-name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
      <span class="na">workload</span><span class="pi">:</span> <span class="s">vm</span>
    <span class="na">privateNetworking</span><span class="pi">:</span> <span class="kc">false</span>
    <span class="na">ssh</span><span class="pi">:</span>
      <span class="na">allow</span><span class="pi">:</span> <span class="kc">true</span>
      <span class="na">publicKeyPath</span><span class="pi">:</span> <span class="s">${RD_EC2_KEYPAIR_NAME}</span>
    <span class="na">volumeSize</span><span class="pi">:</span> <span class="m">500</span>
    <span class="na">volumeIOPS</span><span class="pi">:</span> <span class="m">10000</span>
    <span class="na">volumeThroughput</span><span class="pi">:</span> <span class="m">750</span>
    <span class="na">volumeType</span><span class="pi">:</span> <span class="s">gp3</span>
    <span class="na">propagateASGTags</span><span class="pi">:</span> <span class="kc">true</span>
    <span class="na">tags</span><span class="pi">:</span>
      <span class="na">alpha.eksctl.io/nodegroup-name</span><span class="pi">:</span> <span class="s">ng-${EKS_AZ}-c5-metal</span>
      <span class="na">alpha.eksctl.io/nodegroup-type</span><span class="pi">:</span> <span class="s">managed</span>
      <span class="na">k8s.io/cluster-autoscaler/my-cluster</span><span class="pi">:</span> <span class="s">owned</span>
      <span class="na">k8s.io/cluster-autoscaler/enabled</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
      <span class="c1"># The following tags help CAS determine that this node group is able</span>
      <span class="c1"># to satisfy the label and resource requirements of the KubeVirt VMs.</span>
      <span class="c1"># See: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>
      <span class="na">k8s.io/cluster-autoscaler/node-template/label/kubevirt.io/schedulable</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
</code></pre></div></div>

<p>The last few tags bears additional emphasis. They are required because when a
virtual machine is created, it will have the following requirements:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">requests</span><span class="pi">:</span>
  <span class="na">devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>

<span class="na">nodeSelectors</span><span class="pi">:</span> <span class="s">kubevirt.io/schedulable=true</span>
</code></pre></div></div>

<p>However, at least when scaling from zero for the first time, CAS will have no
knowledge of this information unless the correct AWS tags are added to the node
group. This is why we have the following added to the managed node group’s
tags:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/kvm</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/tun</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/devices.kubevirt.io/vhost-net</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage</span><span class="pi">:</span> <span class="s">50M</span>
<span class="na">k8s.io/cluster-autoscaler/node-template/label/kubevirt.io/schedulable</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
</code></pre></div></div>

<blockquote>
  <p>For more information on these tags, see <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup">Auto-Discovery
Setup</a>.</p>
</blockquote>

<h3 id="create-the-vm-node-groups">Create the VM Node Groups</h3>

<p>We can now create the node group:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yq .availabilityZones[] cluster.yaml <span class="nt">-r</span> | <span class="se">\</span>
    xargs <span class="nt">-I</span><span class="o">{}</span> bash <span class="nt">-c</span> <span class="s2">"
        export EKS_AZ={};
        envsubst &lt; node-group.yaml.template | </span><span class="se">\</span><span class="s2">
        eksctl create nodegroup --config-file -
    "</span>
</code></pre></div></div>

<h2 id="deploy-kubevirt">Deploy KubeVirt</h2>

<blockquote>
  <p>The following was adapted from <a href="https://kubevirt.io/quickstart_cloud/">KubeVirt quickstart with cloud
providers</a>.</p>
</blockquote>

<p>Deploy the KubeVirt operator:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> https://github.com/kubevirt/kubevirt/releases/download/v1.0.0/kubevirt-operator.yaml
</code></pre></div></div>

<p>So that the operator will know how to deploy KubeVirt, let’s add the <code class="language-plaintext highlighter-rouge">KubeVirt</code>
resource:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: kubevirt.io/v1
kind: KubeVirt
metadata:
  name: kubevirt
  namespace: kubevirt
spec:
  certificateRotateStrategy: {}
  configuration:
    developerConfiguration:
      featureGates: []
  customizeComponents: {}
  imagePullPolicy: IfNotPresent
  workloadUpdateStrategy: {}
  infra:
    nodePlacement:
      nodeSelector:
        workload: infra
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
</span><span class="no">EOF
</span></code></pre></div></div>

<blockquote>
  <p>Notice how we are specifically configuring KubeVirt itself to tolerate the
<code class="language-plaintext highlighter-rouge">CriticalAddonsOnly</code> taint. This is so that the KubeVirt services themselves
can be scheduled in the infra nodes instead of the bare metal nodes which we
want to scale down to zero when there are no VMs.</p>
</blockquote>

<p>Wait until KubeVirt is in a <code class="language-plaintext highlighter-rouge">Deployed</code> state:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get <span class="nt">-n</span> kubevirt <span class="nt">-o</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.phase}"</span> <span class="se">\</span>
	kubevirt.kubevirt.io/kubevirt
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Deployed
</code></pre></div></div>

<p>Double check that all KubeVirt components are healthy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-n</span> kubevirt
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                 READY   STATUS    RESTARTS       AGE
pod/virt-api-674467958c-5chhj        1/1     Running   0              98d
pod/virt-api-674467958c-wzcmk        1/1     Running   0              5d
pod/virt-controller-6768977b-49wwb   1/1     Running   0              98d
pod/virt-controller-6768977b-6pfcm   1/1     Running   0              5d
pod/virt-handler-4hztq               1/1     Running   0              5d
pod/virt-handler-x98x5               1/1     Running   0              98d
pod/virt-operator-85f65df79b-lg8xb   1/1     Running   0              5d
pod/virt-operator-85f65df79b-rp8p5   1/1     Running   0              98d
</code></pre></div></div>

<h2 id="deploy-a-vm-to-test">Deploy a VM to test</h2>

<blockquote>
  <p>The following is copied from
<a href="https://kubevirt.io/user-guide/virtual_machines/accessing_virtual_machines/#static-ssh-public-key-injection-via-cloud-init">kubevirt.io</a>.</p>
</blockquote>

<p>First create a secret from your public key:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create secret generic my-pub-key <span class="nt">--from-file</span><span class="o">=</span><span class="nv">key1</span><span class="o">=</span>~/.ssh/id_rsa.pub
</code></pre></div></div>

<p>Next, create the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a VM referencing the Secret using propagation method configDrive</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: testvm
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          rng: {}
        resources:
          requests:
            memory: 1024M
      terminationGracePeriodSeconds: 0
      accessCredentials:
      - sshPublicKey:
          source:
            secret:
              secretName: my-pub-key
          propagationMethod:
            configDrive: {}
      volumes:
      - containerDisk:
          image: quay.io/containerdisks/fedora:latest
        name: containerdisk
      - cloudInitConfigDrive:
          userData: |-
            #cloud-config
            password: fedora
            chpasswd: { expire: False }
        name: cloudinitdisk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Check that the test VM is running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get vm
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME        AGE     STATUS               READY
testvm      30s     Running              True
</code></pre></div></div>

<p>Delete the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl delete testvm
</code></pre></div></div>

<h2 id="set-up-cluster-autoscaler">Set Up Cluster Autoscaler</h2>

<h3 id="prepare-the-permissions-for-cluster-autoscaler">Prepare the permissions for Cluster Autoscaler</h3>

<p>So that CAS can set the desired capacity of each node group dynamically, we
must grant it limited access to certain AWS resources. The first step to this
is to define the IAM policy.</p>

<blockquote>
  <p>This section is based off of the “Create an IAM policy and role” section of
the <a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html">AWS
Autoscaling</a>
documentation.</p>
</blockquote>

<h3 id="create-the-cluster-specific-policy-document">Create the cluster-specific policy document</h3>

<p>Prepare the policy document by rendering the following file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> policy.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "autoscaling:SetDesiredCapacity",
                "autoscaling:TerminateInstanceInAutoScalingGroup"
            ],
            "Resource": "*"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeAutoScalingInstances",
                "autoscaling:DescribeAutoScalingGroups",
                "ec2:DescribeLaunchTemplateVersions",
                "autoscaling:DescribeTags",
                "autoscaling:DescribeLaunchConfigurations",
                "ec2:DescribeInstanceTypes"
            ],
            "Resource": "*"
        }
    ]
}
</span><span class="no">EOF
</span></code></pre></div></div>

<p>The above should be enough for CAS to do its job. Next, create the policy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws iam create-policy <span class="se">\</span>
    <span class="nt">--policy-name</span> eks-<span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span>-<span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span><span class="nt">-ClusterAutoscalerPolicy</span> <span class="se">\</span>
    <span class="nt">--policy-document</span> file://policy.json
</code></pre></div></div>

<blockquote>
  <p>IMPORTANT: Take note of the returned policy ARN. You will need that below.</p>
</blockquote>

<h3 id="create-the-iam-role-and-k8s-service-account-pair">Create the IAM role and k8s service account pair</h3>

<p>The Cluster Autoscaler needs a service account in the k8s cluster that’s
associated with an IAM role that consumes the policy document we created in the
previous section. This is normally a two-step process but can be created in a
single command using <code class="language-plaintext highlighter-rouge">eksctl</code>:</p>

<blockquote>
  <p>For more information on what <code class="language-plaintext highlighter-rouge">eksctl</code> is doing under the covers, see <a href="https://eksctl.io/usage/iamserviceaccounts/#how-it-works">How It
Works</a> from the
<code class="language-plaintext highlighter-rouge">eksctl</code> documentation for IAM Roles for Service Accounts.</p>
</blockquote>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RD_POLICY_ARN</span><span class="o">=</span><span class="s2">"&lt;Get this value from the last command's output&gt;"</span>

eksctl create iamserviceaccount <span class="se">\</span>
	<span class="nt">--cluster</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_CLUSTER_NAME</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--region</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_REGION</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--namespace</span><span class="o">=</span>kube-system <span class="se">\</span>
	<span class="nt">--name</span><span class="o">=</span>cluster-autoscaler <span class="se">\</span>
	<span class="nt">--attach-policy-arn</span><span class="o">=</span><span class="k">${</span><span class="nv">RD_POLICY_ARN</span><span class="k">}</span> <span class="se">\</span>
	<span class="nt">--override-existing-serviceaccounts</span> <span class="se">\</span>
	<span class="nt">--approve</span>
</code></pre></div></div>

<p>Double check that the <code class="language-plaintext highlighter-rouge">cluster-autoscaler</code> service account has been correctly
annotated with the IAM role that was created by <code class="language-plaintext highlighter-rouge">eksctl</code> in the same step:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get sa cluster-autoscaler <span class="nt">-n</span> kube-system <span class="nt">-ojson</span> | <span class="se">\</span>
	jq <span class="nt">-r</span> <span class="s1">'.metadata.annotations | ."eks.amazonaws.com/role-arn"'</span>
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>arn:aws:iam::365499461711:role/eksctl-my-cluster-addon-iamserviceaccount-...
</code></pre></div></div>

<p>Check from the AWS Console if the above role contains the policy that we created
earlier.</p>

<h3 id="deploy-cluster-autoscaler">Deploy Cluster Autoscaler</h3>

<p>First, find the most recent Cluster Autoscaler version that has the same MAJOR
and MINOR version as the kubernetes cluster you’re deploying to.</p>

<p>Get the kube cluster’s version:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl version <span class="nt">-ojson</span> | jq <span class="nt">-r</span> .serverVersion.gitVersion
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v1.27.4-eks-2d98532
</code></pre></div></div>

<p>Choose the appropriate version for CAS. You can get the latest Cluster
Autoscaler versions from its <a href="https://github.com/kubernetes/autoscaler/releases?q=cluster-autoscaler+1&amp;expanded=true">Github Releases
Page</a>.</p>

<p>Example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">CLUSTER_AUTOSCALER_VERSION</span><span class="o">=</span>1.27.3
</code></pre></div></div>

<p>Next, deploy the cluster autoscaler using the deployment template that I
prepared in the <a href="https://github.com/relaxdiego/kubevirt-cas-baremetal">companion
repo</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>envsubst &lt; &lt;<span class="o">(</span>curl https://raw.githubusercontent.com/relaxdiego/kubevirt-cas-baremetal/main/cas-deployment.yaml.template<span class="o">)</span> | <span class="se">\</span>
  kubectl apply <span class="nt">-f</span> -
</code></pre></div></div>

<p>Check the cluster autoscaler status:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get deploy,pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>cluster-autoscaler <span class="nt">-n</span> kube-system
</code></pre></div></div>

<p>Example output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cluster-autoscaler   1/1     1            1           4m1s

NAME                                      READY   STATUS    RESTARTS   AGE
pod/cluster-autoscaler-6c58bd6d89-v8wbn   1/1     Running   0          60s
</code></pre></div></div>

<p>Tail the <code class="language-plaintext highlighter-rouge">cluster-autoscaler</code> pod’s logs to see what’s happening:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> kube-system logs <span class="nt">-f</span> deployment.apps/cluster-autoscaler
</code></pre></div></div>

<p>Below are example log entries from Cluster Autoscaler terminating an unneeded
node:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>node ip-XXXX.YYYY.compute.internal may be removed
...
ip-XXXX.YYYY.compute.internal was unneeded for 1m3.743475455s
</code></pre></div></div>

<p>Once the timeout has been reached (default: 10 minutes), CAS will scale down
the group:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scale-down: removing empty node ip-XXXX.YYYY.compute.internal
Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"kube-system", ...
Successfully added ToBeDeletedTaint on node ip-XXXX.YYYY.compute.internal
Terminating EC2 instance: i-ZZZZ
DeleteInstances was called: ...
</code></pre></div></div>

<blockquote>
  <p>For more information on how Cluster Autoscaler scales down a node group, see
<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work">How does scale-down
work?</a>
from the project’s FAQ.</p>
</blockquote>

<p>When you try to get the list of nodes, you should see the bare metal nodes
tainted such that they are no longer schedulable:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME       STATUS                     ROLES    AGE    VERSION
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready,SchedulingDisabled   &lt;none&gt;   70m    v1.27.3-eks-a5565ad
ip-XXXX    Ready                      &lt;none&gt;   112m   v1.27.3-eks-a5565ad
ip-XXXX    Ready                      &lt;none&gt;   112m   v1.27.3-eks-a5565ad
</code></pre></div></div>

<p>In a few more minutes, the nodes will be deleted.</p>

<p>To try the scale up, just deploy a VM.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Expanding Node Group eks-ng-eacf8ebb ...
Best option to resize: eks-ng-eacf8ebb
Estimated 1 nodes needed in eks-ng-eacf8ebb
Final scale-up plan: [{eks-ng-eacf8ebb 0-&gt;1 (max: 3)}]
Scale-up: setting group eks-ng-eacf8ebb size to 1
Setting asg eks-ng-eacf8ebb size to 1
</code></pre></div></div>

<h2 id="done">Done</h2>

<p>At this point you should have a working, auto-scaling EKS cluster that can host
VMs on bare metal nodes. If you have any questions, ask them
<a href="https://github.com/relaxdiego/relaxdiego.github.com/discussions/new?category=general">here</a>.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html">Amazon EKS Autoscaling</a></li>
  <li><a href="https://aws.plainenglish.io/cluster-autoscaler-amazon-eks-7ffaa24e5938">Cluster Autoscaler in Plain English</a></li>
  <li><a href="https://aws.github.io/aws-eks-best-practices/">AWS EKS Best Practices Guide</a></li>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html">IAM roles for service accounts</a></li>
  <li><a href="https://eksctl.io/usage/iamserviceaccounts/">eksctl create iamserviceaccount</a></li>
</ul>]]></content><author><name>Mark Maglana, Jonathan Kinred, Paul Myjavec</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="Cluster Autoscaler" /><category term="AWS" /><category term="EKS" /><summary type="html"><![CDATA[This post explains how to set up KubeVirt with Cluster Autoscaler on EKS]]></summary></entry></feed>