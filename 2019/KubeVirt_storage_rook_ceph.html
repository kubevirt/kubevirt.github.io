<!doctype html>
<html lang="en">

  <head>
    <!-- Adding Adobe Analytics -->
    <script id="dpal" src="//www.redhat.com/ma/dpal.js" type="text/javascript"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1, shrink-to-fit=no" >
    <meta name="go-import" content="kubevirt.io/kubevirt git https://github.com/kubevirt/kubevirt">
    <meta name="go-import" content="kubevirt.io/client-go git https://github.com/kubevirt/client-go">
    <meta name="go-import" content="kubevirt.io/containerized-data-importer git https://github.com/kubevirt/containerized-data-importer">
    <meta name="go-import" content="kubevirt.io/hostpath-provisioner git https://github.com/kubevirt/hostpath-provisioner">
    <meta name="go-import" content="kubevirt.io/hostpath-provisioner-operator git https://github.com/kubevirt/hostpath-provisioner-operator">
    <meta name="go-import" content="kubevirt.io/qe-tools git https://github.com/kubevirt/qe-tools">
    <meta name="go-import" content="kubevirt.io/machine-remediation git https://github.com/kubevirt/machine-remediation">
    <meta name="go-import" content="kubevirt.io/cloud-provider-kubevirt git https://github.com/kubevirt/cloud-provider-kubevirt">
    <meta name="go-import" content="kubevirt.io/controller-lifecycle-operator-sdk git https://github.com/kubevirt/controller-lifecycle-operator-sdk">
    <meta name="go-import" content="kubevirt.io/ssp-operator git https://github.com/kubevirt/ssp-operator">
    <meta name="go-import" content="kubevirt.io/cpu-nfd-plugin git https://github.com/kubevirt/cpu-nfd-plugin">
    <meta name="go-import" content="kubevirt.io/containerized-data-importer-api git https://github.com/kubevirt/containerized-data-importer-api">
    <meta name="go-import" content="kubevirt.io/api git https://github.com/kubevirt/api">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="/assets/favicon/site.webmanifest">
    <link rel="mask-icon" href="/assets/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#00aba9">
    <meta name="theme-color" content="#ffffff">
    <meta name="google-site-verification" content="eaETLLM6xObn1li9l9eU8lNIBgBpU0OQLXV1faU1svE" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/css/bootstrap.min.css" integrity="sha384-9gVQ4dYFwwWSjIDZnLEWnxCjeSWFphJiwGPXr1jddIhOegiu1FwO5qRGvFXOdJZ4" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="https://kubevirt.io//2019/KubeVirt_storage_rook_ceph.html">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700" rel="stylesheet">
    
    <title>Persistent storage of your Virtual Machines in KubeVirt with Rook | KubeVirt.io</title>
    <!-- # Opengraph protocol properties: https://ogp.me/ -->
    <meta name="author" content="Pedro IbÃ¡Ã±ez Requena" >
    <meta property="og:type" content="article" >
    <meta name="twitter:card" content="summary">
    <meta name="description" content="Persistent storage of your Virtual Machines in KubeVirt with Rook">
    <meta name="keywords" content="rook, ceph, ntp, chronyd" >
    <meta property="og:title" content="Persistent storage of your Virtual Machines in KubeVirt with Rook | KubeVirt.io">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://kubevirt.io//2019/KubeVirt_storage_rook_ceph.html" >
    <meta property="og:image" content="https://kubevirt.io//assets/images/KubeVirt_logo_color.png">
    <meta property="og:description" content="Persistent storage of your Virtual Machines in KubeVirt with Rook" >
    <meta property="og:site_name" content="KubeVirt.io" >
    <meta property="og:article:author" content="Pedro IbÃ¡Ã±ez Requena" >
    <meta property="og:article:published_time" content="2019-10-30 00:00:00 +0000" >
    <meta name="twitter:title" content="Persistent storage of your Virtual Machines in KubeVirt with Rook | KubeVirt.io">
    <meta name="twitter:description" content="Persistent storage of your Virtual Machines in KubeVirt with Rook">

    <link type="application/atom+xml" rel="alternate" href="https://kubevirt.io//feed.xml" title="KubeVirt.io" />
    <script src="//code.jquery.com/jquery.min.js"></script>
    
    <!-- Photoswipe.com gallery-->

    <!-- Core CSS file -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">

    <!-- Skin CSS file (styling of UI - buttons, caption, etc.)
        In the folder of skin CSS file there are also:
        - .png and .svg icons sprite,
        - preloader.gif (for browsers that do not support CSS animations) -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">
</head>


  <body>
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" role="navigation">
        <a class="navbar-brand" href="/">
    <img src="/assets/images/KubeVirt_logo_color.svg" class="navbar-brand-image d-inline-block align-top" alt="KubeVirt.io">
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-th-large"></i>
  </button>
  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav">
      

      
        <li  class="nav-item active" >
          <a class="nav-link text-uppercase" href="/blogs/">Blogs</a>
        </li>
      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="/videos/">Videos</a>
        </li>
      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="/gallery/">Gallery</a>
        </li>
      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="//kubevirt.io/user-guide">Docs</a>
        </li>
      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="/labs/">Labs</a>
        </li>
      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="/community/">Community</a>
        </li>
      

      <li class='nav-item'>
        <form action="/search.html" class="nav-item__search" method="get" autocomplete="off">
          <div class="autocomplete" style="width:150px;">
            <input type="text" id="search-input" class="docs-search--input" placeholder="search term" name="query">
          </div>
          <input id="search-button" type="submit" value="ðŸ”" disabled='true'>
        </form>
      </li>

    </ul>
  </div>
<script>
function autocomplete(inp, arr) {
  /*the autocomplete function takes two arguments,
  the text field element and an array of possible autocompleted values:*/
  var currentFocus;
  /*execute a function when someone writes in the text field:*/
  inp.addEventListener("input", function(e) {
      var a, b, i, val = this.value;
      /*close any already open lists of autocompleted values*/
      closeAllLists();
      if (!val) { return false;}
      currentFocus = -1;
      /*create a DIV element that will contain the items (values):*/
      a = document.createElement("DIV");
      a.setAttribute("id", this.id + "autocomplete-list");
      a.setAttribute("class", "autocomplete-items");
      /*append the DIV element as a child of the autocomplete container:*/
      this.parentNode.appendChild(a);
      /*for each item in the array...*/
      for (i = 0; i < arr.length; i++) {
        /*check if the item starts with the same letters as the text field value:*/
        if (arr[i].substr(0, val.length).toUpperCase() == val.toUpperCase()) {
          /*create a DIV element for each matching element:*/
          b = document.createElement("DIV");
          /*make the matching letters bold:*/
          b.innerHTML = "<strong>" + arr[i].substr(0, val.length) + "</strong>";
          b.innerHTML += arr[i].substr(val.length);
          /*insert a input field that will hold the current array item's value:*/
          b.innerHTML += "<input type='hidden' value='" + arr[i] + "'>";
          /*execute a function when someone clicks on the item value (DIV element):*/
              b.addEventListener("click", function(e) {
              /*insert the value for the autocomplete text field:*/
              inp.value = this.getElementsByTagName("input")[0].value;
              /*close the list of autocompleted values,
              (or any other open lists of autocompleted values:*/
              closeAllLists();
          });
          a.appendChild(b);
        }
      }
  });
  /*execute a function presses a key on the keyboard:*/
  inp.addEventListener("keydown", function(e) {
      document.getElementById("search-button").disabled= undefined;
      var x = document.getElementById(this.id + "autocomplete-list");
      if (x) x = x.getElementsByTagName("div");
      if (e.keyCode == 40) {
        /*If the arrow DOWN key is pressed,
        increase the currentFocus variable:*/
        currentFocus++;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 38) { //up
        /*If the arrow UP key is pressed,
        decrease the currentFocus variable:*/
        currentFocus--;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 13) {
        /*If the ENTER key is pressed, prevent the form from being submitted,*/
        if (currentFocus > -1) {
          /*and simulate a click on the "active" item:*/
          if (x) {
            x[currentFocus].click();
            e.preventDefault();
          }
        }
        if (document.getElementById("search-input").value == "") {
          e.preventDefault();
        }
      }
  });
  function addActive(x) {
    /*a function to classify an item as "active":*/
    if (!x) return false;
    /*start by removing the "active" class on all items:*/
    removeActive(x);
    if (currentFocus >= x.length) currentFocus = 0;
    if (currentFocus < 0) currentFocus = (x.length - 1);
    /*add class "autocomplete-active":*/
    x[currentFocus].classList.add("autocomplete-active");
  }
  function removeActive(x) {
    /*a function to remove the "active" class from all autocomplete items:*/
    for (var i = 0; i < x.length; i++) {
      x[i].classList.remove("autocomplete-active");
    }
  }
  function closeAllLists(elmnt) {
    /*close all autocomplete lists in the document,
    except the one passed as an argument:*/
    var x = document.getElementsByClassName("autocomplete-items");
    for (var i = 0; i < x.length; i++) {
      if (elmnt != x[i] && elmnt != inp) {
      x[i].parentNode.removeChild(x[i]);
    }
  }
}
/*execute a function when someone clicks in the document:*/
document.addEventListener("click", function (e) {
    closeAllLists(e.target);
});
}
</script>

<script>
var mykeywords = ["libvirt", "KubeVirt", "ClearContainers", "virtlet", "CRI", "OpenStack", "ovirt", "release notes", "changelog", "hilights", "network", "flannel", "kubevirt-ansible", "Skydive", "openshift", "glusterfs", "heketi", "virtual machine", "weavenet", "custom resources", "kubevirt objects", "objects", "VirtualMachine", "api", "rbac", "roles", "storage", "ovn", "kubetron", "neutron", "vscode", "development", "debug", "istio", "iptables", "tproxy", "service mesh", "ebtables", "docker", "container", "build", "multus", "roadmap", "kvm", "qemu", "device plugins", "unit testing", "review", "hugepages", "kubevirtci", "ci-cd", "cicd", "memory", "overcommitment", "networking", "CNI", "multiple networks", "ovs-cni", "import", "clone", "upload", "disk image", "cdi", "datavolumes", "volume types", "serviceaccount", "ignition", "coreos", "rhcos", "kubecon", "conference", "gcp", "autodeployer", "metrics", "prometheus", "grafana", "federation", "kubefed", "multicluster", "HCO", "hyperconverged operator", "ansible", "vagrant", "lifecycle", "virtual machines", "website", "community", "vm import", "node drain", "eviction", "nmo", "condition types", "Condition types", "CNCF", "sandbox", "lab", "cri-o", "quickstart", "homelab", "kubernetes", "kubevirt installation", "rook", "ceph", "ntp", "chronyd", "prow", "infrastructure", "kubevirt-tutorial", "CI-CD", "continuous integration", "jenkins", "noVNC", "console", "KubeCon", "cloudnativecon", "America", "talk", "gathering", "contra-lib", "admin", "operations", "create vm", "start vm", "connect to console", "connect to ssh", "stop vm", "remove vm", "operator manual", "basic operations", "laboratory", "installing kubevirt", "use kubevirt", "admin operations", "CDI", "containerized data importer", "octant", "okd", "openshift console", "cockpit", "user interface", "web interface", "virtVNC", "OKD console", "kubevirt upgrade", "upgrading", "OpenShift web console", "OKD", "video", "virtual machine management", "NUMA", "CPU pinning", "QEMU", "KVM", "GPU", "NVIDIA", "GPU workloads", "pass-through", "passthrough", "kubevirt", "Microsoft Windows kubernetes", "Microsoft Windows container", "Windows", "VM", "Advanced VM scheduling", "affinity", "scheduling", "topologyKeys", "Live Migration", "design", "architecture", "security", "operation", "images", "Kubernetes", "windows", "common-templates", "minikube", "addons", "oVirt", "kubevirt-hyperconverged", "cnao", "cluster-network-addons-operator", "kubernetes-nmstate", "nmstate", "bridge", "containerDisk", "registry", "composer-cli", "virt-customize", "builder tool", "prometheus-operator", "node-exporter", "monitoring", "event", "Tekton Pipelines", "KubeVirt Tekton Tasks", "vGPU", "Intel", "Fedora", "go", "authentication", "mesh", "AWS", "EC2", "AMI", "real-time", "CPUManager", ]
autocomplete(document.getElementById("search-input"), mykeywords);
</script>

<script src="/js/clipboard.min.js"></script>

    </nav>

    <main role="main" style="margin-top: 60px;">
      <div class="container">
  <div class="row">
    <div class="col">
      <div class="post">
        <header class="post-header">
          <h1></h1>
          <h1 class="post-title">Persistent storage of your Virtual Machines in KubeVirt with Rook</h1>
          <div class="post-info">
            <span class="post-author">Author: Pedro IbÃ¡Ã±ez Requena</span>
            <div>
              <span class="post-category-name">
                Tags: <a href="/tag/rook">rook</a>&nbsp;|&nbsp;<a href="/tag/ceph">ceph</a>&nbsp;|&nbsp;<a href="/tag/ntp">ntp</a>&nbsp;|&nbsp;<a href="/tag/chronyd">chronyd</a>
              </span>
            </div>
            <div>
              <span class="post-meta">Publication Date: October 30, 2019  </span>
            </div>
            <div>
              <span class="post-category-name">
                Category: news
              </span>
            </div>

          </div>
        </header>
        <article class="post-content">
          <p><img src="/assets/2019-10-30-KubeVirt_storage_rook_ceph/rook-stacked-color_250.png" alt="Rook" title="Rook" />
<img src="/assets/2019-10-30-KubeVirt_storage_rook_ceph/Ceph_Logo_Standard_RGB_120411_fa_250.png" alt="Ceph" title="Ceph" /></p>

<h2 id="introduction">Introduction</h2>

<p>Quoting <a href="https://en.wikipedia.org/wiki/Persistence_(computer_science)">Wikipedia</a>:</p>

<blockquote>
  <p>In computer science, persistence refers to the characteristic of state that outlives the process
that created it. This is achieved in practice by storing the state as data in computer data storage.
Programs have to transfer data to and from storage devices and have to provide mappings from the
native programming-language data structures to the storage device data structures.</p>
</blockquote>

<p>In this post, we are going to show how to set up a persistence system to store VM images with the help of <a href="https://ceph.io/en/">Ceph</a> and the automation of <a href="https://rook.io">Rook</a>.</p>

<h2 id="pre-requisites">Pre-requisites</h2>

<p>Some prerequisites have to be met:</p>

<ul>
  <li>An existent Kubernetes cluster with 3 masters and 1 worker (min) is already set up, itâ€™s not mandatory, but allows to demonstrate an example of a HA Ceph installation.</li>
  <li>Each Kubernetes node has an extra empty disk connected (has to be blank with no filesystem).</li>
  <li>KubeVirt is already installed and running.</li>
</ul>

<p>In this example the following systems names and IP addresses are used:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">System</th>
      <th style="text-align: center">Purpose</th>
      <th style="text-align: center">IP</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">kv-master-00</td>
      <td style="text-align: center">Kubernetes Master node 00</td>
      <td style="text-align: center">192.168.122.6</td>
    </tr>
    <tr>
      <td style="text-align: center">kv-master-01</td>
      <td style="text-align: center">Kubernetes Master node 01</td>
      <td style="text-align: center">192.168.122.106</td>
    </tr>
    <tr>
      <td style="text-align: center">kv-master-02</td>
      <td style="text-align: center">Kubernetes Master node 02</td>
      <td style="text-align: center">192.168.122.206</td>
    </tr>
    <tr>
      <td style="text-align: center">kv-worker-00</td>
      <td style="text-align: center">Kubernetes Worker node 00</td>
      <td style="text-align: center">192.168.122.222</td>
    </tr>
  </tbody>
</table>

<p>For being able to import Virtual Machines, the KubeVirt CDI has to be configured too.</p>

<blockquote>
  <p>Containerized-Data-Importer (CDI) is a persistent storage management add-on for Kubernetes. Its primary goal is to provide a declarative way to build Virtual Machine Disks on PVCs for KubeVirt VMs.
CDI works with standard core Kubernetes resources and is storage device-agnostic, while its primary focus is to build disk images for Kubevirt, itâ€™s also useful outside of a KubeVirt context to use for initializing your Kubernetes Volumes with data.</p>
</blockquote>

<p>In the case your cluster doesnâ€™t have CDI, the following commands will cover CDI operator and the CR setup:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# <span class="nb">export </span><span class="nv">VERSION</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> https://github.com/kubevirt/containerized-data-importer/releases/latest | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"v[0-9]</span><span class="se">\+\.</span><span class="s2">[0-9]*</span><span class="se">\.</span><span class="s2">[0-9]*"</span><span class="si">)</span>

<span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> https://github.com/kubevirt/containerized-data-importer/releases/download/<span class="nv">$VERSION</span>/cdi-operator.yaml
namespace/cdi created
customresourcedefinition.apiextensions.k8s.io/cdis.cdi.kubevirt.io created
configmap/cdi-operator-leader-election-helper created
serviceaccount/cdi-operator created
clusterrole.rbac.authorization.k8s.io/cdi-operator-cluster created
clusterrolebinding.rbac.authorization.k8s.io/cdi-operator created
deployment.apps/cdi-operator created
containerized-data-importer/releases/download/<span class="nv">$VERSION</span>/cdi-operator.yaml

<span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> https://github.com/kubevirt/containerized-data-importer/releases/download/<span class="nv">$VERSION</span>/cdi-cr.yaml
cdi.cdi.kubevirt.io/cdi created

</code></pre></div></div>

<div class="premonition warning"><div class="fa fa-exclamation-circle"></div><div class="content"><p class="header">The nodes of the cluster have to be time synchronized</p><p>This should have been done for you by <code class="language-plaintext highlighter-rouge">chronyd</code></p>


</div></div>
<p>It canâ€™t harm to do it again:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# <span class="k">for </span>i <span class="k">in</span> <span class="si">$(</span><span class="nb">echo </span>6 106 206 222<span class="si">)</span><span class="p">;</span> <span class="k">do </span>ssh <span class="nt">-oStrictHostKeyChecking</span><span class="o">=</span>no <span class="se">\</span>
    root@192.168.122.<span class="nv">$i</span> <span class="nb">sudo </span>chronyc <span class="nt">-a</span> makestep<span class="p">;</span> <span class="k">done

</span>Warning: Permanently added <span class="s1">'192.168.122.6'</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
200 OK
Warning: Permanently added <span class="s1">'192.168.122.106'</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
200 OK
Warning: Permanently added <span class="s1">'192.168.122.206'</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
200 OK
Warning: Permanently added <span class="s1">'192.168.122.222'</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
200 OK
</code></pre></div></div>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p>This step could also be done with ansible (one line or rhel-system-roles.noarch).</p>


</div></div>
<h2 id="installing-rook-in-kubernetes-to-handle-the-ceph-cluster">Installing Rook in Kubernetes to handle the Ceph cluster</h2>

<p>Next, the latest upstream release of Rook has to be cloned:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# git clone https://github.com/rook/rook
Cloning into <span class="s1">'rook'</span>...
remote: Enumerating objects: 1, <span class="k">done</span><span class="nb">.</span>
remote: Counting objects: 100% <span class="o">(</span>1/1<span class="o">)</span>, <span class="k">done</span><span class="nb">.</span>
remote: Total 37745 <span class="o">(</span>delta 0<span class="o">)</span>, reused 0 <span class="o">(</span>delta 0<span class="o">)</span>, pack-reused 37744
Receiving objects: 100% <span class="o">(</span>37745/37745<span class="o">)</span>, 13.02 MiB | 1.54 MiB/s, <span class="k">done</span><span class="nb">.</span>
Resolving deltas: 100% <span class="o">(</span>25309/25309<span class="o">)</span>, <span class="k">done</span><span class="nb">.</span>
</code></pre></div></div>

<p>Now, change the actual directory to the location of the Kubernetes examples where the respective resource definitions can be found:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# <span class="nb">cd </span>rook/cluster/examples/kubernetes/ceph
</code></pre></div></div>

<p>The Rook common resources that make up Rook have to be created:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> common.yaml
<span class="o">(</span>output removed<span class="o">)</span>
</code></pre></div></div>

<p>Next, create the Kubernetes Rook operator:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> operator.yaml
deployment.apps/rook-ceph-operator created
</code></pre></div></div>

<p>To check the progress of the operator pod and the discovery pods starting up, the commands below can be executed. The discovery pods are responsible for investigating the available resources (e.g. disks that can make up OSDâ€™s) across all available Nodes:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# watch kubectl get pods <span class="nt">-n</span> rook-ceph
NAME                                 READY   STATUS             RESTARTS   AGE
rook-ceph-operator-fdfbcc5c5-qs7x8   1/1     Running            1          3m14s
rook-discover-7v65m                  1/1     Running            2          2m19s
rook-discover-cjfdz                  1/1     Running            0          2m19s
rook-discover-f8k4s                  0/1     ImagePullBackOff   0          2m19s
rook-discover-x22hh                  1/1     Running            0          2m19s

NAME                                     READY   STATUS    RESTARTS   AGE
pod/rook-ceph-operator-fdfbcc5c5-qs7x8   1/1     Running   1          4m21s
pod/rook-discover-7v65m                  1/1     Running   2          3m26s
pod/rook-discover-cjfdz                  1/1     Running   0          3m26s
pod/rook-discover-f8k4s                  1/1     Running   0          3m26s
pod/rook-discover-x22hh                  1/1     Running   0          3m26s
</code></pre></div></div>

<p>After, the Ceph cluster configuration inside of the Rook operator has to be prepared:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> cluster.yaml
cephcluster.ceph.rook.io/rook-ceph created
</code></pre></div></div>

<p>One of the key elements of the default cluster configuration is to configure the Ceph cluster to use all nodes and use all devices, i.e. run Rook/Ceph on every system and consume any free disks that it finds; this makes configuring Rook a lot more simple:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# <span class="nb">grep </span>useAll cluster.yml
    useAllNodes: <span class="nb">true
    </span>useAllDevices: <span class="nb">true</span>
    <span class="c"># Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named</span>
</code></pre></div></div>

<p>The progress can be checked now, check the pods in the <code class="language-plaintext highlighter-rouge">rook-ceph</code> namespace:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# watch kubectl <span class="nt">-n</span> rook-ceph get pods
NAME                                            READY   STATUS              RESTARTS   AGE
csi-cephfsplugin-2kqbd                          3/3     Running             0          36s
csi-cephfsplugin-hjnf9                          3/3     Running             0          36s
csi-cephfsplugin-provisioner-75c965db4f-tbgfn   4/4     Running             0          36s
csi-cephfsplugin-provisioner-75c965db4f-vgcwv   4/4     Running             0          36s
csi-cephfsplugin-svcjk                          3/3     Running             0          36s
csi-cephfsplugin-tv6rs                          3/3     Running             0          36s
csi-rbdplugin-dsdwk                             3/3     Running             0          37s
csi-rbdplugin-provisioner-69c9869dc9-bwjv4      5/5     Running             0          37s
csi-rbdplugin-provisioner-69c9869dc9-vzzp9      5/5     Running             0          37s
csi-rbdplugin-vzhzz                             3/3     Running             0          37s
csi-rbdplugin-w5n6x                             3/3     Running             0          37s
csi-rbdplugin-zxjcc                             3/3     Running             0          37s
rook-ceph-mon-a-canary-84c7fc67ff-pf7t5         1/1     Running             0          14s
rook-ceph-mon-b-canary-5f7c7cfbf4-8dvcp         1/1     Running             0          8s
rook-ceph-mon-c-canary-7779478497-7x25x         0/1     ContainerCreating   0          3s
rook-ceph-operator-fdfbcc5c5-qs7x8              1/1     Running             1          9m30s
rook-discover-7v65m                             1/1     Running             2          8m35s
rook-discover-cjfdz                             1/1     Running             0          8m35s
rook-discover-f8k4s                             1/1     Running             0          8m35s
rook-discover-x22hh                             1/1     Running             0          8m35s

</code></pre></div></div>

<p>Wait until the Ceph monitor pods are created. Next, the toolbox pod has to be created; this is useful to verify the status/health of the cluster, getting/setting authentication, and querying the Ceph cluster using standard Ceph tools:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> toolbox.yaml
deployment.apps/rook-ceph-tools created
</code></pre></div></div>

<p>To check how well this is progressing:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph get pods | <span class="nb">grep </span>tool
rook-ceph-tools-856c5bc6b4-s47qm                       1/1     Running   0          31s
</code></pre></div></div>

<p>Before proceeding with the pool and the storage class the Ceph cluster status can be checked already:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# <span class="nv">toolbox</span><span class="o">=</span><span class="si">$(</span>kubectl <span class="nt">-n</span> rook-ceph get pods <span class="nt">-o</span> custom-columns<span class="o">=</span>NAME:.metadata.name <span class="nt">--no-headers</span> | <span class="nb">grep </span>tools<span class="si">)</span>

<span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$toolbox</span> sh
sh-4.2# ceph status
  cluster:
    <span class="nb">id</span>:     5a0bbe74-ce42-4f49-813d-7c434af65aad
    health: HEALTH_WARN
            clock skew detected on mon.c

  services:
    mon: 3 daemons, quorum a,b,c <span class="o">(</span>age 3m<span class="o">)</span>
    mgr: a<span class="o">(</span>active, since 2m<span class="o">)</span>
    osd: 4 osds: 4 up <span class="o">(</span>since 105s<span class="o">)</span>, 4 <span class="k">in</span> <span class="o">(</span>since 105s<span class="o">)</span>

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   4.0 GiB used, 72 GiB / 76 GiB avail
    pgs:
</code></pre></div></div>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>In this example, the health value is <code class="language-plaintext highlighter-rouge">HEALTH_WARN</code> because there is a clock skew between the monitor in node c and the rest of the cluster. If this is your case, go to the troubleshooting point at the end of the blogpost to find out how to solve this issue and get a <code class="language-plaintext highlighter-rouge">HEALTH_OK</code>.</p>


</div></div>
<p>Next, some other resources need to be created. First, the block pool that defines the name (and specification) of the RBD pool that will be used for creating persistent volumes, in this case, is called <code class="language-plaintext highlighter-rouge">replicapool</code>:</p>

<h2 id="configuring-the-cephblockpool-and-the-kubernetes-storageclass-for-using-ceph-hosting-the-virtual-machines">Configuring the CephBlockPool and the Kubernetes StorageClass for using Ceph hosting the Virtual Machines</h2>

<p>The <code class="language-plaintext highlighter-rouge">cephblockpool.yml</code> is based in the <code class="language-plaintext highlighter-rouge">pool.yml</code>, you can check that file in the same directory to learn about the details of each parameter:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# <span class="nb">cat </span>pool.yml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#################################################################################################################</span>
<span class="c1"># Create a Ceph pool with settings for replication in production environments. A minimum of 3 OSDs on</span>
<span class="c1"># different hosts are required in this example.</span>
<span class="c1">#  kubectl create -f pool.yaml</span>
<span class="c1">#################################################################################################################</span>

<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ceph.rook.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">CephBlockPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">replicapool</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="c1"># The failure domain will spread the replicas of the data across different failure zones</span>
  <span class="na">failureDomain</span><span class="pi">:</span> <span class="s">host</span>
  <span class="c1"># For a pool based on raw copies, specify the number of copies. A size of 1 indicates no redundancy.</span>
  <span class="na">replicated</span><span class="pi">:</span>
    <span class="na">size</span><span class="pi">:</span> <span class="m">3</span>
  <span class="c1"># A key/value list of annotations</span>
  <span class="na">annotations</span><span class="pi">:</span>
  <span class="c1">#  key: value</span>
</code></pre></div></div>

<p>The following file has to be created to define the CephBlockPool:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# vim cephblockpool.yml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ceph.rook.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">CephBlockPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">replicapool</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">failureDomain</span><span class="pi">:</span> <span class="s">host</span>
  <span class="na">replicated</span><span class="pi">:</span>
    <span class="na">size</span><span class="pi">:</span> <span class="m">2</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> cephblockpool.yml
cephblockpool.ceph.rook.io/replicapool created

<span class="o">[</span>root@kv-master-00 ~]# kubectl get cephblockpool <span class="nt">-n</span> rook-ceph
NAME          AGE
replicapool   19s
</code></pre></div></div>

<p>Now is time to create the Kubernetes storage class that would be used to create the volumes later:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# vim storageclass.yml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">rook-ceph-block</span>
<span class="c1"># Change "rook-ceph" provisioner prefix to match the operator namespace if needed</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">rook-ceph.rbd.csi.ceph.com</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="c1"># clusterID is the namespace where the rook cluster is running</span>
  <span class="na">clusterID</span><span class="pi">:</span> <span class="s">rook-ceph</span>
  <span class="c1"># Ceph pool into which the RBD image shall be created</span>
  <span class="na">pool</span><span class="pi">:</span> <span class="s">replicapool</span>

  <span class="c1"># RBD image format. Defaults to "2".</span>
  <span class="na">imageFormat</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2"</span>

  <span class="c1"># RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.</span>
  <span class="na">imageFeatures</span><span class="pi">:</span> <span class="s">layering</span>

  <span class="c1"># The secrets contain Ceph admin credentials.</span>
  <span class="na">csi.storage.k8s.io/provisioner-secret-name</span><span class="pi">:</span> <span class="s">rook-ceph-csi</span>
  <span class="na">csi.storage.k8s.io/provisioner-secret-namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
  <span class="na">csi.storage.k8s.io/node-stage-secret-name</span><span class="pi">:</span> <span class="s">rook-ceph-csi</span>
  <span class="na">csi.storage.k8s.io/node-stage-secret-namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>

  <span class="c1"># Specify the filesystem type of the volume. If not specified, csi-provisioner</span>
  <span class="c1"># will set default as `ext4`.</span>
  <span class="na">csi.storage.k8s.io/fstype</span><span class="pi">:</span> <span class="s">xfs</span>

<span class="c1"># Delete the rbd volume when a PVC is deleted</span>
<span class="na">reclaimPolicy</span><span class="pi">:</span> <span class="s">Delete</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> storageclass.yml
storageclass.storage.k8s.io/rook-ceph-block created

<span class="o">[</span>root@kv-master-00 ~]# kubectl get storageclass
NAME              PROVISIONER                  AGE
rook-ceph-block   rook-ceph.rbd.csi.ceph.com   61s
</code></pre></div></div>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p>Special attention to the pool name, it has to be the same as configured in the CephBlockPool.</p>


</div></div>
<p>Now, simply wait for the Ceph OSDâ€™s to finish provisioning and weâ€™ll be done with our Ceph deployment:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# watch  <span class="s2">"kubectl -n rook-ceph get pods | grep rook-ceph-osd-prepare"</span>
rook-ceph-osd-prepare-kv-master-00.kubevirt-io-4npmf   0/1     Completed   0          20m
rook-ceph-osd-prepare-kv-master-01.kubevirt-io-69smd   0/1     Completed   0          20m
rook-ceph-osd-prepare-kv-master-02.kubevirt-io-zm7c2   0/1     Completed   0          20m
rook-ceph-osd-prepare-kv-worker-00.kubevirt-io-5qmjg   0/1     Completed   0          20m

</code></pre></div></div>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p>This process may take a few minutes as it has to zap the disks, deploy a BlueStore configuration on them, and start the OSD service pods across our nodes.</p>


</div></div>
<p>The cluster deployment can be validated now:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$toolbox</span> sh
sh-4.2# ceph <span class="nt">-s</span>
  cluster:
    <span class="nb">id</span>:     5a0bbe74-ce42-4f49-813d-7c434af65aad
    health: HEALTH_WARN
            too few PGs per OSD <span class="o">(</span>4 &lt; min 30<span class="o">)</span>

  services:
    mon: 3 daemons, quorum a,b,c <span class="o">(</span>age 12m<span class="o">)</span>
    mgr: a<span class="o">(</span>active, since 21m<span class="o">)</span>
    osd: 4 osds: 4 up <span class="o">(</span>since 20m<span class="o">)</span>, 4 <span class="k">in</span> <span class="o">(</span>since 20m<span class="o">)</span>

  data:
    pools:   1 pools, 8 pgs
    objects: 0 objects, 0 B
    usage:   4.0 GiB used, 72 GiB / 76 GiB avail
    pgs:     8 active+clean
</code></pre></div></div>

<p>Oh Wait! the health value is again <code class="language-plaintext highlighter-rouge">HEALTH_WARN</code>, no problem! it is because there are too few PGs per OSD, in this case 4, for a minimum value of 30. Letâ€™s fix it changing that value to 256:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$toolbox</span> sh
sh-4.2# ceph osd pool <span class="nb">set </span>replicapool pg_num 256
<span class="nb">set </span>pool 1 pg_num to 256

sh-4.2# ceph <span class="nt">-s</span>
  cluster:
    <span class="nb">id</span>:     5a0bbe74-ce42-4f49-813d-7c434af65aad
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c <span class="o">(</span>age 18m<span class="o">)</span>
    mgr: a<span class="o">(</span>active, since 27m<span class="o">)</span>
    osd: 4 osds: 4 up <span class="o">(</span>since 26m<span class="o">)</span>, 4 <span class="k">in</span> <span class="o">(</span>since 26m<span class="o">)</span>

  data:
    pools:   1 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   4.0 GiB used, 72 GiB / 76 GiB avail
    pgs:     12.109% pgs unknown
             0.391% pgs not active
             224 active+clean
             31  unknown
             1   peering

</code></pre></div></div>

<p>In a moment, Ceph will end peering and the status of the pgs would be <code class="language-plaintext highlighter-rouge">active+clean</code>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sh-4.2# ceph <span class="nt">-s</span>
  cluster:
    <span class="nb">id</span>:     5a0bbe74-ce42-4f49-813d-7c434af65aad
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c <span class="o">(</span>age 21m<span class="o">)</span>
    mgr: a<span class="o">(</span>active, since 29m<span class="o">)</span>
    osd: 4 osds: 4 up <span class="o">(</span>since 28m<span class="o">)</span>, 4 <span class="k">in</span> <span class="o">(</span>since 28m<span class="o">)</span>

  data:
    pools:   1 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   4.0 GiB used, 72 GiB / 76 GiB avail
    pgs:     256 active+clean
</code></pre></div></div>

<p>Some additional checks on the Ceph cluster can be performed:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sh-4.2# ceph osd tree
ID CLASS WEIGHT  TYPE NAME                         STATUS REWEIGHT PRI-AFF
<span class="nt">-1</span>       0.07434 root default
<span class="nt">-9</span>       0.01859     host kv-master-00-kubevirt-io
 3   hdd 0.01859         osd.3                         up  1.00000 1.00000
<span class="nt">-7</span>       0.01859     host kv-master-01-kubevirt-io
 2   hdd 0.01859         osd.2                         up  1.00000 1.00000
<span class="nt">-3</span>       0.01859     host kv-master-02-kubevirt-io
 0   hdd 0.01859         osd.0                         up  1.00000 1.00000
<span class="nt">-5</span>       0.01859     host kv-worker-00-kubevirt-io
 1   hdd 0.01859         osd.1                         up  1.00000 1.00000

sh-4.2# ceph osd status
+----+--------------------------+-------+-------+--------+---------+--------+---------+-----------+
| <span class="nb">id</span> |           host           |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
+----+--------------------------+-------+-------+--------+---------+--------+---------+-----------+
| 0  | kv-master-02.kubevirt-io | 1026M | 17.9G |    0   |     0   |    0   |     0   | exists,up |
| 1  | kv-worker-00.kubevirt-io | 1026M | 17.9G |    0   |     0   |    0   |     0   | exists,up |
| 2  | kv-master-01.kubevirt-io | 1026M | 17.9G |    0   |     0   |    0   |     0   | exists,up |
| 3  | kv-master-00.kubevirt-io | 1026M | 17.9G |    0   |     0   |    0   |     0   | exists,up |
+----+--------------------------+-------+-------+--------+---------+--------+---------+-----------+

</code></pre></div></div>

<p>That should match the available block devices in the nodes, letâ€™s check it in the <code class="language-plaintext highlighter-rouge">kv-master-00</code> node:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# lsblk
NAME                                                                                                 MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                                                                                                   11:0    1  512K  0 rom
vda                                                                                                  253:0    0   50G  0 disk
â””â”€vda1                                                                                               253:1    0   50G  0 part /
vdb                                                                                                  253:16   0   20G  0 disk
â””â”€ceph--09112f92--11cd--4284--b763--447065cc169c-osd--data--0102789c--852c--4696--96ce--54c2ad3a848b 252:0    0   19G  0 lvm
</code></pre></div></div>

<p>To validate that the pods are running on the correct nodes, check the <code class="language-plaintext highlighter-rouge">NODE</code> column below:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl get pods <span class="nt">-n</span> rook-ceph <span class="nt">-o</span> wide | egrep <span class="s1">'(NAME|osd)'</span>
NAME                                                   READY   STATUS      RESTARTS   AGE   IP                NODE                       NOMINATED NODE   READINESS GATES
rook-ceph-osd-0-8689c68c78-rgdbj                       1/1     Running     0          31m   10.244.2.9        kv-master-02.kubevirt-io   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-1-574cb85d9d-vs2jc                       1/1     Running     0          31m   10.244.3.18       kv-worker-00.kubevirt-io   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-2-65b54c458f-zkk6v                       1/1     Running     0          31m   10.244.1.10       kv-master-01.kubevirt-io   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-3-5fd97cd4c9-2xd6c                       1/1     Running     0          30m   10.244.0.10       kv-master-00.kubevirt-io   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-prepare-kv-master-00.kubevirt-io-4npmf   0/1     Completed   0          31m   10.244.0.9        kv-master-00.kubevirt-io   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-prepare-kv-master-01.kubevirt-io-69smd   0/1     Completed   0          31m   10.244.1.9        kv-master-01.kubevirt-io   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-prepare-kv-master-02.kubevirt-io-zm7c2   0/1     Completed   0          31m   10.244.2.8        kv-master-02.kubevirt-io   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-prepare-kv-worker-00.kubevirt-io-5qmjg   0/1     Completed   0          31m   10.244.3.17       kv-worker-00.kubevirt-io   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>

<p>All good!</p>

<p>For validating the storage provisioning through the new Ceph cluster managed by the Rook operator, a persistent volume claim (PVC) can be created:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# vim pvc.yml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pv-claim</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">rook-ceph-block</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ceph]# kubectl create <span class="nt">-f</span> pvc.yml
persistentvolumeclaim/pv-claim created
</code></pre></div></div>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p>Ensure that the <code class="language-plaintext highlighter-rouge">storageClassName</code> contains the name of the storage class you have created, in this case, <code class="language-plaintext highlighter-rouge">rook-ceph-block</code></p>


</div></div>
<p>For checking that it has been bound, list the PVCs and look for the ones in the <code class="language-plaintext highlighter-rouge">rook-ceph-block</code> storageclass:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
pv-claim   Bound    pvc-62a9738a-e027-4a68-9ecf-16278711ff64   1Gi        RWO            rook-ceph-block   63s
</code></pre></div></div>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p>If the volume is still in a â€˜Pendingâ€™ state, likely, that one of the pods havenâ€™t come up correctly or one of the steps above has been missed. To check it, the command â€˜kubectl get pods -n rook-cephâ€™ can be executed for viewing the running/failed pods.</p>


</div></div>
<p>Before proceeding letâ€™s clean up the temporary PVC:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl delete pvc pv-claim
persistentvolumeclaim <span class="s2">"pv-claim"</span> deleted
</code></pre></div></div>

<h2 id="creating-a-virtual-machine-in-kubevirt-backed-by-ceph">Creating a Virtual Machine in KubeVirt backed by Ceph</h2>

<p>Once the Ceph cluster is up and running, the first Virtual Machine can be created, to do so, a YML example file is being downloaded and modified:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# wget https://raw.githubusercontent.com/kubevirt/containerized-data-importer/master/manifests/example/vm-dv.yaml
<span class="o">[</span>root@kv-master-00 ~]# <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/hdd/rook-ceph-block/'</span> vm-dv.yaml
<span class="o">[</span>root@kv-master-00 ~]# <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/fedora/centos/'</span> vm-dv.yaml
<span class="o">[</span>root@kv-master-00 ~]# <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s@https://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img@http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2@'</span> vm-dv.yaml
<span class="o">[</span>root@kv-master-00 ~]# <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/storage: 100M/storage: 9G/'</span> vm-dv.yaml
<span class="o">[</span>root@kv-master-00 ~]# <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/memory: 64M/memory: 1G/'</span> vm-dv.yaml
</code></pre></div></div>

<p>The modified YAML could be run already like this but a user wonâ€™t be able to log in as we donâ€™t know the password used in that image. <code class="language-plaintext highlighter-rouge">cloud-init</code> can be used to change the password of the default user of that image <code class="language-plaintext highlighter-rouge">centos</code> and grant us access, two parts have to be added:</p>

<ul>
  <li>Add a second disk after the <code class="language-plaintext highlighter-rouge">datavolumevolume</code> (already existing), in this example is called <code class="language-plaintext highlighter-rouge">cloudint</code>:</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# vim vm-dv.yaml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">template</span><span class="pi">:</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm-datavolume</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">domain</span><span class="pi">:</span>
      <span class="na">devices</span><span class="pi">:</span>
        <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">datavolumevolume</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
</code></pre></div></div>

<ul>
  <li>Afterwards, add the volume at the end of the file, after the volume already defined as <code class="language-plaintext highlighter-rouge">datavolumevolume</code>, in this example itâ€™s also called <code class="language-plaintext highlighter-rouge">cloudinit</code>:</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# vim vm-dv.yaml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">dataVolume</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">centos-dv</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">datavolumevolume</span>
  <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
      <span class="na">userData</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">#cloud-config</span>
        <span class="s">password: changeme</span>
        <span class="s">chpasswd: { expire: False }</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
</code></pre></div></div>

<p>The password value (<code class="language-plaintext highlighter-rouge">changeme</code> in this example), can be set to your preferred one.</p>

<p>Once the YAML file is prepared the Virtual Machine can be created and started:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl create <span class="nt">-f</span> vm-dv.yaml
virtualmachine.kubevirt.io/vm-centos-datavolume created

<span class="o">[</span>root@kv-master-00 ~]# kubectl get vm
NAME                   AGE   RUNNING   VOLUME
vm-centos-datavolume              62m   <span class="nb">false</span>
</code></pre></div></div>

<p>Letâ€™s wait a little bit until the importer pod finishes, meanwhile you can check it with:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl get pods
NAME                       READY   STATUS              RESTARTS   AGE
importer-centos-dv-8v6l5   0/1     ContainerCreating   0          12s
</code></pre></div></div>

<p>Once that pods ends, the Virtual Machine can be started (in this case the virt parameter can be used because of the <a href="https://kubevirt.io/user-guide/operations/virtctl_client_tool/#retrieving-the-virtctl-client-tool">krew plugin system</a>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 tmp]# kubectl virt start vm-centos-datavolume
VM vm-centos-datavolume was scheduled to start

<span class="o">[</span>root@kv-master-00 ~]# kubectl get vmi
NAME                   AGE    PHASE     IP            NODENAME
vm-centos-datavolume   2m4s   Running   10.244.3.20   kv-worker-00.kubevirt-io

<span class="o">[</span>root@kv-master-00 ~]# kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
centos-dv   Bound    pvc-5604eb4a-21dd-4dca-8bb7-fbacb0791402   9Gi        RWO            rook-ceph-block   7m34s
</code></pre></div></div>

<p>Awesome! the Virtual Machine is running in a pod through KubeVirt and itâ€™s backed up with Ceph under the management of Rook. Now itâ€™s the time for grabbing a coffee to allow cloud-init to do its job. A little while later letâ€™s connect to that VM console:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl virt console vm-centos-datavolume
Successfully connected to vm-centos-datavolume console. The escape sequence is ^]

CentOS Linux 7 <span class="o">(</span>Core<span class="o">)</span>
Kernel 3.10.0-957.27.2.el7.x86_64 on an x86_64


vm-centos-datavolume login: centos
Password:
<span class="o">[</span>centos@vm-centos-datavolume ~]<span class="err">$</span>
</code></pre></div></div>

<p>And there it is! our Kubernetes cluster provided with virtualization capabilities thanks to KubeVirt and backed up with a strong Ceph cluster under the management of Rook.</p>

<h2 id="troubleshooting">Troubleshooting</h2>

<p>It might happen that once the Ceph cluster is created, the hosts are not properly time-synchronized, in that case, the Ceph configuration can be modified to allow a bigger time difference between the nodes, in this case, the variable <code class="language-plaintext highlighter-rouge">mon clock drift allowed</code> is changed to 0.5 seconds, the steps to do so are the following:</p>

<ul>
  <li>Connect to the toolbox pod to check the cluster status</li>
  <li>Modify the configMap with the Ceph cluster configuration</li>
  <li>Verify the changes</li>
  <li>Remove the mon pods to apply the new configuration</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$toolbox</span> sh
sh-4.2# ceph status
  cluster:
    <span class="nb">id</span>:     5a0bbe74-ce42-4f49-813d-7c434af65aad
    health: HEALTH_WARN
            clock skew detected on mon.c

  services:
    mon: 3 daemons, quorum a,b,c <span class="o">(</span>age 3m<span class="o">)</span>
    mgr: a<span class="o">(</span>active, since 2m<span class="o">)</span>
    osd: 4 osds: 4 up <span class="o">(</span>since 105s<span class="o">)</span>, 4 <span class="k">in</span> <span class="o">(</span>since 105s<span class="o">)</span>

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   4.0 GiB used, 72 GiB / 76 GiB avail
    pgs:

<span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph edit ConfigMap rook-config-override <span class="nt">-o</span> yaml
config: |
    <span class="o">[</span>global]
    mon clock drift allowed <span class="o">=</span> 0.5

<span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph get ConfigMap rook-config-override <span class="nt">-o</span> yaml
apiVersion: v1
data:
  config: |
    <span class="o">[</span>global]
    mon clock drift allowed <span class="o">=</span> 0.5
kind: ConfigMap
metadata:
  creationTimestamp: <span class="s2">"2019-10-18T14:08:39Z"</span>
  name: rook-config-override
  namespace: rook-ceph
  ownerReferences:
  - apiVersion: ceph.rook.io/v1
    blockOwnerDeletion: <span class="nb">true
    </span>kind: CephCluster
    name: rook-ceph
    uid: d0bd3351-e630-44af-b981-550e8a2a50ec
  resourceVersion: <span class="s2">"12831"</span>
  selfLink: /api/v1/namespaces/rook-ceph/configmaps/rook-config-override
  uid: bdf1f1fb-967a-410b-a2bd-b4067ce005d2

<span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph delete pod <span class="si">$(</span>kubectl <span class="nt">-n</span> rook-ceph get pods <span class="nt">-o</span> custom-columns<span class="o">=</span>NAME:.metadata.name <span class="nt">--no-headers</span>| <span class="nb">grep </span>mon<span class="si">)</span>
pod <span class="s2">"rook-ceph-mon-a-8565577958-xtznq"</span> deleted
pod <span class="s2">"rook-ceph-mon-b-79b696df8d-qdcpw"</span> deleted
pod <span class="s2">"rook-ceph-mon-c-5df78f7f96-dr2jn"</span> deleted

<span class="o">[</span>root@kv-master-00 ~]# kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$toolbox</span> sh
sh-4.2# ceph status                                                                         cluster:
    <span class="nb">id</span>:     5a0bbe74-ce42-4f49-813d-7c434af65aad
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c <span class="o">(</span>age 43s<span class="o">)</span>
    mgr: a<span class="o">(</span>active, since 9m<span class="o">)</span>
    osd: 4 osds: 4 up <span class="o">(</span>since 8m<span class="o">)</span>, 4 <span class="k">in</span> <span class="o">(</span>since 8m<span class="o">)</span>

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   4.0 GiB used, 72 GiB / 76 GiB avail
    pgs:
</code></pre></div></div>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://kubernetes.io/docs/setup/">Kubernetes getting started</a></li>
  <li><a href="https://github.com/kubevirt/containerized-data-importer">KubeVirt Containerized Data Importer</a></li>
  <li><a href="https://ceph.io/en/">Ceph: free-software storage platform</a></li>
  <li><a href="https://docs.ceph.com/en/latest/start/hardware-recommendations/">Ceph hardware recommendations</a></li>
  <li><a href="https://rook.io/">Rook: Open-Source,Cloud-Native Storage for Kubernetes</a></li>
  <li><a href="https://kubevirt.io/user-guide">KubeVirt User Guide</a></li>
</ul>

        </article>
        
        

<a class="twitter-share-button" href="https://twitter.com/intent/tweet?text=Persistent storage of your Virtual Machines in KubeVirt with Rook&url=https://www.kubevirt.io/2019/KubeVirt_storage_rook_ceph.html&screen_name=kubevirt" aria-label="Share this on Twitter">
  <i class="fab fa-twitter mr-1"></i> Tweet
</a>
<hr/>


      </div>
    </div>
  </div>
</div>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
<script src="/js/photoswipe-page.js">
</script>

    </main>

    <footer class="footer" role="footer">
      <div class="container-fluid">
  <div class="row justify-content-between">
    <div class="col-sm-12 col-md-5">
      <p>We are a <a href="https://cncf.io/">Cloud Native Computing Foundation</a> sandbox project.</p>
      <p><a href="https://cncf.io/"><img src="/assets/images/cncf-color.png" alt="Cloud Native Computing Foundation"/></a></p>
    </div>
    <div class="col-sm-12 col-md-5" style="text-align: center;">
      <p class="text-md-right">

        <a href="https://twitter.com/kubevirt" data-toggle="tooltip" data-placement="top" title="Follow us on Twitter!" aria-label="Visit us on Twitter" class="link-social-twitter">
          <i class="fab fa-twitter fa-lg"></i>
        </a>

        <a href="https://kubernetes.slack.com/archives/C8ED7RKFE" data-toggle="tooltip" data-placement="top" title="Join our Slack channel!" class="link-social-slack">
          <i class="fab fa-slack fa-lg"></i>
        </a>

        <a href="https://github.com/kubevirt" data-toggle="tooltip" data-placement="top" title="Check our GitHub!" class="link-social-github">
          <i class="fab fa-github fa-lg"></i>
        </a>

        <a href="https://groups.google.com/forum/#!forum/kubevirt-dev" data-toggle="tooltip" data-placement="top" title="Join our mailing list!" class="link-social-mail">
          <i class="fas fa-envelope fa-lg"></i>
        </a>

        <a href="https://calendar.google.com/calendar/u/0/embed?src=kubevirt@cncf.io&ctz=GMT" data-toggle="tooltip" data-placement="top" title="See our events calendar!" class="link-social-calendar">
          <i class="fas fa-calendar fa-lg"></i>
        </a>

        <a href="https://www.youtube.com/channel/UC2FH36TbZizw25pVT1P3C3g/videos" data-toggle="tooltip" data-placement="top" title="Subscribe to our YouTube channel!" class="link-social-youtube">
          <i class="fab fa-youtube fa-lg"></i>
        </a>

      </p>
    </div>
  </div>
  <div class="row">
    <div class="col text-sm-left footer-licensing" style="text-align: center;">
      Copyright 2022 The KubeVirt Contributors<br>
      Copyright 2022 The Linux Foundation. All Rights Reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a> page.<br>
      This site is powered by <a href="https://www.netlify.com/legal/open-source-policy/">Netlify</a>.
      <p class="privacy-statement text-sm-left" style="text-align: center;">
        <a href="/privacy" class="privacy-statement-link">Privacy Statement</a>
      </p>
  </div>
</div>
<script src="/js/copy.js"></script>

    </footer>

    <script defer src="https://use.fontawesome.com/releases/v5.1.0/js/all.js" integrity="sha384-3LK/3kTpDE/Pkp8gTNp2gR/2gOiwQ6QaO7Td0zV76UFJVhqLl4Vl3KL1We6q6wR9" crossorigin="anonymous"></script>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js" integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js" integrity="sha384-uefMccjFJAIv6A+rW+L4AHf99KvxDjWSu1z9VI8SKNVmz4sk7buKt/6v9KI65qnm" crossorigin="anonymous"></script>
    <script src="/js/kubevirt-io.js"></script>
    <!-- Photoswipe -->
    <!-- Core JS file -->
    <script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
    <!-- UI JS file -->
    <script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

    <!-- This comes from DTM/DPAL and must be latest entry in body-->

    <script type="text/javascript">
        if (("undefined" !== typeof _satellite) && ("function" === typeof _satellite.pageBottom)) {
            _satellite.pageBottom();
        }
    </script>
  </body>
</html>
